{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "881f8095",
   "metadata": {},
   "source": [
    "# CLIP Text Embeddings for FLUX\n",
    "\n",
    "This notebook generates **CLIP text embeddings** for use with FLUX image generation.\n",
    "\n",
    "- **Model**: OpenAI CLIP-ViT-Large-Patch14\n",
    "- **Embedding dimension**: 768\n",
    "- **Sequence length**: 77 tokens\n",
    "- **Output shape**: [77, 768]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7z5tnwxjg6c",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "    T[Text Prompt]\n",
    "    \n",
    "    TOK[CLIP Tokenizer]\n",
    "    ENC[CLIP-L Text Encoder]\n",
    "    \n",
    "    EMB[Text Embedding<br/>77 × 768]\n",
    "    \n",
    "    FLUX[FLUX<br/>Diffusion Transformer]\n",
    "    \n",
    "    T --> TOK --> ENC --> EMB\n",
    "    EMB -->|sequence conditioning| FLUX\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b82c06e5-0a76-4fad-8030-52250564b406",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T13:28:39.401116Z",
     "iopub.status.busy": "2026-01-15T13:28:39.400930Z",
     "iopub.status.idle": "2026-01-15T13:28:45.413173Z",
     "shell.execute_reply": "2026-01-15T13:28:45.412663Z",
     "shell.execute_reply.started": "2026-01-15T13:28:39.401102Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model from: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/data/models/clip-vit-large-patch14...\n",
      "✓ Loading from local folder...\n",
      "\n",
      "✓ CLIP loaded successfully!\n",
      "  Embedding dimension: 768\n",
      "  Max sequence length: 77\n",
      "  Loaded from: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/data/models/clip-vit-large-patch14\n",
      "  Model dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load models path from config\n",
    "current_dir = Path.cwd()\n",
    "models_path_file = current_dir.parent / \"misc/paths/models.txt\"\n",
    "with open(models_path_file, 'r') as f:\n",
    "    models_path = f.read().strip()\n",
    "MODELS_DIR = current_dir.parent / models_path\n",
    "\n",
    "# Define CLIP model path\n",
    "CLIP_MODEL_PATH = MODELS_DIR / \"clip-vit-large-patch14\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load CLIP from local folder\n",
    "print(f\"Loading CLIP model from: {CLIP_MODEL_PATH}...\")\n",
    "if not os.path.exists(CLIP_MODEL_PATH):\n",
    "    print(\"\\n⚠️  Model not found locally. Downloading from Hugging Face...\")\n",
    "    print(\"This model is ~1.7GB and will take a few minutes.\")\n",
    "    print(\"Please be patient...\\n\")\n",
    "    \n",
    "    # Download and save to local folder\n",
    "    clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    clip_model = CLIPTextModel.from_pretrained(\n",
    "        \"openai/clip-vit-large-patch14\",\n",
    "        torch_dtype=torch.bfloat16  # Use bfloat16 to match FLUX\n",
    "    )\n",
    "    \n",
    "    # Save to local folder\n",
    "    print(f\"Saving model to {CLIP_MODEL_PATH}...\")\n",
    "    clip_tokenizer.save_pretrained(CLIP_MODEL_PATH)\n",
    "    clip_model.save_pretrained(CLIP_MODEL_PATH)\n",
    "    print(\"✓ Model downloaded and saved locally!\\n\")\n",
    "else:\n",
    "    print(\"✓ Loading from local folder...\\n\")\n",
    "\n",
    "# Load from local folder\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(CLIP_MODEL_PATH, local_files_only=True)\n",
    "clip_model = CLIPTextModel.from_pretrained(\n",
    "    CLIP_MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 to match FLUX\n",
    "    local_files_only=True\n",
    ").to(device)\n",
    "clip_model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"✓ CLIP loaded successfully!\")\n",
    "print(f\"  Embedding dimension: {clip_model.config.hidden_size}\")\n",
    "print(f\"  Max sequence length: {clip_tokenizer.model_max_length}\")\n",
    "print(f\"  Loaded from: {CLIP_MODEL_PATH}\")\n",
    "print(f\"  Model dtype: {next(clip_model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7gk3yu7dz8q",
   "metadata": {},
   "source": [
    "## Generate Single Embedding\n",
    "\n",
    "Enter a prompt to generate its CLIP embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f2d52fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T13:28:47.675482Z",
     "iopub.status.busy": "2026-01-15T13:28:47.675134Z",
     "iopub.status.idle": "2026-01-15T13:28:47.688241Z",
     "shell.execute_reply": "2026-01-15T13:28:47.687712Z",
     "shell.execute_reply.started": "2026-01-15T13:28:47.675467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea700a6c76cc4d5fa4bd005e54813a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='an elephant', description='Prompt:', layout=Layout(height='80px', width='80%'), placeholder='E…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120c29fca2da492eaa2e44b03d9abdae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Generate CLIP Embedding', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e98b08e93e4ecab45efde62548504f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create text input widget\n",
    "clip_prompt_input = widgets.Textarea(\n",
    "    value='an elephant',\n",
    "    placeholder='Enter your prompt here',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='80%', height='80px')\n",
    ")\n",
    "clip_generate_button = widgets.Button(\n",
    "    description='Generate CLIP Embedding',\n",
    "    button_style='success'\n",
    ")\n",
    "clip_output_area = widgets.Output()\n",
    "\n",
    "# Global variable to store current embedding\n",
    "current_clip_embedding = None\n",
    "current_clip_tokens = None\n",
    "\n",
    "def generate_clip_embedding(b):\n",
    "    global current_clip_embedding, current_clip_tokens\n",
    "    \n",
    "    with clip_output_area:\n",
    "        clip_output_area.clear_output()\n",
    "        \n",
    "        prompt = clip_prompt_input.value\n",
    "        print(f\"Generating CLIP embedding for: '{prompt}'\\n\")\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = clip_tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,  # CLIP uses 77 tokens\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Get token strings for display\n",
    "        token_ids = tokens['input_ids'][0].tolist()\n",
    "        token_strings = [clip_tokenizer.decode([tid]) for tid in token_ids]\n",
    "        \n",
    "        # Find how many real tokens (non-padding)\n",
    "        num_real_tokens = (tokens['input_ids'][0] != clip_tokenizer.pad_token_id).sum().item()\n",
    "        \n",
    "        print(f\"Tokenized into {num_real_tokens} real tokens (+ {77 - num_real_tokens} padding):\")\n",
    "        print(\"First 10 tokens:\", token_strings[:10])\n",
    "        print()\n",
    "        \n",
    "        # Generate embedding\n",
    "        with torch.no_grad():\n",
    "            tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "            outputs = clip_model(**tokens)\n",
    "            embedding = outputs.last_hidden_state  # Shape: [1, 77, embedding_dim]\n",
    "        \n",
    "        # Convert bfloat16 to float32 before converting to numpy\n",
    "        current_clip_embedding = embedding.float().cpu().numpy()[0]  # Shape: [77, embedding_dim]\n",
    "        current_clip_tokens = token_strings\n",
    "        \n",
    "        embedding_dim = current_clip_embedding.shape[1]\n",
    "        total_numbers = current_clip_embedding.shape[0] * current_clip_embedding.shape[1]\n",
    "        \n",
    "        print(f\"✓ CLIP embedding generated!\")\n",
    "        print(f\"  Shape: {current_clip_embedding.shape}\")\n",
    "        print(f\"  Total numbers: {total_numbers:,}\")\n",
    "        print(f\"  Size: {current_clip_embedding.nbytes / 1024:.2f} KB\")\n",
    "        print()\n",
    "        print(f\"First token '{token_strings[0]}' embedding (first 10 values):\")\n",
    "        print(current_clip_embedding[0, :10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7po1h3x4v8",
   "metadata": {},
   "source": [
    "## Save Embedding\n",
    "\n",
    "Save the generated embedding to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cd30141",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T13:29:37.845959Z",
     "iopub.status.busy": "2026-01-15T13:29:37.845770Z",
     "iopub.status.idle": "2026-01-15T13:29:37.856228Z",
     "shell.execute_reply": "2026-01-15T13:29:37.855817Z",
     "shell.execute_reply.started": "2026-01-15T13:29:37.845943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea700a6c76cc4d5fa4bd005e54813a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='an elephant', description='Prompt:', layout=Layout(height='80px', width='80%'), placeholder='E…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120c29fca2da492eaa2e44b03d9abdae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Generate CLIP Embedding', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e98b08e93e4ecab45efde62548504f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35451a491ff4a1ca96583273a29dbb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Save Embedding', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd41fa164124b82960ed6b606dd2a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Define embeddings directory\n",
    "current_dir = Path.cwd()\n",
    "EMBEDDINGS_DIR = current_dir.parent / \"data/embeddings/CLIP\"\n",
    "os.makedirs(EMBEDDINGS_DIR, exist_ok=True)\n",
    "\n",
    "clip_save_button = widgets.Button(\n",
    "    description='Save Embedding',\n",
    "    button_style='primary'\n",
    ")\n",
    "\n",
    "clip_save_output = widgets.Output()\n",
    "\n",
    "def save_clip_embedding(b):\n",
    "    with clip_save_output:\n",
    "        clip_save_output.clear_output()\n",
    "        \n",
    "        if current_clip_embedding is None:\n",
    "            print(\"❌ No embedding to save! Generate an embedding first.\")\n",
    "            return\n",
    "        \n",
    "        # Get first 4 non-padding tokens (excluding special tokens)\n",
    "        filename_tokens = []\n",
    "        for token in current_clip_tokens:\n",
    "            # Skip special tokens and padding, clean up CLIP-specific markers\n",
    "            cleaned_token = token.strip().replace('</w>', '').replace('<|startoftext|>', '').replace('<|endoftext|>', '')\n",
    "            if cleaned_token and cleaned_token not in ['<|startoftext|>', '<|endoftext|>', '']:\n",
    "                filename_tokens.append(cleaned_token)\n",
    "            if len(filename_tokens) >= 4:\n",
    "                break\n",
    "        \n",
    "        # Create filename from first 4 tokens\n",
    "        filename = \"_\".join(filename_tokens) + \".json\"\n",
    "        filepath = EMBEDDINGS_DIR / filename\n",
    "        \n",
    "        # Save embedding\n",
    "        embedding_data = {\n",
    "            \"prompt\": clip_prompt_input.value,\n",
    "            \"embedding\": current_clip_embedding.tolist(),\n",
    "            \"shape\": list(current_clip_embedding.shape)\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(embedding_data, f)\n",
    "        \n",
    "        print(f\"✓ Embedding saved to:\")\n",
    "        print(f\"  {filepath}\")\n",
    "        print(f\"  Size: {os.path.getsize(filepath) / 1024:.2f} KB\")\n",
    "\n",
    "clip_generate_button.on_click(generate_clip_embedding)\n",
    "display(clip_prompt_input, clip_generate_button, clip_output_area)\n",
    "\n",
    "clip_save_button.on_click(save_clip_embedding)\n",
    "\n",
    "display(clip_save_button, clip_save_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7y01djpmmdq",
   "metadata": {},
   "source": [
    "## Batch Generation from Text Input\n",
    "\n",
    "Enter multiple prompts (one per line) to generate and save embeddings for all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d907613-73dc-4166-8032-ed7dca052fc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T13:30:29.150141Z",
     "iopub.status.busy": "2026-01-15T13:30:29.149921Z",
     "iopub.status.idle": "2026-01-15T13:30:29.161951Z",
     "shell.execute_reply": "2026-01-15T13:30:29.161535Z",
     "shell.execute_reply.started": "2026-01-15T13:30:29.150125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch CLIP Embedding Generator\n",
      "Output directory: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/data/embeddings/CLIP\n",
      "Enter prompts (one per line):\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92f63eecdab4f229d53c6a277d04e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='an elephant\\na red sports car\\na mountain landscape with snow', description='Prompts:', layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131032577421428d82447ee6d7a92e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='warning', description='Batch Generate & Save', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6638a2a675634eb1b273525530c989b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Batch generate CLIP embeddings from text input\n",
    "batch_prompt_input = widgets.Textarea(\n",
    "    value='an elephant\\na red sports car\\na mountain landscape with snow',\n",
    "    placeholder='Enter prompts, one per line',\n",
    "    description='Prompts:',\n",
    "    layout=widgets.Layout(width='80%', height='150px')\n",
    ")\n",
    "\n",
    "batch_generate_button = widgets.Button(\n",
    "    description='Batch Generate & Save',\n",
    "    button_style='warning'\n",
    ")\n",
    "\n",
    "batch_output_area = widgets.Output()\n",
    "\n",
    "def batch_generate_clip_embeddings(b):\n",
    "    with batch_output_area:\n",
    "        batch_output_area.clear_output()\n",
    "        \n",
    "        # Parse prompts (one per line)\n",
    "        prompts = [p.strip() for p in batch_prompt_input.value.strip().split('\\n') if p.strip()]\n",
    "        \n",
    "        if not prompts:\n",
    "            print(\"No prompts provided!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Generating {len(prompts)} CLIP embeddings...\\n\")\n",
    "        \n",
    "        for i, prompt in enumerate(prompts, 1):\n",
    "            print(f\"[{i}/{len(prompts)}] '{prompt[:50]}{'...' if len(prompt) > 50 else ''}'\")\n",
    "            \n",
    "            # Tokenize\n",
    "            tokens = clip_tokenizer(\n",
    "                prompt,\n",
    "                padding=\"max_length\",\n",
    "                max_length=77,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Get token strings for filename\n",
    "            token_ids = tokens['input_ids'][0].tolist()\n",
    "            token_strings = [clip_tokenizer.decode([tid]) for tid in token_ids]\n",
    "            \n",
    "            # Generate embedding\n",
    "            with torch.no_grad():\n",
    "                tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "                outputs = clip_model(**tokens)\n",
    "                embedding = outputs.last_hidden_state.float().cpu().numpy()[0]\n",
    "            \n",
    "            # Create filename from first 4 tokens\n",
    "            filename_tokens = []\n",
    "            for token in token_strings:\n",
    "                cleaned = token.strip().replace('</w>', '').replace('<|startoftext|>', '').replace('<|endoftext|>', '')\n",
    "                if cleaned and cleaned not in ['<|startoftext|>', '<|endoftext|>', '']:\n",
    "                    filename_tokens.append(cleaned)\n",
    "                if len(filename_tokens) >= 4:\n",
    "                    break\n",
    "            \n",
    "            filename = \"_\".join(filename_tokens) + \".json\"\n",
    "            filepath = EMBEDDINGS_DIR / filename\n",
    "            \n",
    "            # Save embedding\n",
    "            embedding_data = {\n",
    "                \"prompt\": prompt,\n",
    "                \"embedding\": embedding.tolist(),\n",
    "                \"shape\": list(embedding.shape)\n",
    "            }\n",
    "            \n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(embedding_data, f)\n",
    "            \n",
    "            print(f\"   ✓ Saved: {filename}\")\n",
    "        \n",
    "        print(f\"\\n✓ All {len(prompts)} embeddings saved to:\")\n",
    "        print(f\"  {EMBEDDINGS_DIR}\")\n",
    "\n",
    "\n",
    "\n",
    "batch_generate_button.on_click(batch_generate_clip_embeddings)\n",
    "\n",
    "print(\"Batch CLIP Embedding Generator\")\n",
    "print(f\"Output directory: {EMBEDDINGS_DIR}\")\n",
    "print(\"Enter prompts (one per line):\\n\")\n",
    "display(batch_prompt_input, batch_generate_button, batch_output_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g3lgseftf3u",
   "metadata": {},
   "source": [
    "## Batch Generation from Example Prompts File\n",
    "\n",
    "Load CLIP prompts from `misc/example_prompts.txt` and generate embeddings. Files are saved to `examples/` subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "scooq9mirl",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T14:00:29.812768Z",
     "iopub.status.busy": "2026-01-15T14:00:29.812563Z",
     "iopub.status.idle": "2026-01-15T14:00:29.829154Z",
     "shell.execute_reply": "2026-01-15T14:00:29.828645Z",
     "shell.execute_reply.started": "2026-01-15T14:00:29.812748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate CLIP embeddings from example_prompts.txt\n",
      "Source: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/misc/example_prompts.txt\n",
      "Base output: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/data/embeddings/examples/clip\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f98d906694441dcba00f9c05911111b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Prompt Set:', index=1, options=(('Short prompts', 'short'), ('CLIP prompts (77 tokens)',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8522a69d291d45fbac2f255844640cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='info', description='Generate from File', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f81db45ae1d46fe98b35687d1a94285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Paths\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "BASE_EXAMPLES_DIR = current_dir.parent / \"data/embeddings/examples/clip\"\n",
    "os.makedirs(BASE_EXAMPLES_DIR, exist_ok=True)\n",
    "\n",
    "prompts_file = current_dir.parent / \"misc/example_prompts.txt\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# CLIP UX config\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "CLIP_CONFIG = {\n",
    "    'short': {\n",
    "        'section_name': 'Short prompts',\n",
    "        'subdir': 'short',\n",
    "        'max_length': 77\n",
    "    },\n",
    "    '77_tokens': {\n",
    "        'section_name': 'CLIP prompts',\n",
    "        'subdir': '77_tokens',\n",
    "        'max_length': 77\n",
    "    }\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Prompt loader (section-aware)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def load_clip_prompts_from_file(filepath, section_name):\n",
    "    \"\"\"Load a specific prompt section from example_prompts.txt.\"\"\"\n",
    "    if not filepath.exists():\n",
    "        return []\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    sections = content.split('#')\n",
    "    target_section = None\n",
    "\n",
    "    for section in sections:\n",
    "        if section_name in section:\n",
    "            target_section = section\n",
    "            break\n",
    "\n",
    "    if target_section is None:\n",
    "        return []\n",
    "\n",
    "    prompts = []\n",
    "    for line in target_section.splitlines():\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith('#'):\n",
    "            prompts.append(line)\n",
    "\n",
    "    return prompts\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Widgets\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "clip_token_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('Short prompts', 'short'),\n",
    "        ('CLIP prompts (77 tokens)', '77_tokens')\n",
    "    ],\n",
    "    value='77_tokens',\n",
    "    description='Prompt Set:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "file_batch_button = widgets.Button(\n",
    "    description='Generate from File',\n",
    "    button_style='info'\n",
    ")\n",
    "\n",
    "file_batch_output = widgets.Output()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Batch generation\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def batch_generate_from_file(b):\n",
    "    with file_batch_output:\n",
    "        file_batch_output.clear_output()\n",
    "\n",
    "        selection = clip_token_selector.value\n",
    "        config = CLIP_CONFIG[selection]\n",
    "\n",
    "        max_length = config['max_length']   # always 77 for CLIP\n",
    "        output_dir = BASE_EXAMPLES_DIR / config['subdir']\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        prompts = load_clip_prompts_from_file(\n",
    "            prompts_file,\n",
    "            config['section_name']\n",
    "        )\n",
    "\n",
    "        if not prompts:\n",
    "            print(f\"❌ No prompts found for section '{config['section_name']}'.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Loaded {len(prompts)} prompts\")\n",
    "        print(f\"Section: {config['section_name']}\")\n",
    "        print(f\"Token length: {max_length}\")\n",
    "        print(f\"Output: {output_dir}\\n\")\n",
    "\n",
    "        for i, prompt in enumerate(prompts, 1):\n",
    "            print(f\"[{i}/{len(prompts)}] {prompt[:60]}{'...' if len(prompt) > 60 else ''}\")\n",
    "\n",
    "            tokens = clip_tokenizer(\n",
    "                prompt,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "                outputs = clip_model(**tokens)\n",
    "                embedding = outputs.last_hidden_state.float().cpu().numpy()[0]\n",
    "\n",
    "            # Filename from first meaningful tokens\n",
    "            token_ids = tokens['input_ids'][0].tolist()\n",
    "            token_strings = [clip_tokenizer.decode([tid]) for tid in token_ids]\n",
    "\n",
    "            filename_tokens = []\n",
    "            for token in token_strings:\n",
    "                cleaned = (\n",
    "                    token.replace('</w>', '')\n",
    "                         .replace('<|startoftext|>', '')\n",
    "                         .replace('<|endoftext|>', '')\n",
    "                         .strip()\n",
    "                )\n",
    "                if cleaned:\n",
    "                    filename_tokens.append(cleaned)\n",
    "                if len(filename_tokens) >= 4:\n",
    "                    break\n",
    "\n",
    "            filename = \"_\".join(filename_tokens) + \".json\"\n",
    "            filepath = output_dir / filename\n",
    "\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"embedding\": embedding.tolist(),\n",
    "                    \"shape\": list(embedding.shape),\n",
    "                    \"max_length\": max_length,\n",
    "                    \"prompt_set\": config['section_name']\n",
    "                }, f)\n",
    "\n",
    "            print(f\"   ✓ Saved: {filename}\")\n",
    "\n",
    "        print(\"\\n✅ All CLIP embeddings generated successfully.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# UI wiring\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "file_batch_button.on_click(batch_generate_from_file)\n",
    "\n",
    "print(\"Generate CLIP embeddings from example_prompts.txt\")\n",
    "print(f\"Source: {prompts_file}\")\n",
    "print(f\"Base output: {BASE_EXAMPLES_DIR}\\n\")\n",
    "\n",
    "display(clip_token_selector, file_batch_button, file_batch_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zzurn17ix9m",
   "metadata": {},
   "source": [
    "---\n",
    "<sub>Latent Vandalism Workshop • Laura Wagner, 2026 • [laurajul.github.io](https://laurajul.github.io/)</sub>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flux uv",
   "language": "python",
   "name": "uv_flux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
