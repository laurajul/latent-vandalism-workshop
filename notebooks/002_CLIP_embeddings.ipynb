{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "881f8095",
   "metadata": {},
   "source": "# CLIP Text Embeddings for FLUX\n\nThis notebook generates **CLIP text embeddings** for use with FLUX image generation.\n\n- **Model**: OpenAI CLIP-ViT-Large-Patch14\n- **Embedding dimension**: 768\n- **Sequence length**: 77 tokens\n- **Output shape**: [77, 768]"
  },
  {
   "cell_type": "markdown",
   "id": "7z5tnwxjg6c",
   "source": "```mermaid\nflowchart LR\n    T[Text Prompt]\n    \n    TOK[CLIP Tokenizer]\n    ENC[CLIP-L Text Encoder]\n    \n    EMB[Text Embedding<br/>77 × 768]\n    \n    FLUX[FLUX<br/>Diffusion Transformer]\n    \n    T --> TOK --> ENC --> EMB\n    EMB -->|sequence conditioning| FLUX\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82c06e5-0a76-4fad-8030-52250564b406",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T16:45:57.535344Z",
     "iopub.status.busy": "2026-01-12T16:45:57.535171Z",
     "iopub.status.idle": "2026-01-12T16:46:01.849939Z",
     "shell.execute_reply": "2026-01-12T16:46:01.849380Z",
     "shell.execute_reply.started": "2026-01-12T16:45:57.535329Z"
    }
   },
   "outputs": [],
   "source": "from transformers import CLIPTextModel, CLIPTokenizer\nimport torch\nimport os\nfrom pathlib import Path\n\n# Load models path from config\ncurrent_dir = Path.cwd()\nmodels_path_file = current_dir.parent / \"misc/paths/models.txt\"\nwith open(models_path_file, 'r') as f:\n    models_path = f.read().strip()\nMODELS_DIR = current_dir.parent / models_path\n\n# Define CLIP model path\nCLIP_MODEL_PATH = MODELS_DIR / \"clip-vit-large-patch14\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load CLIP from local folder\nprint(f\"Loading CLIP model from: {CLIP_MODEL_PATH}...\")\nif not os.path.exists(CLIP_MODEL_PATH):\n    print(\"\\n⚠️  Model not found locally. Downloading from Hugging Face...\")\n    print(\"This model is ~1.7GB and will take a few minutes.\")\n    print(\"Please be patient...\\n\")\n    \n    # Download and save to local folder\n    clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n    clip_model = CLIPTextModel.from_pretrained(\n        \"openai/clip-vit-large-patch14\",\n        torch_dtype=torch.bfloat16  # Use bfloat16 to match FLUX\n    )\n    \n    # Save to local folder\n    print(f\"Saving model to {CLIP_MODEL_PATH}...\")\n    clip_tokenizer.save_pretrained(CLIP_MODEL_PATH)\n    clip_model.save_pretrained(CLIP_MODEL_PATH)\n    print(\"✓ Model downloaded and saved locally!\\n\")\nelse:\n    print(\"✓ Loading from local folder...\\n\")\n\n# Load from local folder\nclip_tokenizer = CLIPTokenizer.from_pretrained(CLIP_MODEL_PATH, local_files_only=True)\nclip_model = CLIPTextModel.from_pretrained(\n    CLIP_MODEL_PATH,\n    torch_dtype=torch.bfloat16,  # Use bfloat16 to match FLUX\n    local_files_only=True\n).to(device)\nclip_model.eval()  # Set to evaluation mode\n\nprint(f\"✓ CLIP loaded successfully!\")\nprint(f\"  Embedding dimension: {clip_model.config.hidden_size}\")\nprint(f\"  Max sequence length: {clip_tokenizer.model_max_length}\")\nprint(f\"  Loaded from: {CLIP_MODEL_PATH}\")\nprint(f\"  Model dtype: {next(clip_model.parameters()).dtype}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f2d52fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T16:46:04.526718Z",
     "iopub.status.busy": "2026-01-12T16:46:04.526368Z",
     "iopub.status.idle": "2026-01-12T16:46:04.540122Z",
     "shell.execute_reply": "2026-01-12T16:46:04.539543Z",
     "shell.execute_reply.started": "2026-01-12T16:46:04.526702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5d9793e1814fd1909f1ee0b32b79e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='an elephant', description='Prompt:', layout=Layout(height='80px', width='80%'), placeholder='E…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5344a066c84335adf9b5729234395b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Generate CLIP Embedding', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4296f355956d45878d0cd53dca1af41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create text input widget\n",
    "clip_prompt_input = widgets.Textarea(\n",
    "    value='an elephant',\n",
    "    placeholder='Enter your prompt here',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='80%', height='80px')\n",
    ")\n",
    "clip_generate_button = widgets.Button(\n",
    "    description='Generate CLIP Embedding',\n",
    "    button_style='success'\n",
    ")\n",
    "clip_output_area = widgets.Output()\n",
    "\n",
    "# Global variable to store current embedding\n",
    "current_clip_embedding = None\n",
    "current_clip_tokens = None\n",
    "\n",
    "def generate_clip_embedding(b):\n",
    "    global current_clip_embedding, current_clip_tokens\n",
    "    \n",
    "    with clip_output_area:\n",
    "        clip_output_area.clear_output()\n",
    "        \n",
    "        prompt = clip_prompt_input.value\n",
    "        print(f\"Generating CLIP embedding for: '{prompt}'\\n\")\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = clip_tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,  # CLIP uses 77 tokens\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Get token strings for display\n",
    "        token_ids = tokens['input_ids'][0].tolist()\n",
    "        token_strings = [clip_tokenizer.decode([tid]) for tid in token_ids]\n",
    "        \n",
    "        # Find how many real tokens (non-padding)\n",
    "        num_real_tokens = (tokens['input_ids'][0] != clip_tokenizer.pad_token_id).sum().item()\n",
    "        \n",
    "        print(f\"Tokenized into {num_real_tokens} real tokens (+ {77 - num_real_tokens} padding):\")\n",
    "        print(\"First 10 tokens:\", token_strings[:10])\n",
    "        print()\n",
    "        \n",
    "        # Generate embedding\n",
    "        with torch.no_grad():\n",
    "            tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "            outputs = clip_model(**tokens)\n",
    "            embedding = outputs.last_hidden_state  # Shape: [1, 77, embedding_dim]\n",
    "        \n",
    "        # Convert bfloat16 to float32 before converting to numpy\n",
    "        current_clip_embedding = embedding.float().cpu().numpy()[0]  # Shape: [77, embedding_dim]\n",
    "        current_clip_tokens = token_strings\n",
    "        \n",
    "        embedding_dim = current_clip_embedding.shape[1]\n",
    "        total_numbers = current_clip_embedding.shape[0] * current_clip_embedding.shape[1]\n",
    "        \n",
    "        print(f\"✓ CLIP embedding generated!\")\n",
    "        print(f\"  Shape: {current_clip_embedding.shape}\")\n",
    "        print(f\"  Total numbers: {total_numbers:,}\")\n",
    "        print(f\"  Size: {current_clip_embedding.nbytes / 1024:.2f} KB\")\n",
    "        print()\n",
    "        print(f\"First token '{token_strings[0]}' embedding (first 10 values):\")\n",
    "        print(current_clip_embedding[0, :10])\n",
    "\n",
    "clip_generate_button.on_click(generate_clip_embedding)\n",
    "display(clip_prompt_input, clip_generate_button, clip_output_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cd30141",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T16:46:09.798777Z",
     "iopub.status.busy": "2026-01-12T16:46:09.798581Z",
     "iopub.status.idle": "2026-01-12T16:46:09.807452Z",
     "shell.execute_reply": "2026-01-12T16:46:09.806988Z",
     "shell.execute_reply.started": "2026-01-12T16:46:09.798762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3152d4cb804124803bd1a922a3d4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Save Embedding', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3b36a0634847e2abe9cb49056a34c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Define embeddings directory\n",
    "current_dir = Path.cwd()\n",
    "EMBEDDINGS_DIR = current_dir.parent / \"data/embeddings/CLIP\"\n",
    "os.makedirs(EMBEDDINGS_DIR, exist_ok=True)\n",
    "\n",
    "clip_save_button = widgets.Button(\n",
    "    description='Save Embedding',\n",
    "    button_style='primary'\n",
    ")\n",
    "\n",
    "clip_save_output = widgets.Output()\n",
    "\n",
    "def save_clip_embedding(b):\n",
    "    with clip_save_output:\n",
    "        clip_save_output.clear_output()\n",
    "        \n",
    "        if current_clip_embedding is None:\n",
    "            print(\"❌ No embedding to save! Generate an embedding first.\")\n",
    "            return\n",
    "        \n",
    "        # Get first 4 non-padding tokens (excluding special tokens)\n",
    "        filename_tokens = []\n",
    "        for token in current_clip_tokens:\n",
    "            # Skip special tokens and padding, clean up CLIP-specific markers\n",
    "            cleaned_token = token.strip().replace('</w>', '').replace('<|startoftext|>', '').replace('<|endoftext|>', '')\n",
    "            if cleaned_token and cleaned_token not in ['<|startoftext|>', '<|endoftext|>', '']:\n",
    "                filename_tokens.append(cleaned_token)\n",
    "            if len(filename_tokens) >= 4:\n",
    "                break\n",
    "        \n",
    "        # Create filename from first 4 tokens\n",
    "        filename = \"_\".join(filename_tokens) + \".json\"\n",
    "        filepath = EMBEDDINGS_DIR / filename\n",
    "        \n",
    "        # Save embedding\n",
    "        embedding_data = {\n",
    "            \"prompt\": clip_prompt_input.value,\n",
    "            \"embedding\": current_clip_embedding.tolist(),\n",
    "            \"shape\": list(current_clip_embedding.shape)\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(embedding_data, f)\n",
    "        \n",
    "        print(f\"✓ Embedding saved to:\")\n",
    "        print(f\"  {filepath}\")\n",
    "        print(f\"  Size: {os.path.getsize(filepath) / 1024:.2f} KB\")\n",
    "\n",
    "clip_save_button.on_click(save_clip_embedding)\n",
    "\n",
    "display(clip_save_button, clip_save_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d907613-73dc-4166-8032-ed7dca052fc7",
   "metadata": {},
   "outputs": [],
   "source": "# Batch generate CLIP embeddings for multiple prompts\nBATCH_DIR = EMBEDDINGS_DIR / \"examples\"\nos.makedirs(BATCH_DIR, exist_ok=True)\n\nbatch_prompt_input = widgets.Textarea(\n    value='an elephant\\na red sports car\\na mountain landscape with snow',\n    placeholder='Enter up to 10 prompts, one per line',\n    description='Prompts:',\n    layout=widgets.Layout(width='80%', height='200px')\n)\n\nbatch_generate_button = widgets.Button(\n    description='Batch Generate & Save',\n    button_style='warning'\n)\n\nbatch_output_area = widgets.Output()\n\ndef batch_generate_clip_embeddings(b):\n    with batch_output_area:\n        batch_output_area.clear_output()\n        \n        # Parse prompts (one per line, max 10)\n        prompts = [p.strip() for p in batch_prompt_input.value.strip().split('\\n') if p.strip()]\n        prompts = prompts[:10]  # Limit to 10\n        \n        if not prompts:\n            print(\"❌ No prompts provided!\")\n            return\n        \n        print(f\"Generating {len(prompts)} CLIP embeddings...\\n\")\n        \n        for i, prompt in enumerate(prompts, 1):\n            print(f\"[{i}/{len(prompts)}] '{prompt[:50]}{'...' if len(prompt) > 50 else ''}'\")\n            \n            # Tokenize\n            tokens = clip_tokenizer(\n                prompt,\n                padding=\"max_length\",\n                max_length=77,\n                truncation=True,\n                return_tensors=\"pt\"\n            )\n            \n            # Get token strings for filename\n            token_ids = tokens['input_ids'][0].tolist()\n            token_strings = [clip_tokenizer.decode([tid]) for tid in token_ids]\n            \n            # Generate embedding\n            with torch.no_grad():\n                tokens = {k: v.to(device) for k, v in tokens.items()}\n                outputs = clip_model(**tokens)\n                embedding = outputs.last_hidden_state.float().cpu().numpy()[0]\n            \n            # Create filename from first 4 tokens\n            filename_tokens = []\n            for token in token_strings:\n                cleaned = token.strip().replace('</w>', '').replace('<|startoftext|>', '').replace('<|endoftext|>', '')\n                if cleaned and cleaned not in ['<|startoftext|>', '<|endoftext|>', '']:\n                    filename_tokens.append(cleaned)\n                if len(filename_tokens) >= 4:\n                    break\n            \n            filename = \"_\".join(filename_tokens) + \".json\"\n            filepath = BATCH_DIR / filename\n            \n            # Save embedding\n            embedding_data = {\n                \"prompt\": prompt,\n                \"embedding\": embedding.tolist(),\n                \"shape\": list(embedding.shape)\n            }\n            \n            with open(filepath, 'w') as f:\n                json.dump(embedding_data, f)\n            \n            print(f\"   ✓ Saved: {filename}\")\n        \n        print(f\"\\n✓ All {len(prompts)} embeddings saved to:\")\n        print(f\"  {BATCH_DIR}\")\n\nbatch_generate_button.on_click(batch_generate_clip_embeddings)\n\nprint(\"Batch CLIP Embedding Generator\")\nprint(f\"Output directory: {BATCH_DIR}\")\nprint(\"Enter up to 10 prompts (one per line):\\n\")\ndisplay(batch_prompt_input, batch_generate_button, batch_output_area)"
  },
  {
   "cell_type": "markdown",
   "id": "zzurn17ix9m",
   "source": "---\n<sub>Latent Vandalism Workshop • Laura Wagner, 2026 • [laurajul.github.io](https://laurajul.github.io/)</sub>",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flux uv",
   "language": "python",
   "name": "uv_flux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}