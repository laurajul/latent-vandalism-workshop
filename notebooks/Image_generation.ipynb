{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 Embedding Manipulation for FLUX\n",
    "\n",
    "This notebook lets you:\n",
    "1. Generate T5 embeddings from text prompts\n",
    "2. Save/load embeddings as JSON\n",
    "3. Manually edit embedding values\n",
    "4. Apply custom attention masks\n",
    "5. Generate images with FLUX using modified embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Run this cell first to install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T22:00:32.981182Z",
     "iopub.status.busy": "2026-01-10T22:00:32.981062Z",
     "iopub.status.idle": "2026-01-10T22:03:23.620406Z",
     "shell.execute_reply": "2026-01-10T22:03:23.619915Z",
     "shell.execute_reply.started": "2026-01-10T22:00:32.981170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Models directory: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshopt/data/models\n",
      "T5 path: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshopt/data/models/t5-v1_1-xxl\n",
      "FLUX path: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshopt/data/models/FLUX.1-schnell\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "from diffusers import FluxPipeline\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Image as IPImage\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create models directory\n",
    "current_dir = Path.cwd()\n",
    "MODELS_DIR = current_dir.parent / \"data/models\"\n",
    "T5_MODEL_PATH = os.path.join(MODELS_DIR, \"t5-v1_1-xxl\")\n",
    "CLIP_MODEL_PATH = os.path.join(MODELS_DIR, \"clip\")\n",
    "FLUX_MODEL_PATH = os.path.join(MODELS_DIR, \"FLUX.1-schnell\")\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "print(f\"Models directory: {os.path.abspath(MODELS_DIR)}\")\n",
    "print(f\"T5 path: {os.path.abspath(T5_MODEL_PATH)}\")\n",
    "print(f\"FLUX path: {os.path.abspath(FLUX_MODEL_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T22:03:38.213827Z",
     "iopub.status.busy": "2026-01-10T22:03:38.213641Z",
     "iopub.status.idle": "2026-01-10T22:03:38.471142Z",
     "shell.execute_reply": "2026-01-10T22:03:38.470646Z",
     "shell.execute_reply.started": "2026-01-10T22:03:38.213812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for HF token at: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshopt/misc/credentials/hf.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Hugging Face token loaded and authenticated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load Hugging Face token from file\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the token file path\n",
    "current_dir = Path.cwd()\n",
    "token_file = current_dir.parent / \"misc/credentials/hf.txt\"\n",
    "\n",
    "print(f\"Looking for HF token at: {token_file}\")\n",
    "\n",
    "if token_file.exists():\n",
    "    with open(token_file, 'r') as f:\n",
    "        hf_token = f.read().strip()\n",
    "    \n",
    "    # Set the token as an environment variable\n",
    "    os.environ['HF_TOKEN'] = hf_token\n",
    "    \n",
    "    # Also login using huggingface_hub\n",
    "    from huggingface_hub import login\n",
    "    login(token=hf_token)\n",
    "    \n",
    "    print(\"✓ Hugging Face token loaded and authenticated successfully!\")\n",
    "else:\n",
    "    print(f\"❌ Token file not found at: {token_file}\")\n",
    "    print(\"Please create the file or update the path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T22:03:39.523961Z",
     "iopub.status.busy": "2026-01-10T22:03:39.523776Z",
     "iopub.status.idle": "2026-01-10T22:03:39.526257Z",
     "shell.execute_reply": "2026-01-10T22:03:39.525572Z",
     "shell.execute_reply.started": "2026-01-10T22:03:39.523946Z"
    }
   },
   "outputs": [],
   "source": [
    "MODELS_DIR = current_dir.parent / \"data/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T22:03:40.290643Z",
     "iopub.status.busy": "2026-01-10T22:03:40.290468Z",
     "iopub.status.idle": "2026-01-10T22:06:20.818288Z",
     "shell.execute_reply": "2026-01-10T22:06:20.786918Z",
     "shell.execute_reply.started": "2026-01-10T22:03:40.290629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "279262ac097e42cfac94f280592cb5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197e74e8fc804bfeb3a34f1e1846b095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c096b4c2bc3460f8ba3ff15eeecab77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load FLUX\n",
    "try:\n",
    "    if not os.path.exists(FLUX_MODEL_PATH):\n",
    "        flux_pipe = FluxPipeline.from_pretrained(\n",
    "            \"black-forest-labs/FLUX.1-schnell\",\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        flux_pipe.save_pretrained(FLUX_MODEL_PATH)\n",
    "    else:\n",
    "        flux_pipe = FluxPipeline.from_pretrained(\n",
    "            FLUX_MODEL_PATH,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            local_files_only=True\n",
    "        )\n",
    "    \n",
    "    flux_pipe = flux_pipe.to(device)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading FLUX: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T22:35:38.525765Z",
     "iopub.status.busy": "2026-01-10T22:35:38.525543Z",
     "iopub.status.idle": "2026-01-10T22:35:38.533183Z",
     "shell.execute_reply": "2026-01-10T22:35:38.532696Z",
     "shell.execute_reply.started": "2026-01-10T22:35:38.525747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88646d269b4247c2abb1d0b3d8e47393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='info', description='Save Modified Embedding', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07de9fa9d5f4469bebe7e4f04331019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save modified embedding to JSON\n",
    "def save_modified_embedding(filename=\"modified_embedding.json\"):\n",
    "    if modified_embedding is None:\n",
    "        print(\"❌ No modified embedding to save! Apply a manipulation first.\")\n",
    "        return\n",
    "    \n",
    "    data = {\n",
    "        \"embedding\": modified_embedding.tolist(),\n",
    "        \"tokens\": current_tokens,\n",
    "        \"shape\": list(modified_embedding.shape),\n",
    "        \"prompt\": prompt_input.value\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    \n",
    "    file_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "    print(f\"✓ Modified embedding saved to '{filename}' ({file_size:.2f} MB)\")\n",
    "\n",
    "# Save modified embedding button\n",
    "save_modified_button = widgets.Button(description='Save Modified Embedding', button_style='info')\n",
    "save_modified_output = widgets.Output()\n",
    "\n",
    "def on_save_modified_click(b):\n",
    "    with save_modified_output:\n",
    "        save_modified_output.clear_output()\n",
    "        save_modified_embedding()\n",
    "\n",
    "save_modified_button.on_click(on_save_modified_click)\n",
    "display(save_modified_button, save_modified_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T22:24:20.553029Z",
     "iopub.status.busy": "2026-01-10T22:24:20.552873Z",
     "iopub.status.idle": "2026-01-10T22:24:20.560940Z",
     "shell.execute_reply": "2026-01-10T22:24:20.560468Z",
     "shell.execute_reply.started": "2026-01-10T22:24:20.553011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a750425625a048f08b3d17154f8a240f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='warning', description='Load Modified Embedding', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869231c8043740a696d6e40c9210d1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load modified embedding from JSON\n",
    "def load_modified_embedding(filename=\"modified_embedding.json\"):\n",
    "    global modified_embedding, current_tokens\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    modified_embedding = np.array(data['embedding'])\n",
    "    current_tokens = data['tokens']\n",
    "    \n",
    "    print(f\"✓ Modified embedding loaded from '{filename}'\")\n",
    "    print(f\"  Original prompt: {data['prompt']}\")\n",
    "    print(f\"  Shape: {modified_embedding.shape}\")\n",
    "    print(f\"  Non-zero values: {np.count_nonzero(modified_embedding):,}\")\n",
    "\n",
    "# Load modified embedding button\n",
    "load_modified_button = widgets.Button(description='Load Modified Embedding', button_style='warning')\n",
    "load_modified_output = widgets.Output()\n",
    "\n",
    "def on_load_modified_click(b):\n",
    "    with load_modified_output:\n",
    "        load_modified_output.clear_output()\n",
    "        try:\n",
    "            load_modified_embedding()\n",
    "        except FileNotFoundError:\n",
    "            print(\"❌ File 'modified_embedding.json' not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading: {e}\")\n",
    "\n",
    "load_modified_button.on_click(on_load_modified_click)\n",
    "display(load_modified_button, load_modified_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Manual Attention Masking\n",
    "\n",
    "Control which tokens get how much attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display tokens and create sliders\n",
    "def create_attention_sliders(num_tokens=10):\n",
    "    if current_tokens is None:\n",
    "        print(\"❌ Generate an embedding first!\")\n",
    "        return None\n",
    "    \n",
    "    # Find non-padding tokens\n",
    "    real_tokens = []\n",
    "    for i, token in enumerate(current_tokens):\n",
    "        if token.strip() and token != '<pad>' and i < num_tokens:\n",
    "            real_tokens.append((i, token))\n",
    "    \n",
    "    print(f\"Creating sliders for first {len(real_tokens)} tokens:\")\n",
    "    print(real_tokens)\n",
    "    print()\n",
    "    \n",
    "    sliders = []\n",
    "    for idx, token in real_tokens:\n",
    "        slider = widgets.FloatSlider(\n",
    "            value=1.0 / len(real_tokens),  # Equal distribution\n",
    "            min=0.0,\n",
    "            max=1.0,\n",
    "            step=0.01,\n",
    "            description=f'{idx}: {token[:15]}',\n",
    "            readout_format='.0%',\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "        sliders.append(slider)\n",
    "    \n",
    "    # Normalize button\n",
    "    normalize_button = widgets.Button(\n",
    "        description='Normalize to 100%',\n",
    "        button_style='info'\n",
    "    )\n",
    "    \n",
    "    total_label = widgets.Label(value='Total: 100%')\n",
    "    \n",
    "    def update_total(*args):\n",
    "        total = sum(s.value for s in sliders)\n",
    "        total_label.value = f'Total: {total*100:.1f}%'\n",
    "        if abs(total - 1.0) > 0.01:\n",
    "            total_label.value += ' ⚠️ Should sum to 100%'\n",
    "    \n",
    "    def normalize(*args):\n",
    "        total = sum(s.value for s in sliders)\n",
    "        if total > 0:\n",
    "            for s in sliders:\n",
    "                s.value = s.value / total\n",
    "        update_total()\n",
    "    \n",
    "    for slider in sliders:\n",
    "        slider.observe(update_total, 'value')\n",
    "    \n",
    "    normalize_button.on_click(normalize)\n",
    "    \n",
    "    update_total()\n",
    "    \n",
    "    return sliders, normalize_button, total_label, real_tokens\n",
    "\n",
    "attention_controls = create_attention_sliders(num_tokens=10)\n",
    "\n",
    "if attention_controls:\n",
    "    sliders, normalize_btn, total_lbl, real_tokens = attention_controls\n",
    "    display(widgets.VBox(sliders + [normalize_btn, total_lbl]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Attention Mask Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_mask():\n",
    "    if attention_controls is None:\n",
    "        print(\"❌ Create attention sliders first!\")\n",
    "        return None\n",
    "    \n",
    "    sliders, _, _, real_tokens = attention_controls\n",
    "    \n",
    "    # Create mask array (512 tokens)\n",
    "    mask = np.zeros(512)\n",
    "    \n",
    "    # Set weights from sliders\n",
    "    for slider, (idx, token) in zip(sliders, real_tokens):\n",
    "        mask[idx] = slider.value\n",
    "    \n",
    "    print(\"Attention Mask created:\")\n",
    "    print(f\"Non-zero weights: {np.count_nonzero(mask)}\")\n",
    "    for slider, (idx, token) in zip(sliders, real_tokens):\n",
    "        print(f\"  Token {idx} '{token}': {slider.value*100:.1f}%\")\n",
    "    \n",
    "    return mask\n",
    "\n",
    "mask_button = widgets.Button(description='Create Mask', button_style='success')\n",
    "mask_output = widgets.Output()\n",
    "\n",
    "attention_mask = None\n",
    "\n",
    "def on_mask_click(b):\n",
    "    global attention_mask\n",
    "    with mask_output:\n",
    "        mask_output.clear_output()\n",
    "        attention_mask = create_attention_mask()\n",
    "\n",
    "mask_button.on_click(on_mask_click)\n",
    "display(mask_button, mask_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T21:38:16.909223Z",
     "iopub.status.busy": "2026-01-10T21:38:16.909032Z",
     "iopub.status.idle": "2026-01-10T21:38:16.911354Z",
     "shell.execute_reply": "2026-01-10T21:38:16.910899Z",
     "shell.execute_reply.started": "2026-01-10T21:38:16.909207Z"
    }
   },
   "source": [
    "### Load CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T22:18:14.377045Z",
     "iopub.status.busy": "2026-01-10T22:18:14.376867Z",
     "iopub.status.idle": "2026-01-10T22:18:16.369058Z",
     "shell.execute_reply": "2026-01-10T22:18:16.368505Z",
     "shell.execute_reply.started": "2026-01-10T22:18:14.377031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model from: ./models/clip-vit-large-patch14...\n",
      "✓ Loading from local folder...\n",
      "\n",
      "✓ CLIP loaded successfully!\n",
      "  Embedding dimension: 768\n",
      "  Max sequence length: 77\n",
      "  Loaded from: ./models/clip-vit-large-patch14\n",
      "  Model dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Define CLIP model path\n",
    "CLIP_MODEL_PATH = \"./models/clip-vit-large-patch14\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load CLIP from local folder\n",
    "print(f\"Loading CLIP model from: {CLIP_MODEL_PATH}...\")\n",
    "if not os.path.exists(CLIP_MODEL_PATH):\n",
    "    print(\"\\n⚠️  Model not found locally. Downloading from Hugging Face...\")\n",
    "    print(\"This model is ~1.7GB and will take a few minutes.\")\n",
    "    print(\"Please be patient...\\n\")\n",
    "    \n",
    "    # Download and save to local folder\n",
    "    clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    clip_model = CLIPTextModel.from_pretrained(\n",
    "        \"openai/clip-vit-large-patch14\",\n",
    "        torch_dtype=torch.bfloat16  # Use bfloat16 to match FLUX\n",
    "    )\n",
    "    \n",
    "    # Save to local folder\n",
    "    print(f\"Saving model to {CLIP_MODEL_PATH}...\")\n",
    "    clip_tokenizer.save_pretrained(CLIP_MODEL_PATH)\n",
    "    clip_model.save_pretrained(CLIP_MODEL_PATH)\n",
    "    print(\"✓ Model downloaded and saved locally!\\n\")\n",
    "else:\n",
    "    print(\"✓ Loading from local folder...\\n\")\n",
    "\n",
    "# Load from local folder\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(CLIP_MODEL_PATH, local_files_only=True)\n",
    "clip_model = CLIPTextModel.from_pretrained(\n",
    "    CLIP_MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 to match FLUX\n",
    "    local_files_only=True\n",
    ").to(device)\n",
    "clip_model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"✓ CLIP loaded successfully!\")\n",
    "print(f\"  Embedding dimension: {clip_model.config.hidden_size}\")\n",
    "print(f\"  Max sequence length: {clip_tokenizer.model_max_length}\")\n",
    "print(f\"  Loaded from: {CLIP_MODEL_PATH}\")\n",
    "print(f\"  Model dtype: {next(clip_model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T22:20:50.637674Z",
     "iopub.status.busy": "2026-01-10T22:20:50.637457Z",
     "iopub.status.idle": "2026-01-10T22:20:50.646266Z",
     "shell.execute_reply": "2026-01-10T22:20:50.645636Z",
     "shell.execute_reply.started": "2026-01-10T22:20:50.637656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84baa4448d854aedbc5e96bfd5a868aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='info', description='Save CLIP Embedding', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128809854ecc49bb98b5c77307ab17f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def save_clip_embedding(filename=\"clip_embedding.json\"):\n",
    "    if current_clip_embedding is None:\n",
    "        print(\"❌ No CLIP embedding to save! Generate one first.\")\n",
    "        return\n",
    "    \n",
    "    data = {\n",
    "        \"embedding\": current_clip_embedding.tolist(),\n",
    "        \"tokens\": current_clip_tokens,\n",
    "        \"shape\": list(current_clip_embedding.shape),\n",
    "        \"prompt\": clip_prompt_input.value\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    \n",
    "    file_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "    print(f\"✓ CLIP embedding saved to '{filename}' ({file_size:.2f} MB)\")\n",
    "\n",
    "# Save button\n",
    "clip_save_button = widgets.Button(description='Save CLIP Embedding', button_style='info')\n",
    "clip_save_output = widgets.Output()\n",
    "\n",
    "def on_clip_save_click(b):\n",
    "    with clip_save_output:\n",
    "        clip_save_output.clear_output()\n",
    "        save_clip_embedding()\n",
    "\n",
    "clip_save_button.on_click(on_clip_save_click)\n",
    "display(clip_save_button, clip_save_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Image from embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_from_embedding(embedding_array, output_filename=\"generated_from_embedding.png\", seed=42):\n",
    "    \"\"\"\n",
    "    Generate image using custom T5 embedding by injecting it into FLUX pipeline.\n",
    "    \"\"\"\n",
    "    if 'flux_pipe' not in globals():\n",
    "        print(\"❌ FLUX not loaded!\")\n",
    "        return None\n",
    "    \n",
    "    if embedding_array is None:\n",
    "        print(\"❌ No embedding provided! Load an embedding first.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Generating image from custom embedding...\")\n",
    "    print(f\"  Embedding shape: {embedding_array.shape}\")\n",
    "    print(f\"  Expected shape: [512, 4096]\")\n",
    "    \n",
    "    # Convert numpy to torch tensor with bfloat16 to match FLUX\n",
    "    # First ensure numpy array is float32, then convert to bfloat16\n",
    "    embedding_tensor = torch.from_numpy(embedding_array.astype(np.float32)).to(\n",
    "        device=device,\n",
    "        dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Add batch dimension: [512, 4096] -> [1, 512, 4096]\n",
    "    embedding_tensor = embedding_tensor.unsqueeze(0)\n",
    "    \n",
    "    print(f\"  Tensor shape: {embedding_tensor.shape}\")\n",
    "    print(f\"  Device: {embedding_tensor.device}\")\n",
    "    print(f\"  Dtype: {embedding_tensor.dtype}\")\n",
    "    \n",
    "    # Generate pooled embeddings using CLIP text encoder\n",
    "    print(\"\\nGenerating pooled embeddings from CLIP...\")\n",
    "    try:\n",
    "        # Use the original prompt to get pooled embeddings from CLIP\n",
    "        prompt = prompt_input.value\n",
    "        \n",
    "        # Use FLUX's built-in encode_prompt method to get the correct pooled embeddings\n",
    "        # This ensures we get exactly what FLUX expects\n",
    "        with torch.no_grad():\n",
    "            (\n",
    "                _,  # We already have T5 embeddings\n",
    "                pooled_embeds,  # This is what we need from CLIP\n",
    "                _,  # text_ids\n",
    "            ) = flux_pipe.encode_prompt(\n",
    "                prompt=prompt,\n",
    "                prompt_2=None,\n",
    "                device=device,\n",
    "                num_images_per_prompt=1,\n",
    "                prompt_embeds=None,\n",
    "                pooled_prompt_embeds=None,\n",
    "                max_sequence_length=512,\n",
    "            )\n",
    "        \n",
    "        print(f\"  Pooled embeddings shape: {pooled_embeds.shape}\")\n",
    "        print(f\"  Pooled embeddings dtype: {pooled_embeds.dtype}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating pooled embeddings: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    \n",
    "    # Generate image using both embeddings\n",
    "    try:\n",
    "        print(\"\\nRunning diffusion process (4 steps)...\")\n",
    "        image = flux_pipe(\n",
    "            prompt_embeds=embedding_tensor,\n",
    "            pooled_prompt_embeds=pooled_embeds,\n",
    "            num_inference_steps=4,\n",
    "            guidance_scale=0.0,\n",
    "            height=1024,\n",
    "            width=1024,\n",
    "            generator=torch.manual_seed(seed)\n",
    "        ).images[0]\n",
    "        \n",
    "        image.save(output_filename)\n",
    "        print(f\"\\n✓ Image generated and saved to '{output_filename}'\")\n",
    "        \n",
    "        return image\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating image: {e}\")\n",
    "        print(\"\\nThis might happen if:\")\n",
    "        print(\"  - Embedding dimensions don't match\")\n",
    "        print(\"  - Insufficient VRAM/RAM\")\n",
    "        print(\"  - dtype mismatch (check that tensors are bfloat16)\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Generate from current embedding button\n",
    "generate_from_embedding_button = widgets.Button(\n",
    "    description='Generate from Current Embedding',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "# Generate from modified embedding button\n",
    "generate_from_modified_button = widgets.Button(\n",
    "    description='Generate from Modified Embedding',\n",
    "    button_style='warning',\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "generation_output = widgets.Output()\n",
    "\n",
    "def on_generate_from_embedding(b):\n",
    "    with generation_output:\n",
    "        generation_output.clear_output(wait=True)\n",
    "        if current_embedding is not None:\n",
    "            image = generate_image_from_embedding(\n",
    "                current_embedding,\n",
    "                output_filename=\"image_original_embedding.png\"\n",
    "            )\n",
    "            if image:\n",
    "                display(image)\n",
    "        else:\n",
    "            print(\"❌ No embedding loaded! Generate or load an embedding first.\")\n",
    "\n",
    "def on_generate_from_modified(b):\n",
    "    with generation_output:\n",
    "        generation_output.clear_output(wait=True)\n",
    "        if modified_embedding is not None:\n",
    "            image = generate_image_from_embedding(\n",
    "                modified_embedding,\n",
    "                output_filename=\"image_modified_embedding.png\"\n",
    "            )\n",
    "            if image:\n",
    "                display(image)\n",
    "        else:\n",
    "            print(\"❌ No modified embedding! Apply a manipulation first or load one.\")\n",
    "\n",
    "generate_from_embedding_button.on_click(on_generate_from_embedding)\n",
    "generate_from_modified_button.on_click(on_generate_from_modified)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Generate Images from Embeddings</h3>\"),\n",
    "    generate_from_embedding_button,\n",
    "    generate_from_modified_button\n",
    "]), generation_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T22:21:02.074735Z",
     "iopub.status.busy": "2026-01-10T22:21:02.074530Z",
     "iopub.status.idle": "2026-01-10T22:21:02.081805Z",
     "shell.execute_reply": "2026-01-10T22:21:02.081314Z",
     "shell.execute_reply.started": "2026-01-10T22:21:02.074720Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3bbe98de1d41ce9b89928f8e961c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='warning', description='Load CLIP Embedding', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa59164d8f040b588ac1f3e75497cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_clip_embedding(filename=\"clip_embedding.json\"):\n",
    "    global current_clip_embedding, current_clip_tokens\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    current_clip_embedding = np.array(data['embedding'])\n",
    "    current_clip_tokens = data['tokens']\n",
    "    \n",
    "    print(f\"✓ CLIP embedding loaded from '{filename}'\")\n",
    "    print(f\"  Original prompt: {data['prompt']}\")\n",
    "    print(f\"  Shape: {current_clip_embedding.shape}\")\n",
    "    print(f\"  First token: '{current_clip_tokens[0]}'\")\n",
    "\n",
    "# Load button\n",
    "load_clip_button = widgets.Button(description='Load CLIP Embedding', button_style='warning')\n",
    "load_clip_output = widgets.Output()\n",
    "\n",
    "def on_load_clip_click(b):\n",
    "    with load_clip_output:\n",
    "        load_clip_output.clear_output()\n",
    "        try:\n",
    "            load_clip_embedding()\n",
    "        except FileNotFoundError:\n",
    "            print(\"❌ File 'clip_embedding.json' not found. Save a CLIP embedding first.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading CLIP embedding: {e}\")\n",
    "\n",
    "load_clip_button.on_click(on_load_clip_click)\n",
    "display(load_clip_button, load_clip_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP + T5 EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T22:24:32.434397Z",
     "iopub.status.busy": "2026-01-10T22:24:32.434214Z",
     "iopub.status.idle": "2026-01-10T22:24:32.457693Z",
     "shell.execute_reply": "2026-01-10T22:24:32.457167Z",
     "shell.execute_reply.started": "2026-01-10T22:24:32.434383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f981230a9a3e43b792144a9b99dda073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Generate Images from Embeddings</h3>'), HTML(value='<p><b>Choose your combinati…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd20e68f48e4492b9c53763a7210f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_image_from_embedding(\n",
    "    t5_embedding_array, \n",
    "    clip_embedding_array=None,\n",
    "    output_filename=\"generated_from_embedding.png\", \n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate image using custom T5 and CLIP embeddings by injecting them into FLUX pipeline.\n",
    "    \"\"\"\n",
    "    if 'flux_pipe' not in globals():\n",
    "        print(\"❌ FLUX not loaded!\")\n",
    "        return None\n",
    "    \n",
    "    if t5_embedding_array is None:\n",
    "        print(\"❌ No T5 embedding provided! Load an embedding first.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Generating image from custom embeddings...\")\n",
    "    print(f\"  T5 embedding shape: {t5_embedding_array.shape}\")\n",
    "    print(f\"  Expected T5 shape: [512, 4096]\")\n",
    "    \n",
    "    # Convert T5 numpy to torch tensor with bfloat16 to match FLUX\n",
    "    t5_embedding_tensor = torch.from_numpy(t5_embedding_array.astype(np.float32)).to(\n",
    "        device=device,\n",
    "        dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Add batch dimension: [512, 4096] -> [1, 512, 4096]\n",
    "    t5_embedding_tensor = t5_embedding_tensor.unsqueeze(0)\n",
    "    \n",
    "    print(f\"  T5 tensor shape: {t5_embedding_tensor.shape}\")\n",
    "    print(f\"  Device: {t5_embedding_tensor.device}\")\n",
    "    print(f\"  Dtype: {t5_embedding_tensor.dtype}\")\n",
    "    \n",
    "    # Handle CLIP pooled embeddings\n",
    "    print(\"\\nProcessing CLIP pooled embeddings...\")\n",
    "    try:\n",
    "        if clip_embedding_array is not None:\n",
    "            # Use provided CLIP embedding\n",
    "            print(\"  Using provided CLIP embedding from JSON\")\n",
    "            print(f\"  CLIP embedding shape: {clip_embedding_array.shape}\")\n",
    "            print(f\"  Expected CLIP shape: [77, 768]\")\n",
    "            \n",
    "            # FLUX typically uses the last token embedding as pooled embeddings\n",
    "            # (the EOS token), not the first token (CLS). Let's try that.\n",
    "            # We'll also try mean pooling as an alternative.\n",
    "            \n",
    "            # Option 1: Use EOS token (last non-padding token)\n",
    "            # For CLIP, this is typically the last token\n",
    "            pooled_embeds = torch.from_numpy(clip_embedding_array[-1:].astype(np.float32)).to(\n",
    "                device=device,\n",
    "                dtype=torch.bfloat16\n",
    "            )\n",
    "            \n",
    "            # Option 2: Mean pooling (uncomment to try this instead)\n",
    "            # pooled_embeds = torch.from_numpy(clip_embedding_array.mean(axis=0, keepdims=True).astype(np.float32)).to(\n",
    "            #     device=device,\n",
    "            #     dtype=torch.bfloat16\n",
    "            # )\n",
    "            \n",
    "            print(f\"  Using last token embedding as pooled embedding\")\n",
    "            # Shape should be [1, 768]\n",
    "            \n",
    "        else:\n",
    "            # Fall back to generating from text using CLIP\n",
    "            print(\"  No CLIP embedding provided, generating from prompt text...\")\n",
    "            prompt = prompt_input.value\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                (\n",
    "                    _,  # We already have T5 embeddings\n",
    "                    pooled_embeds,  # This is what we need from CLIP\n",
    "                    _,  # text_ids\n",
    "                ) = flux_pipe.encode_prompt(\n",
    "                    prompt=prompt,\n",
    "                    prompt_2=None,\n",
    "                    device=device,\n",
    "                    num_images_per_prompt=1,\n",
    "                    prompt_embeds=None,\n",
    "                    pooled_prompt_embeds=None,\n",
    "                    max_sequence_length=512,\n",
    "                )\n",
    "        \n",
    "        print(f\"  Pooled embeddings shape: {pooled_embeds.shape}\")\n",
    "        print(f\"  Pooled embeddings dtype: {pooled_embeds.dtype}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing pooled embeddings: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    \n",
    "    # Generate image using both embeddings\n",
    "    try:\n",
    "        print(\"\\nRunning diffusion process (4 steps)...\")\n",
    "        image = flux_pipe(\n",
    "            prompt_embeds=t5_embedding_tensor,\n",
    "            pooled_prompt_embeds=pooled_embeds,\n",
    "            num_inference_steps=4,\n",
    "            guidance_scale=0.0,\n",
    "            height=1024,\n",
    "            width=1024,\n",
    "            generator=torch.manual_seed(seed)\n",
    "        ).images[0]\n",
    "        \n",
    "        image.save(output_filename)\n",
    "        print(f\"\\n✓ Image generated and saved to '{output_filename}'\")\n",
    "        \n",
    "        return image\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating image: {e}\")\n",
    "        print(\"\\nThis might happen if:\")\n",
    "        print(\"  - Embedding dimensions don't match\")\n",
    "        print(\"  - Insufficient VRAM/RAM\")\n",
    "        print(\"  - dtype mismatch (check that tensors are bfloat16)\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Create 4 buttons for all combinations\n",
    "button1 = widgets.Button(\n",
    "    description='T5 + CLIP',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='250px')\n",
    ")\n",
    "\n",
    "button2 = widgets.Button(\n",
    "    description='T5 + Modified CLIP',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='250px')\n",
    ")\n",
    "\n",
    "button3 = widgets.Button(\n",
    "    description='Modified T5 + CLIP',\n",
    "    button_style='warning',\n",
    "    layout=widgets.Layout(width='250px')\n",
    ")\n",
    "\n",
    "button4 = widgets.Button(\n",
    "    description='Modified T5 + Modified CLIP',\n",
    "    button_style='danger',\n",
    "    layout=widgets.Layout(width='250px')\n",
    ")\n",
    "\n",
    "generation_output = widgets.Output()\n",
    "\n",
    "# Callback 1: T5 + CLIP\n",
    "def on_generate_t5_clip(b):\n",
    "    with generation_output:\n",
    "        generation_output.clear_output(wait=True)\n",
    "        \n",
    "        # Check if embeddings exist\n",
    "        t5_emb = globals().get('current_embedding', None)\n",
    "        clip_emb = globals().get('current_clip_embedding', None)\n",
    "        \n",
    "        if t5_emb is None:\n",
    "            print(\"❌ No T5 embedding loaded! Generate or load a T5 embedding first.\")\n",
    "            return\n",
    "        \n",
    "        image = generate_image_from_embedding(\n",
    "            t5_embedding_array=t5_emb,\n",
    "            clip_embedding_array=clip_emb,\n",
    "            output_filename=\"image_t5_clip.png\"\n",
    "        )\n",
    "        if image:\n",
    "            display(image)\n",
    "\n",
    "# Callback 2: T5 + Modified CLIP\n",
    "def on_generate_t5_modified_clip(b):\n",
    "    with generation_output:\n",
    "        generation_output.clear_output(wait=True)\n",
    "        \n",
    "        t5_emb = globals().get('current_embedding', None)\n",
    "        clip_emb = globals().get('modified_clip_embedding', None)\n",
    "        \n",
    "        if t5_emb is None:\n",
    "            print(\"❌ No T5 embedding loaded! Generate or load a T5 embedding first.\")\n",
    "            return\n",
    "        \n",
    "        if clip_emb is None:\n",
    "            print(\"❌ No modified CLIP embedding! Apply a CLIP manipulation first.\")\n",
    "            return\n",
    "        \n",
    "        image = generate_image_from_embedding(\n",
    "            t5_embedding_array=t5_emb,\n",
    "            clip_embedding_array=clip_emb,\n",
    "            output_filename=\"image_t5_modified_clip.png\"\n",
    "        )\n",
    "        if image:\n",
    "            display(image)\n",
    "\n",
    "# Callback 3: Modified T5 + CLIP\n",
    "def on_generate_modified_t5_clip(b):\n",
    "    with generation_output:\n",
    "        generation_output.clear_output(wait=True)\n",
    "        \n",
    "        t5_emb = globals().get('modified_embedding', None)\n",
    "        clip_emb = globals().get('current_clip_embedding', None)\n",
    "        \n",
    "        if t5_emb is None:\n",
    "            print(\"❌ No modified T5 embedding! Apply a T5 manipulation first.\")\n",
    "            return\n",
    "        \n",
    "        image = generate_image_from_embedding(\n",
    "            t5_embedding_array=t5_emb,\n",
    "            clip_embedding_array=clip_emb,\n",
    "            output_filename=\"image_modified_t5_clip.png\"\n",
    "        )\n",
    "        if image:\n",
    "            display(image)\n",
    "\n",
    "# Callback 4: Modified T5 + Modified CLIP\n",
    "def on_generate_modified_t5_modified_clip(b):\n",
    "    with generation_output:\n",
    "        generation_output.clear_output(wait=True)\n",
    "        \n",
    "        t5_emb = globals().get('modified_embedding', None)\n",
    "        clip_emb = globals().get('modified_clip_embedding', None)\n",
    "        \n",
    "        if t5_emb is None:\n",
    "            print(\"❌ No modified T5 embedding! Apply a T5 manipulation first.\")\n",
    "            return\n",
    "        \n",
    "        if clip_emb is None:\n",
    "            print(\"❌ No modified CLIP embedding! Apply a CLIP manipulation first.\")\n",
    "            return\n",
    "        \n",
    "        image = generate_image_from_embedding(\n",
    "            t5_embedding_array=t5_emb,\n",
    "            clip_embedding_array=clip_emb,\n",
    "            output_filename=\"image_modified_t5_modified_clip.png\"\n",
    "        )\n",
    "        if image:\n",
    "            display(image)\n",
    "\n",
    "# Attach callbacks\n",
    "button1.on_click(on_generate_t5_clip)\n",
    "button2.on_click(on_generate_t5_modified_clip)\n",
    "button3.on_click(on_generate_modified_t5_clip)\n",
    "button4.on_click(on_generate_modified_t5_modified_clip)\n",
    "\n",
    "# Display all buttons\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Generate Images from Embeddings</h3>\"),\n",
    "    widgets.HTML(\"<p><b>Choose your combination:</b></p>\"),\n",
    "    button1,\n",
    "    button2,\n",
    "    button3,\n",
    "    button4\n",
    "]), generation_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare Original vs Modified Embeddings\n",
    "\n",
    "Generate both images and view them side-by-side to see the effect of your modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T22:27:57.162024Z",
     "iopub.status.busy": "2026-01-10T22:27:57.161832Z",
     "iopub.status.idle": "2026-01-10T22:27:57.175836Z",
     "shell.execute_reply": "2026-01-10T22:27:57.175385Z",
     "shell.execute_reply.started": "2026-01-10T22:27:57.162009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68eac96981e4b00ba13352279cd497c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>⚡ Generate Comparison</h3><p>This will generate images from both embeddings wit…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59905cab15149e0b08191aa31be9f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compare_embeddings(seed=42):\n",
    "    \"\"\"\n",
    "    Generate images from both original and modified embeddings and display side-by-side.\n",
    "    \"\"\"\n",
    "    if 'flux_pipe' not in globals():\n",
    "        print(\"❌ FLUX not loaded!\")\n",
    "        return\n",
    "    \n",
    "    if current_embedding is None:\n",
    "        print(\"❌ No original embedding! Generate or load an embedding first.\")\n",
    "        return\n",
    "    \n",
    "    if modified_embedding is None:\n",
    "        print(\"❌ No modified embedding! Apply a manipulation first.\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"EMBEDDING COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Show embedding statistics\n",
    "    print(\"\\nORIGINAL EMBEDDING:\")\n",
    "    print(f\"  Non-zero values: {np.count_nonzero(current_embedding):,}\")\n",
    "    print(f\"  Mean: {current_embedding.mean():.4f}\")\n",
    "    print(f\"  Std: {current_embedding.std():.4f}\")\n",
    "    print(f\"  Min: {current_embedding.min():.4f}\")\n",
    "    print(f\"  Max: {current_embedding.max():.4f}\")\n",
    "    \n",
    "    print(\"\\nMODIFIED EMBEDDING:\")\n",
    "    print(f\"  Non-zero values: {np.count_nonzero(modified_embedding):,}\")\n",
    "    print(f\"  Mean: {modified_embedding.mean():.4f}\")\n",
    "    print(f\"  Std: {modified_embedding.std():.4f}\")\n",
    "    print(f\"  Min: {modified_embedding.min():.4f}\")\n",
    "    print(f\"  Max: {modified_embedding.max():.4f}\")\n",
    "    \n",
    "    diff = np.abs(current_embedding - modified_embedding).sum()\n",
    "    print(f\"\\nTOTAL ABSOLUTE DIFFERENCE: {diff:,.2f}\")\n",
    "    print(f\"Percentage changed: {(np.count_nonzero(current_embedding != modified_embedding) / current_embedding.size * 100):.2f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GENERATING IMAGES...\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Generate from original\n",
    "    print(\"[1/2] Generating from ORIGINAL embedding...\")\n",
    "    img1 = generate_image_from_embedding(\n",
    "        current_embedding,\n",
    "        output_filename=\"comparison_original.png\",\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    if img1 is None:\n",
    "        return\n",
    "    \n",
    "    print(\"\\n[2/2] Generating from MODIFIED embedding...\")\n",
    "    img2 = generate_image_from_embedding(\n",
    "        modified_embedding,\n",
    "        output_filename=\"comparison_modified.png\",\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    if img2 is None:\n",
    "        return\n",
    "    \n",
    "    # Display side by side\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARISON RESULTS\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    axes[0].imshow(img1)\n",
    "    axes[0].set_title('Original Embedding', fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(img2)\n",
    "    axes[1].set_title('Modified Embedding', fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Prompt: \"{prompt_input.value}\"', fontsize=12, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"comparison_sidebyside.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Comparison complete!\")\n",
    "    print(\"\\nFiles saved:\")\n",
    "    print(\"  - comparison_original.png\")\n",
    "    print(\"  - comparison_modified.png\")\n",
    "    print(\"  - comparison_sidebyside.png\")\n",
    "\n",
    "# Comparison button\n",
    "compare_button = widgets.Button(\n",
    "    description='Compare Original vs Modified',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='300px', height='50px')\n",
    ")\n",
    "\n",
    "compare_output = widgets.Output()\n",
    "\n",
    "def on_compare_click(b):\n",
    "    with compare_output:\n",
    "        compare_output.clear_output(wait=True)\n",
    "        compare_embeddings()\n",
    "\n",
    "compare_button.on_click(on_compare_click)\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>⚡ Generate Comparison</h3><p>This will generate images from both embeddings with the same random seed.</p>\"),\n",
    "    compare_button\n",
    "]), compare_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "What you've learned:\n",
    "\n",
    "1. ✓ Generate T5-XXL embeddings from text (512 tokens × 4096 dimensions = 2,097,152 numbers)\n",
    "2. ✓ Save/load embeddings as JSON files\n",
    "3. ✓ Manually manipulate embedding values\n",
    "4. ✓ Create custom attention masks with sliders\n",
    "5. ✓ Generate images directly from custom embeddings using FLUX\n",
    "6. ✓ Compare original vs modified embeddings side-by-side\n",
    "7. ✓ Understand the complete pipeline: text → T5-XXL → embeddings → FLUX → image\n",
    "\n",
    "## Workflow:\n",
    "\n",
    "1. **Generate embedding** from your text prompt\n",
    "2. **Save to JSON** for backup\n",
    "3. **Apply manipulations** (zero out values, etc.)\n",
    "4. **Create attention masks** (optional - for future use)\n",
    "5. **Generate images** from both original and modified embeddings\n",
    "6. **Compare results** to see what your changes did!\n",
    "\n",
    "## Next Steps:\n",
    "\n",
    "- Experiment with different zero percentages (10%, 30%, 50%, 90%)\n",
    "- Try zeroing specific token embeddings instead of random values\n",
    "- Compare results with different prompts\n",
    "- Implement attention mask injection (requires modifying cross-attention layers)\n",
    "- Explore interpolating between two different embeddings\n",
    "- Try adding noise to embeddings and see the effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up cache to free home directory space\n",
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def cleanup_cache():\n",
    "    \"\"\"Delete cache directories to free up home quota\"\"\"\n",
    "    cache_dirs = [\n",
    "        Path.home() / \"data\" / \".cache\",\n",
    "        Path.home() / \".cache\" / \"huggingface\",\n",
    "        Path.home() / \".cache\" / \"torch\",\n",
    "        Path.home() / \".cache\" / \"uv\",\n",
    "    ]\n",
    "    \n",
    "    total_freed = 0\n",
    "    for cache_dir in cache_dirs:\n",
    "        if cache_dir.exists():\n",
    "            try:\n",
    "                # Get size before deletion (approximate)\n",
    "                size = sum(f.stat().st_size for f in cache_dir.rglob('*') if f.is_file())\n",
    "                shutil.rmtree(cache_dir)\n",
    "                total_freed += size\n",
    "                print(f\"\\u2713 Deleted {cache_dir}: {size / 1024**3:.2f} GB\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\u2717 Could not delete {cache_dir}: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal freed: {total_freed / 1024**3:.2f} GB\")\n",
    "\n",
    "# Run cleanup\n",
    "cleanup_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flux uv",
   "language": "python",
   "name": "uv_flux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
