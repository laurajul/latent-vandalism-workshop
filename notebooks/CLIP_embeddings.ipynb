{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "881f8095",
   "metadata": {},
   "source": [
    "### Load CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82c06e5-0a76-4fad-8030-52250564b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Define CLIP model path\n",
    "CLIP_MODEL_PATH = \"./models/clip-vit-large-patch14\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load CLIP from local folder\n",
    "print(f\"Loading CLIP model from: {CLIP_MODEL_PATH}...\")\n",
    "if not os.path.exists(CLIP_MODEL_PATH):\n",
    "    print(\"\\n⚠️  Model not found locally. Downloading from Hugging Face...\")\n",
    "    print(\"This model is ~1.7GB and will take a few minutes.\")\n",
    "    print(\"Please be patient...\\n\")\n",
    "    \n",
    "    # Download and save to local folder\n",
    "    clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    clip_model = CLIPTextModel.from_pretrained(\n",
    "        \"openai/clip-vit-large-patch14\",\n",
    "        torch_dtype=torch.bfloat16  # Use bfloat16 to match FLUX\n",
    "    )\n",
    "    \n",
    "    # Save to local folder\n",
    "    print(f\"Saving model to {CLIP_MODEL_PATH}...\")\n",
    "    clip_tokenizer.save_pretrained(CLIP_MODEL_PATH)\n",
    "    clip_model.save_pretrained(CLIP_MODEL_PATH)\n",
    "    print(\"✓ Model downloaded and saved locally!\\n\")\n",
    "else:\n",
    "    print(\"✓ Loading from local folder...\\n\")\n",
    "\n",
    "# Load from local folder\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(CLIP_MODEL_PATH, local_files_only=True)\n",
    "clip_model = CLIPTextModel.from_pretrained(\n",
    "    CLIP_MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 to match FLUX\n",
    "    local_files_only=True\n",
    ").to(device)\n",
    "clip_model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"✓ CLIP loaded successfully!\")\n",
    "print(f\"  Embedding dimension: {clip_model.config.hidden_size}\")\n",
    "print(f\"  Max sequence length: {clip_tokenizer.model_max_length}\")\n",
    "print(f\"  Loaded from: {CLIP_MODEL_PATH}\")\n",
    "print(f\"  Model dtype: {next(clip_model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2d52fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create text input widget\n",
    "clip_prompt_input = widgets.Textarea(\n",
    "    value='an elephant',\n",
    "    placeholder='Enter your prompt here',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='80%', height='80px')\n",
    ")\n",
    "clip_generate_button = widgets.Button(\n",
    "    description='Generate CLIP Embedding',\n",
    "    button_style='success'\n",
    ")\n",
    "clip_output_area = widgets.Output()\n",
    "\n",
    "# Global variable to store current embedding\n",
    "current_clip_embedding = None\n",
    "current_clip_tokens = None\n",
    "\n",
    "def generate_clip_embedding(b):\n",
    "    global current_clip_embedding, current_clip_tokens\n",
    "    \n",
    "    with clip_output_area:\n",
    "        clip_output_area.clear_output()\n",
    "        \n",
    "        prompt = clip_prompt_input.value\n",
    "        print(f\"Generating CLIP embedding for: '{prompt}'\\n\")\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = clip_tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,  # CLIP uses 77 tokens\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Get token strings for display\n",
    "        token_ids = tokens['input_ids'][0].tolist()\n",
    "        token_strings = [clip_tokenizer.decode([tid]) for tid in token_ids]\n",
    "        \n",
    "        # Find how many real tokens (non-padding)\n",
    "        num_real_tokens = (tokens['input_ids'][0] != clip_tokenizer.pad_token_id).sum().item()\n",
    "        \n",
    "        print(f\"Tokenized into {num_real_tokens} real tokens (+ {77 - num_real_tokens} padding):\")\n",
    "        print(\"First 10 tokens:\", token_strings[:10])\n",
    "        print()\n",
    "        \n",
    "        # Generate embedding\n",
    "        with torch.no_grad():\n",
    "            tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "            outputs = clip_model(**tokens)\n",
    "            embedding = outputs.last_hidden_state  # Shape: [1, 77, embedding_dim]\n",
    "        \n",
    "        # Convert bfloat16 to float32 before converting to numpy\n",
    "        current_clip_embedding = embedding.float().cpu().numpy()[0]  # Shape: [77, embedding_dim]\n",
    "        current_clip_tokens = token_strings\n",
    "        \n",
    "        embedding_dim = current_clip_embedding.shape[1]\n",
    "        total_numbers = current_clip_embedding.shape[0] * current_clip_embedding.shape[1]\n",
    "        \n",
    "        print(f\"✓ CLIP embedding generated!\")\n",
    "        print(f\"  Shape: {current_clip_embedding.shape}\")\n",
    "        print(f\"  Total numbers: {total_numbers:,}\")\n",
    "        print(f\"  Size: {current_clip_embedding.nbytes / 1024:.2f} KB\")\n",
    "        print()\n",
    "        print(f\"First token '{token_strings[0]}' embedding (first 10 values):\")\n",
    "        print(current_clip_embedding[0, :10])\n",
    "\n",
    "clip_generate_button.on_click(generate_clip_embedding)\n",
    "display(clip_prompt_input, clip_generate_button, clip_output_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd30141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_clip_embedding(filename=\"clip_embedding.json\"):\n",
    "    if current_clip_embedding is None:\n",
    "        print(\"❌ No CLIP embedding to save! Generate one first.\")\n",
    "        return\n",
    "    \n",
    "    data = {\n",
    "        \"embedding\": current_clip_embedding.tolist(),\n",
    "        \"tokens\": current_clip_tokens,\n",
    "        \"shape\": list(current_clip_embedding.shape),\n",
    "        \"prompt\": clip_prompt_input.value\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    \n",
    "    file_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "    print(f\"✓ CLIP embedding saved to '{filename}' ({file_size:.2f} MB)\")\n",
    "\n",
    "# Save button\n",
    "clip_save_button = widgets.Button(description='Save CLIP Embedding', button_style='info')\n",
    "clip_save_output = widgets.Output()\n",
    "\n",
    "def on_clip_save_click(b):\n",
    "    with clip_save_output:\n",
    "        clip_save_output.clear_output()\n",
    "        save_clip_embedding()\n",
    "\n",
    "clip_save_button.on_click(on_clip_save_click)\n",
    "display(clip_save_button, clip_save_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
