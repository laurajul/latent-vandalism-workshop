{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f06314-ec5f-4bc5-981a-c7784365609d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T15:12:01.618088Z",
     "iopub.status.busy": "2026-01-12T15:12:01.617959Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import os\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "#from diffusers import FluxPipeline\n",
    "#from IPython.display import display, Image as IPImage\n",
    "#from PIL import Image\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create models directory\n",
    "current_dir = Path.cwd()\n",
    "MODELS_DIR = current_dir.parent / \"data/models\"\n",
    "T5_MODEL_PATH = os.path.join(MODELS_DIR, \"t5-v1_1-xxl\")\n",
    "\n",
    "# os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "# print(f\"Models directory: {os.path.abspath(MODELS_DIR)}\")\n",
    "# print(f\"T5 path: {os.path.abspath(T5_MODEL_PATH)}\")\n",
    "# print(f\"FLUX path: {os.path.abspath(FLUX_MODEL_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9f39d8-2d37-4743-9967-645ed885ca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load T5-XXL from local folder\n",
    "print(f\"Loading T5 model from: {T5_MODEL_PATH}...\")\n",
    "\n",
    "if not os.path.exists(T5_MODEL_PATH):\n",
    "    print(\"\\n⚠️  Model not found locally. Downloading from Hugging Face...\")\n",
    "    print(\"This is a large model (~11GB) and will take several minutes.\")\n",
    "    print(\"Please be patient...\\n\")\n",
    "    \n",
    "    # Download and save to local folder\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"google/t5-v1_1-xxl\")\n",
    "    t5_model = T5EncoderModel.from_pretrained(\n",
    "        \"google/t5-v1_1-xxl\",\n",
    "        torch_dtype=torch.bfloat16  # Use bfloat16 to match FLUX\n",
    "    )\n",
    "    \n",
    "    # Save to local folder\n",
    "    print(f\"Saving model to {T5_MODEL_PATH}...\")\n",
    "    tokenizer.save_pretrained(T5_MODEL_PATH)\n",
    "    t5_model.save_pretrained(T5_MODEL_PATH)\n",
    "    print(\"✓ Model downloaded and saved locally!\\n\")\n",
    "else:\n",
    "    print(\"✓ Loading from local folder...\\n\")\n",
    "\n",
    "# Load from local folder\n",
    "tokenizer = T5Tokenizer.from_pretrained(T5_MODEL_PATH, local_files_only=True)\n",
    "t5_model = T5EncoderModel.from_pretrained(\n",
    "    T5_MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 to match FLUX\n",
    "    local_files_only=True\n",
    ").to(device)\n",
    "\n",
    "t5_model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5bcc3-c63f-4848-9110-c05eaa4548df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text input widget\n",
    "prompt_input = widgets.Textarea(\n",
    "    value='a red cat sitting on a blue table',\n",
    "    placeholder='Enter your prompt here',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='80%', height='80px')\n",
    ")\n",
    "\n",
    "generate_button = widgets.Button(\n",
    "    description='Generate Embedding',\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Global variable to store current embedding\n",
    "current_embedding = None\n",
    "current_tokens = None\n",
    "\n",
    "def generate_embedding(b):\n",
    "    global current_embedding, current_tokens\n",
    "    \n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        \n",
    "        prompt = prompt_input.value\n",
    "        print(f\"Generating embedding for: '{prompt}'\\n\")\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Get token strings for display\n",
    "        token_ids = tokens['input_ids'][0].tolist()\n",
    "        token_strings = [tokenizer.decode([tid]) for tid in token_ids]\n",
    "        \n",
    "        # Find how many real tokens (non-padding)\n",
    "        num_real_tokens = (tokens['input_ids'][0] != tokenizer.pad_token_id).sum().item()\n",
    "        \n",
    "        print(f\"Tokenized into {num_real_tokens} real tokens (+ {512 - num_real_tokens} padding):\")\n",
    "        print(\"First 10 tokens:\", token_strings[:10])\n",
    "        print()\n",
    "        \n",
    "        # Generate embedding\n",
    "        with torch.no_grad():\n",
    "            tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "            outputs = t5_model(**tokens)\n",
    "            embedding = outputs.last_hidden_state  # Shape: [1, 512, embedding_dim]\n",
    "        \n",
    "        # Convert bfloat16 to float32 before converting to numpy\n",
    "        current_embedding = embedding.float().cpu().numpy()[0]  # Shape: [512, embedding_dim]\n",
    "        current_tokens = token_strings\n",
    "        \n",
    "        embedding_dim = current_embedding.shape[1]\n",
    "        total_numbers = current_embedding.shape[0] * current_embedding.shape[1]\n",
    "        \n",
    "        print(f\"✓ Embedding generated!\")\n",
    "        print(f\"  Shape: {current_embedding.shape}\")\n",
    "        print(f\"  Total numbers: {total_numbers:,}\")\n",
    "        print(f\"  Size: {current_embedding.nbytes / 1024:.2f} KB\")\n",
    "        print()\n",
    "        print(f\"First token '{token_strings[0]}' embedding (first 10 values):\")\n",
    "        print(current_embedding[0, :10])\n",
    "\n",
    "generate_button.on_click(generate_embedding)\n",
    "\n",
    "display(prompt_input, generate_button, output_area)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flux uv",
   "language": "python",
   "name": "uv_flux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
