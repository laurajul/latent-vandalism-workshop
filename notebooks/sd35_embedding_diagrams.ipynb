{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SD 3.5 Pipeline: Understanding Embedding Flow\n",
    "\n",
    "This notebook explains how embeddings are processed and used in the Stable Diffusion 3.5 pipeline, specifically for our workshop where we're bypassing text encoding and directly injecting pre-computed embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Standard SD 3.5 Pipeline (Normal Text-to-Image)\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Text Prompt] --> B[T5-XXL Encoder]\n",
    "    A --> C[CLIP-L Encoder]\n",
    "    A --> D[CLIP-G Encoder]\n",
    "    \n",
    "    B --> E[T5 Text Embeddings<br/>77 tokens × 4096 dims]\n",
    "    C --> F[CLIP-L Text Embeddings<br/>77 tokens × 768 dims]\n",
    "    C --> G[CLIP-L Pooled<br/>Single vector: 768 dims]\n",
    "    D --> H[CLIP-G Text Embeddings<br/>77 tokens × 1280 dims]\n",
    "    D --> I[CLIP-G Pooled<br/>Single vector: 1280 dims]\n",
    "    \n",
    "    E --> J[Concatenate Text Embeddings]\n",
    "    F --> J\n",
    "    H --> J\n",
    "    \n",
    "    J --> K[Combined Context<br/>77 tokens × 6144 dims<br/>4096+768+1280]\n",
    "    \n",
    "    G --> L[Concatenate Pooled]\n",
    "    I --> L\n",
    "    \n",
    "    L --> M[Pooled Embeddings<br/>2048 dims<br/>768+1280]\n",
    "    \n",
    "    K --> N[MMDiT Transformer]\n",
    "    M --> N\n",
    "    \n",
    "    O[Random Latent Noise] --> N\n",
    "    \n",
    "    N --> P[Denoising Process<br/>Multiple Steps]\n",
    "    P --> Q[Final Latent]\n",
    "    Q --> R[VAE Decoder]\n",
    "    R --> S[Output Image]\n",
    "    \n",
    "    style K fill:#e1f5ff\n",
    "    style M fill:#ffe1f5\n",
    "    style N fill:#fff4e1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Our Modified Pipeline (Direct Embedding Injection)\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[T5 Embeddings JSON<br/>77 × 4096] --> B[Load & Parse]\n",
    "    C[CLIP-L Embeddings JSON<br/>77 × 768] --> D[Load & Parse]\n",
    "    E[CLIP-L Pooled JSON<br/>768 dims] --> F[Load & Parse]\n",
    "    G[CLIP-G Embeddings JSON<br/>77 × 1280] --> H[Load & Parse]\n",
    "    I[CLIP-G Pooled JSON<br/>1280 dims] --> J[Load & Parse]\n",
    "    \n",
    "    B --> K[Optional: Scale/Modify<br/>e.g., × 1.5, invert, etc.]\n",
    "    D --> L[Optional: Scale/Modify]\n",
    "    F --> M[Optional: Scale/Modify]\n",
    "    H --> N[Optional: Scale/Modify]\n",
    "    J --> O[Optional: Scale/Modify]\n",
    "    \n",
    "    K --> P[Concatenate Text Embeddings]\n",
    "    L --> P\n",
    "    N --> P\n",
    "    \n",
    "    P --> Q[Combined Context<br/>77 × 6144 dims]\n",
    "    \n",
    "    M --> R[Concatenate Pooled]\n",
    "    O --> R\n",
    "    \n",
    "    R --> S[Pooled Embeddings<br/>2048 dims]\n",
    "    \n",
    "    Q --> T[MMDiT Transformer]\n",
    "    S --> T\n",
    "    \n",
    "    U[Random Latent Noise] --> T\n",
    "    \n",
    "    T --> V[Denoising Process]\n",
    "    V --> W[Final Latent]\n",
    "    W --> X[VAE Decoder]\n",
    "    X --> Y[Output Image]\n",
    "    \n",
    "    style K fill:#ffcccc\n",
    "    style L fill:#ffcccc\n",
    "    style M fill:#ffcccc\n",
    "    style N fill:#ffcccc\n",
    "    style O fill:#ffcccc\n",
    "    style Q fill:#e1f5ff\n",
    "    style S fill:#ffe1f5\n",
    "    style T fill:#fff4e1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detailed View: How Embeddings Flow Into MMDiT\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    subgraph \"Text Embeddings (Context)\"\n",
    "        A[T5-XXL<br/>77 × 4096]\n",
    "        B[CLIP-L<br/>77 × 768]\n",
    "        C[CLIP-G<br/>77 × 1280]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Pooled Embeddings (Global Conditioning)\"\n",
    "        D[CLIP-L Pooled<br/>768]\n",
    "        E[CLIP-G Pooled<br/>1280]\n",
    "    end\n",
    "    \n",
    "    A --> F[Concat]\n",
    "    B --> F\n",
    "    C --> F\n",
    "    \n",
    "    F --> G[Context Vector<br/>77 × 6144]\n",
    "    \n",
    "    D --> H[Concat]\n",
    "    E --> H\n",
    "    \n",
    "    H --> I[Pooled Vector<br/>2048]\n",
    "    \n",
    "    subgraph \"MMDiT Transformer Block\"\n",
    "        G --> J[Cross-Attention<br/>Keys & Values]\n",
    "        K[Noisy Latent] --> L[Self-Attention<br/>Queries]\n",
    "        L --> M[Attention]\n",
    "        J --> M\n",
    "        M --> N[Feed Forward]\n",
    "        \n",
    "        I --> O[AdaLN Modulation<br/>Scale & Shift]\n",
    "        O --> L\n",
    "        O --> N\n",
    "    end\n",
    "    \n",
    "    N --> P[Denoised Output]\n",
    "    \n",
    "    style G fill:#e1f5ff\n",
    "    style I fill:#ffe1f5\n",
    "    style M fill:#d4edda\n",
    "    style O fill:#fff3cd\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Key Roles of Each Embedding Type\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph \"T5-XXL Embeddings\"\n",
    "        A[Rich Semantic Understanding<br/>4096 dimensions per token]\n",
    "        A --> A1[Detailed descriptions]\n",
    "        A --> A2[Complex relationships]\n",
    "        A --> A3[Nuanced concepts]\n",
    "    end\n",
    "    \n",
    "    subgraph \"CLIP-L Embeddings\"\n",
    "        B[Visual-Text Alignment<br/>768 dimensions per token]\n",
    "        B --> B1[Object recognition]\n",
    "        B --> B2[Style understanding]\n",
    "        B --> B3[Composition hints]\n",
    "    end\n",
    "    \n",
    "    subgraph \"CLIP-G Embeddings\"\n",
    "        C[High-Level Visual Concepts<br/>1280 dimensions per token]\n",
    "        C --> C1[Artistic style]\n",
    "        C --> C2[Overall aesthetics]\n",
    "        C --> C3[Global structure]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Pooled Embeddings\"\n",
    "        D[Global Image Characteristics]\n",
    "        D --> D1[Overall style modulation]\n",
    "        D --> D2[Quality/aesthetic level]\n",
    "        D --> D3[Conditioning strength]\n",
    "    end\n",
    "    \n",
    "    A1 --> E[Cross-Attention]\n",
    "    A2 --> E\n",
    "    A3 --> E\n",
    "    B1 --> E\n",
    "    B2 --> E\n",
    "    B3 --> E\n",
    "    C1 --> E\n",
    "    C2 --> E\n",
    "    C3 --> E\n",
    "    \n",
    "    D1 --> F[AdaLN Modulation]\n",
    "    D2 --> F\n",
    "    D3 --> F\n",
    "    \n",
    "    E --> G[Image Generation]\n",
    "    F --> G\n",
    "    \n",
    "    style E fill:#e1f5ff\n",
    "    style F fill:#ffe1f5\n",
    "    style G fill:#d4edda\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Workshop Experiments: What We Can Explore\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Saved Embeddings JSON] --> B{Modification Type}\n",
    "    \n",
    "    B --> C[Scaling]\n",
    "    B --> D[Inversion]\n",
    "    B --> E[Mixing]\n",
    "    B --> F[Zeroing]\n",
    "    B --> G[Interpolation]\n",
    "    \n",
    "    C --> C1[T5 × 1.5, CLIP × 0.5]\n",
    "    D --> D1[Negative embeddings]\n",
    "    E --> E1[Blend different prompt embeddings]\n",
    "    F --> F1[Remove specific encoder influence]\n",
    "    G --> G1[Morph between concepts]\n",
    "    \n",
    "    C1 --> H[Modified Embeddings]\n",
    "    D1 --> H\n",
    "    E1 --> H\n",
    "    F1 --> H\n",
    "    G1 --> H\n",
    "    \n",
    "    H --> I[Inject into Pipeline]\n",
    "    I --> J[Generate Image]\n",
    "    J --> K{Results}\n",
    "    \n",
    "    K --> L[Impossible from text alone]\n",
    "    K --> M[Novel visual combinations]\n",
    "    K --> N[Understanding encoder roles]\n",
    "    \n",
    "    style H fill:#ffcccc\n",
    "    style J fill:#d4edda\n",
    "    style L fill:#fff4e1\n",
    "    style M fill:#fff4e1\n",
    "    style N fill:#fff4e1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Embedding Dimensions:\n",
    "- **T5-XXL**: 77 tokens × 4096 dimensions\n",
    "- **CLIP-L**: 77 tokens × 768 dimensions + 768 pooled\n",
    "- **CLIP-G**: 77 tokens × 1280 dimensions + 1280 pooled\n",
    "\n",
    "### How They're Used:\n",
    "1. **Text embeddings** (77 × 6144 combined) → Cross-attention in MMDiT\n",
    "2. **Pooled embeddings** (2048 combined) → AdaLN modulation in MMDiT\n",
    "\n",
    "### Workshop Advantages:\n",
    "- Direct manipulation of semantic space\n",
    "- Bypass text encoding limitations\n",
    "- Create impossible-to-prompt images\n",
    "- Understand individual encoder contributions\n",
    "- Discover emergent behaviors through raw embedding manipulation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
