{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 Embedding Manipulation for FLUX\n",
    "\n",
    "This notebook lets you:\n",
    "1. Generate T5 embeddings from text prompts\n",
    "2. Save/load embeddings as JSON\n",
    "3. Manually edit embedding values\n",
    "4. Apply custom attention masks\n",
    "5. Generate images with FLUX using modified embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Run this cell first to install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch diffusers accelerate sentencepiece protobuf ipywidgets numpy pillow --break-system-packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "from diffusers import FluxPipeline\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Image as IPImage\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load T5 Model (Small Version)\n",
    "\n",
    "We'll use `google/t5-v1_1-small` which is much smaller than T5-XXL but compatible for experimentation.\n",
    "\n",
    "**Note:** FLUX officially uses T5-XXL, but for learning purposes, T5-small works. The embedding dimension will be 512 instead of 4096."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load T5-small (for experimentation - faster and smaller)\n",
    "# For production use: \"google/t5-v1_1-xxl\" but it's 11GB+\n",
    "t5_model_name = \"google/t5-v1_1-small\"  # 242MB, embedding_dim=512\n",
    "\n",
    "print(f\"Loading T5 model: {t5_model_name}...\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
    "t5_model = T5EncoderModel.from_pretrained(t5_model_name).to(device)\n",
    "t5_model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"T5 loaded! Embedding dimension: {t5_model.config.d_model}\")\n",
    "print(f\"Max sequence length: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Input Widget and Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text input widget\n",
    "prompt_input = widgets.Textarea(\n",
    "    value='a red cat sitting on a blue table',\n",
    "    placeholder='Enter your prompt here',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='80%', height='80px')\n",
    ")\n",
    "\n",
    "generate_button = widgets.Button(\n",
    "    description='Generate Embedding',\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Global variable to store current embedding\n",
    "current_embedding = None\n",
    "current_tokens = None\n",
    "\n",
    "def generate_embedding(b):\n",
    "    global current_embedding, current_tokens\n",
    "    \n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        \n",
    "        prompt = prompt_input.value\n",
    "        print(f\"Generating embedding for: '{prompt}'\\n\")\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Get token strings for display\n",
    "        token_ids = tokens['input_ids'][0].tolist()\n",
    "        token_strings = [tokenizer.decode([tid]) for tid in token_ids]\n",
    "        \n",
    "        # Find how many real tokens (non-padding)\n",
    "        num_real_tokens = (tokens['input_ids'][0] != tokenizer.pad_token_id).sum().item()\n",
    "        \n",
    "        print(f\"Tokenized into {num_real_tokens} real tokens (+ {512 - num_real_tokens} padding):\")\n",
    "        print(\"First 10 tokens:\", token_strings[:10])\n",
    "        print()\n",
    "        \n",
    "        # Generate embedding\n",
    "        with torch.no_grad():\n",
    "            tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "            outputs = t5_model(**tokens)\n",
    "            embedding = outputs.last_hidden_state  # Shape: [1, 512, embedding_dim]\n",
    "        \n",
    "        current_embedding = embedding.cpu().numpy()[0]  # Shape: [512, embedding_dim]\n",
    "        current_tokens = token_strings\n",
    "        \n",
    "        embedding_dim = current_embedding.shape[1]\n",
    "        total_numbers = current_embedding.shape[0] * current_embedding.shape[1]\n",
    "        \n",
    "        print(f\"✓ Embedding generated!\")\n",
    "        print(f\"  Shape: {current_embedding.shape}\")\n",
    "        print(f\"  Total numbers: {total_numbers:,}\")\n",
    "        print(f\"  Size: {current_embedding.nbytes / 1024:.2f} KB\")\n",
    "        print()\n",
    "        print(f\"First token '{token_strings[0]}' embedding (first 10 values):\")\n",
    "        print(current_embedding[0, :10])\n",
    "\n",
    "generate_button.on_click(generate_embedding)\n",
    "\n",
    "display(prompt_input, generate_button, output_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save Embedding to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embedding(filename=\"embedding.json\"):\n",
    "    if current_embedding is None:\n",
    "        print(\"❌ No embedding to save! Generate one first.\")\n",
    "        return\n",
    "    \n",
    "    data = {\n",
    "        \"embedding\": current_embedding.tolist(),\n",
    "        \"tokens\": current_tokens,\n",
    "        \"shape\": list(current_embedding.shape),\n",
    "        \"prompt\": prompt_input.value\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    \n",
    "    file_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "    print(f\"✓ Embedding saved to '{filename}' ({file_size:.2f} MB)\")\n",
    "\n",
    "# Save button\n",
    "save_button = widgets.Button(description='Save Embedding', button_style='info')\n",
    "save_output = widgets.Output()\n",
    "\n",
    "def on_save_click(b):\n",
    "    with save_output:\n",
    "        save_output.clear_output()\n",
    "        save_embedding()\n",
    "\n",
    "save_button.on_click(on_save_click)\n",
    "display(save_button, save_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Embedding from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(filename=\"embedding.json\"):\n",
    "    global current_embedding, current_tokens\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    current_embedding = np.array(data['embedding'])\n",
    "    current_tokens = data['tokens']\n",
    "    \n",
    "    print(f\"✓ Embedding loaded from '{filename}'\")\n",
    "    print(f\"  Original prompt: {data['prompt']}\")\n",
    "    print(f\"  Shape: {current_embedding.shape}\")\n",
    "    print(f\"  First token: '{current_tokens[0]}'\")\n",
    "\n",
    "# Load button\n",
    "load_button = widgets.Button(description='Load Embedding', button_style='warning')\n",
    "load_output = widgets.Output()\n",
    "\n",
    "def on_load_click(b):\n",
    "    with load_output:\n",
    "        load_output.clear_output()\n",
    "        try:\n",
    "            load_embedding()\n",
    "        except FileNotFoundError:\n",
    "            print(\"❌ File 'embedding.json' not found. Save an embedding first.\")\n",
    "\n",
    "load_button.on_click(on_load_click)\n",
    "display(load_button, load_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Manual Embedding Manipulation\n",
    "\n",
    "Example: Zero out a percentage of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manipulate_embedding(zero_percentage=0.3):\n",
    "    global current_embedding\n",
    "    \n",
    "    if current_embedding is None:\n",
    "        print(\"❌ No embedding loaded!\")\n",
    "        return\n",
    "    \n",
    "    # Create a copy\n",
    "    modified = current_embedding.copy()\n",
    "    \n",
    "    # Randomly zero out a percentage of values\n",
    "    total_values = modified.size\n",
    "    num_zeros = int(total_values * zero_percentage)\n",
    "    \n",
    "    # Random indices\n",
    "    flat_modified = modified.flatten()\n",
    "    zero_indices = np.random.choice(total_values, num_zeros, replace=False)\n",
    "    flat_modified[zero_indices] = 0.0\n",
    "    \n",
    "    modified = flat_modified.reshape(current_embedding.shape)\n",
    "    \n",
    "    print(f\"✓ Zeroed out {num_zeros:,} values ({zero_percentage*100}%)\")\n",
    "    print(f\"  Original non-zero: {np.count_nonzero(current_embedding):,}\")\n",
    "    print(f\"  Modified non-zero: {np.count_nonzero(modified):,}\")\n",
    "    \n",
    "    return modified\n",
    "\n",
    "# Manipulation widget\n",
    "zero_slider = widgets.FloatSlider(\n",
    "    value=0.3,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.05,\n",
    "    description='Zero %:',\n",
    "    readout_format='.0%'\n",
    ")\n",
    "\n",
    "manipulate_button = widgets.Button(description='Apply Manipulation', button_style='danger')\n",
    "manipulate_output = widgets.Output()\n",
    "\n",
    "modified_embedding = None\n",
    "\n",
    "def on_manipulate_click(b):\n",
    "    global modified_embedding\n",
    "    with manipulate_output:\n",
    "        manipulate_output.clear_output()\n",
    "        modified_embedding = manipulate_embedding(zero_slider.value)\n",
    "\n",
    "manipulate_button.on_click(on_manipulate_click)\n",
    "display(zero_slider, manipulate_button, manipulate_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Manual Attention Masking\n",
    "\n",
    "Control which tokens get how much attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display tokens and create sliders\n",
    "def create_attention_sliders(num_tokens=10):\n",
    "    if current_tokens is None:\n",
    "        print(\"❌ Generate an embedding first!\")\n",
    "        return None\n",
    "    \n",
    "    # Find non-padding tokens\n",
    "    real_tokens = []\n",
    "    for i, token in enumerate(current_tokens):\n",
    "        if token.strip() and token != '<pad>' and i < num_tokens:\n",
    "            real_tokens.append((i, token))\n",
    "    \n",
    "    print(f\"Creating sliders for first {len(real_tokens)} tokens:\")\n",
    "    print(real_tokens)\n",
    "    print()\n",
    "    \n",
    "    sliders = []\n",
    "    for idx, token in real_tokens:\n",
    "        slider = widgets.FloatSlider(\n",
    "            value=1.0 / len(real_tokens),  # Equal distribution\n",
    "            min=0.0,\n",
    "            max=1.0,\n",
    "            step=0.01,\n",
    "            description=f'{idx}: {token[:15]}',\n",
    "            readout_format='.0%',\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "        sliders.append(slider)\n",
    "    \n",
    "    # Normalize button\n",
    "    normalize_button = widgets.Button(\n",
    "        description='Normalize to 100%',\n",
    "        button_style='info'\n",
    "    )\n",
    "    \n",
    "    total_label = widgets.Label(value='Total: 100%')\n",
    "    \n",
    "    def update_total(*args):\n",
    "        total = sum(s.value for s in sliders)\n",
    "        total_label.value = f'Total: {total*100:.1f}%'\n",
    "        if abs(total - 1.0) > 0.01:\n",
    "            total_label.value += ' ⚠️ Should sum to 100%'\n",
    "    \n",
    "    def normalize(*args):\n",
    "        total = sum(s.value for s in sliders)\n",
    "        if total > 0:\n",
    "            for s in sliders:\n",
    "                s.value = s.value / total\n",
    "        update_total()\n",
    "    \n",
    "    for slider in sliders:\n",
    "        slider.observe(update_total, 'value')\n",
    "    \n",
    "    normalize_button.on_click(normalize)\n",
    "    \n",
    "    update_total()\n",
    "    \n",
    "    return sliders, normalize_button, total_label, real_tokens\n",
    "\n",
    "attention_controls = create_attention_sliders(num_tokens=10)\n",
    "\n",
    "if attention_controls:\n",
    "    sliders, normalize_btn, total_lbl, real_tokens = attention_controls\n",
    "    display(widgets.VBox(sliders + [normalize_btn, total_lbl]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Attention Mask Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_mask():\n",
    "    if attention_controls is None:\n",
    "        print(\"❌ Create attention sliders first!\")\n",
    "        return None\n",
    "    \n",
    "    sliders, _, _, real_tokens = attention_controls\n",
    "    \n",
    "    # Create mask array (512 tokens)\n",
    "    mask = np.zeros(512)\n",
    "    \n",
    "    # Set weights from sliders\n",
    "    for slider, (idx, token) in zip(sliders, real_tokens):\n",
    "        mask[idx] = slider.value\n",
    "    \n",
    "    print(\"Attention Mask created:\")\n",
    "    print(f\"Non-zero weights: {np.count_nonzero(mask)}\")\n",
    "    for slider, (idx, token) in zip(sliders, real_tokens):\n",
    "        print(f\"  Token {idx} '{token}': {slider.value*100:.1f}%\")\n",
    "    \n",
    "    return mask\n",
    "\n",
    "mask_button = widgets.Button(description='Create Mask', button_style='success')\n",
    "mask_output = widgets.Output()\n",
    "\n",
    "attention_mask = None\n",
    "\n",
    "def on_mask_click(b):\n",
    "    global attention_mask\n",
    "    with mask_output:\n",
    "        mask_output.clear_output()\n",
    "        attention_mask = create_attention_mask()\n",
    "\n",
    "mask_button.on_click(on_mask_click)\n",
    "display(mask_button, mask_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load FLUX Model\n",
    "\n",
    "**Warning:** FLUX-schnell is still large (~24GB). This will take time and require significant VRAM/RAM.\n",
    "\n",
    "If you don't have enough resources, skip this section and the embedding/mask experiments above are still valuable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FLUX-schnell (smallest FLUX variant)\n",
    "print(\"Loading FLUX-schnell... This will take several minutes and requires ~24GB download.\")\n",
    "print(\"If you don't have enough resources, you can skip this cell.\")\n",
    "\n",
    "try:\n",
    "    flux_pipe = FluxPipeline.from_pretrained(\n",
    "        \"black-forest-labs/FLUX.1-schnell\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    flux_pipe = flux_pipe.to(device)\n",
    "    print(\"✓ FLUX loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading FLUX: {e}\")\n",
    "    print(\"You may need to:\")\n",
    "    print(\"1. Accept the license at https://huggingface.co/black-forest-labs/FLUX.1-schnell\")\n",
    "    print(\"2. Login with: huggingface-cli login\")\n",
    "    print(\"3. Have enough disk space (~24GB) and RAM/VRAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Image with Custom Embedding and Mask\n",
    "\n",
    "**Note:** This is a simplified version. Full integration would require modifying FLUX's pipeline to accept custom embeddings and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_simple(prompt_text, use_mask=False):\n",
    "    \"\"\"\n",
    "    Simple generation using text prompt.\n",
    "    For custom embeddings, you'd need to modify the pipeline.\n",
    "    \"\"\"\n",
    "    if 'flux_pipe' not in globals():\n",
    "        print(\"❌ FLUX not loaded!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Generating image for: '{prompt_text}'\")\n",
    "    \n",
    "    if use_mask and attention_mask is not None:\n",
    "        print(\"Note: Custom masking requires pipeline modification.\")\n",
    "        print(\"Generating with standard pipeline for now...\")\n",
    "    \n",
    "    image = flux_pipe(\n",
    "        prompt=prompt_text,\n",
    "        num_inference_steps=4,  # Schnell is optimized for 4 steps\n",
    "        guidance_scale=0.0,  # Schnell doesn't use guidance\n",
    "    ).images[0]\n",
    "    \n",
    "    output_path = \"generated_image.png\"\n",
    "    image.save(output_path)\n",
    "    print(f\"✓ Image saved to '{output_path}'\")\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Generate button\n",
    "generate_image_button = widgets.Button(\n",
    "    description='Generate Image',\n",
    "    button_style='primary'\n",
    ")\n",
    "image_output = widgets.Output()\n",
    "\n",
    "def on_generate_image_click(b):\n",
    "    with image_output:\n",
    "        image_output.clear_output(wait=True)\n",
    "        image = generate_image_simple(prompt_input.value, use_mask=False)\n",
    "        if image:\n",
    "            display(image)\n",
    "\n",
    "generate_image_button.on_click(on_generate_image_click)\n",
    "display(generate_image_button, image_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced: Custom Pipeline Integration (Template)\n",
    "\n",
    "To truly use custom embeddings and masks, you'd need to modify the FLUX pipeline. Here's a template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a template showing where you'd inject custom embeddings\n",
    "# Full implementation requires deep diving into the diffusers library\n",
    "\n",
    "\"\"\"\n",
    "def generate_with_custom_embedding(embedding, attention_mask=None):\n",
    "    # 1. Convert numpy embedding to torch tensor\n",
    "    embedding_tensor = torch.from_numpy(embedding).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 2. If you have attention_mask, convert it too\n",
    "    if attention_mask is not None:\n",
    "        mask_tensor = torch.from_numpy(attention_mask).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 3. You'd need to modify flux_pipe's forward pass to:\n",
    "    #    - Skip text encoding\n",
    "    #    - Use your embedding_tensor directly\n",
    "    #    - Apply mask_tensor in cross-attention layers\n",
    "    \n",
    "    # This requires:\n",
    "    # - Accessing flux_pipe.transformer\n",
    "    # - Hooking into cross-attention layers\n",
    "    # - Replacing attention weights\n",
    "    \n",
    "    pass\n",
    "\"\"\"\n",
    "\n",
    "print(\"Custom embedding integration requires modifying the diffusers pipeline.\")\n",
    "print(\"This is an advanced topic - consider exploring:\")\n",
    "print(\"1. diffusers library source code\")\n",
    "print(\"2. Custom pipeline examples in diffusers documentation\")\n",
    "print(\"3. Attention manipulation techniques (Prompt-to-Prompt, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "What you've learned:\n",
    "\n",
    "1. ✓ Generate T5 embeddings from text (512 tokens × 512 dimensions = 262,144 numbers)\n",
    "2. ✓ Save/load embeddings as JSON files\n",
    "3. ✓ Manually manipulate embedding values\n",
    "4. ✓ Create custom attention masks with sliders\n",
    "5. ✓ Understand the pipeline: text → T5 → embeddings → FLUX → image\n",
    "\n",
    "Next steps:\n",
    "- Experiment with zeroing different percentages of values\n",
    "- Try extreme attention masks (90% on one token)\n",
    "- Explore the diffusers library to implement custom pipeline integration\n",
    "- Compare images with/without modifications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
