{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7971246",
   "metadata": {},
   "source": [
    "# T5-XXL Embeddings for FLUX\n",
    "\n",
    "This notebook generates **T5-XXL text embeddings** for use with FLUX image generation.\n",
    "\n",
    "- **Model**: Google T5-v1.1-XXL encoder\n",
    "- **Embedding dimension**: 4096\n",
    "- **Sequence length**: 512 tokens\n",
    "- **Output shape**: [512, 4096]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1in794n1bh",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "    T[Text Prompt]\n",
    "    \n",
    "    TOK[T5 Tokenizer]\n",
    "    ENC[T5-XXL Encoder]\n",
    "    \n",
    "    EMB[Text Embedding<br/>512 √ó 4096]\n",
    "    \n",
    "    FLUX[FLUX<br/>Diffusion Transformer]\n",
    "    \n",
    "    T --> TOK --> ENC --> EMB\n",
    "    EMB -->|sequence conditioning| FLUX\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f06314-ec5f-4bc5-981a-c7784365609d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T11:41:24.567670Z",
     "iopub.status.busy": "2026-01-15T11:41:24.567470Z",
     "iopub.status.idle": "2026-01-15T11:41:28.779564Z",
     "shell.execute_reply": "2026-01-15T11:41:28.779018Z",
     "shell.execute_reply.started": "2026-01-15T11:41:24.567655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import os\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "#from diffusers import FluxPipeline\n",
    "#from IPython.display import display, Image as IPImage\n",
    "from PIL import Image\n",
    "# Set device\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load models path from config\n",
    "current_dir = Path.cwd()\n",
    "models_path_file = current_dir.parent / \"misc/paths/models.txt\"\n",
    "with open(models_path_file, 'r') as f:\n",
    "    models_path = f.read().strip()\n",
    "MODELS_DIR = current_dir.parent / models_path\n",
    "T5_MODEL_PATH = os.path.join(MODELS_DIR, \"t5-v1_1-xxl\")\n",
    "# os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "# print(f\"Models directory: {os.path.abspath(MODELS_DIR)}\")\n",
    "# print(f\"T5 path: {os.path.abspath(T5_MODEL_PATH)}\")\n",
    "# print(f\"FLUX path: {os.path.abspath(FLUX_MODEL_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc9f39d8-2d37-4743-9967-645ed885ca07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T11:41:50.253509Z",
     "iopub.status.busy": "2026-01-15T11:41:50.253202Z",
     "iopub.status.idle": "2026-01-15T11:42:36.729245Z",
     "shell.execute_reply": "2026-01-15T11:42:36.728612Z",
     "shell.execute_reply.started": "2026-01-15T11:41:50.253494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading T5 model from: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/data/models/t5-v1_1-xxl...\n",
      "‚úì Loading from local folder...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061e632bb6ac454d89e4df8746eb11b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "T5EncoderModel(\n",
       "  (shared): Embedding(32128, 4096)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 4096)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 64)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load T5-XXL from local folder\n",
    "print(f\"Loading T5 model from: {T5_MODEL_PATH}...\")\n",
    "\n",
    "if not os.path.exists(T5_MODEL_PATH):\n",
    "    print(\"\\n‚ö†Ô∏è  Model not found locally. Downloading from Hugging Face...\")\n",
    "    print(\"This is a large model (~11GB) and will take several minutes.\")\n",
    "    print(\"Please be patient...\\n\")\n",
    "    \n",
    "    # Download and save to local folder\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"google/t5-v1_1-xxl\")\n",
    "    t5_model = T5EncoderModel.from_pretrained(\n",
    "        \"google/t5-v1_1-xxl\",\n",
    "        torch_dtype=torch.bfloat16  # Use bfloat16 to match FLUX\n",
    "    )\n",
    "    \n",
    "    # Save to local folder\n",
    "    print(f\"Saving model to {T5_MODEL_PATH}...\")\n",
    "    tokenizer.save_pretrained(T5_MODEL_PATH)\n",
    "    t5_model.save_pretrained(T5_MODEL_PATH)\n",
    "    print(\"‚úì Model downloaded and saved locally!\\n\")\n",
    "else:\n",
    "    print(\"‚úì Loading from local folder...\\n\")\n",
    "\n",
    "# Load from local folder\n",
    "tokenizer = T5Tokenizer.from_pretrained(T5_MODEL_PATH, local_files_only=True)\n",
    "t5_model = T5EncoderModel.from_pretrained(\n",
    "    T5_MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 to match FLUX\n",
    "    local_files_only=True\n",
    ").to(device)\n",
    "\n",
    "t5_model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ad5bcc3-c63f-4848-9110-c05eaa4548df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T11:42:36.729997Z",
     "iopub.status.busy": "2026-01-15T11:42:36.729847Z",
     "iopub.status.idle": "2026-01-15T11:42:36.742365Z",
     "shell.execute_reply": "2026-01-15T11:42:36.741886Z",
     "shell.execute_reply.started": "2026-01-15T11:42:36.729981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5dbc9119f1474e9fff9f2908e5f349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='a red cat sitting on a blue table', description='Prompt:', layout=Layout(height='80px', width=‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff694c3cabd4316b8597bb8619ce8dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Generate Embedding', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c982db83cef24ee2a5cdab2c8578d1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create text input widget\n",
    "prompt_input = widgets.Textarea(\n",
    "    value='a red cat sitting on a blue table',\n",
    "    placeholder='Enter your prompt here',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='80%', height='80px')\n",
    ")\n",
    "\n",
    "generate_button = widgets.Button(\n",
    "    description='Generate Embedding',\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Global variable to store current embedding\n",
    "current_embedding = None\n",
    "current_tokens = None\n",
    "\n",
    "def generate_embedding(b):\n",
    "    global current_embedding, current_tokens\n",
    "    \n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        \n",
    "        prompt = prompt_input.value\n",
    "        print(f\"Generating embedding for: '{prompt}'\\n\")\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Get token strings for display\n",
    "        token_ids = tokens['input_ids'][0].tolist()\n",
    "        token_strings = [tokenizer.decode([tid]) for tid in token_ids]\n",
    "        \n",
    "        # Find how many real tokens (non-padding)\n",
    "        num_real_tokens = (tokens['input_ids'][0] != tokenizer.pad_token_id).sum().item()\n",
    "        \n",
    "        print(f\"Tokenized into {num_real_tokens} real tokens (+ {512 - num_real_tokens} padding):\")\n",
    "        print(\"First 10 tokens:\", token_strings[:10])\n",
    "        print()\n",
    "        \n",
    "        # Generate embedding\n",
    "        with torch.no_grad():\n",
    "            tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "            outputs = t5_model(**tokens)\n",
    "            embedding = outputs.last_hidden_state  # Shape: [1, 512, embedding_dim]\n",
    "        \n",
    "        # Convert bfloat16 to float32 before converting to numpy\n",
    "        current_embedding = embedding.float().cpu().numpy()[0]  # Shape: [512, embedding_dim]\n",
    "        current_tokens = token_strings\n",
    "        \n",
    "        embedding_dim = current_embedding.shape[1]\n",
    "        total_numbers = current_embedding.shape[0] * current_embedding.shape[1]\n",
    "        \n",
    "        print(f\"‚úì Embedding generated!\")\n",
    "        print(f\"  Shape: {current_embedding.shape}\")\n",
    "        print(f\"  Total numbers: {total_numbers:,}\")\n",
    "        print(f\"  Size: {current_embedding.nbytes / 1024:.2f} KB\")\n",
    "        print()\n",
    "        print(f\"First token '{token_strings[0]}' embedding (first 10 values):\")\n",
    "        print(current_embedding[0, :10])\n",
    "\n",
    "generate_button.on_click(generate_embedding)\n",
    "\n",
    "display(prompt_input, generate_button, output_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d5f15b4-33e2-4b82-8951-b2f650454a05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T11:42:36.742833Z",
     "iopub.status.busy": "2026-01-15T11:42:36.742710Z",
     "iopub.status.idle": "2026-01-15T11:42:36.754555Z",
     "shell.execute_reply": "2026-01-15T11:42:36.754157Z",
     "shell.execute_reply.started": "2026-01-15T11:42:36.742820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10a3457482d4a1fb6ceaf19aab71020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Save Embedding', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109c92668a4549b79df3c608f34c75ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save embedding to file\n",
    "EMBEDDINGS_DIR = current_dir.parent / \"data/embeddings/T5\"\n",
    "os.makedirs(EMBEDDINGS_DIR, exist_ok=True)\n",
    "\n",
    "save_button = widgets.Button(\n",
    "    description='Save Embedding',\n",
    "    button_style='primary'\n",
    ")\n",
    "\n",
    "save_output = widgets.Output()\n",
    "\n",
    "def save_embedding(b):\n",
    "    with save_output:\n",
    "        save_output.clear_output()\n",
    "        \n",
    "        if current_embedding is None:\n",
    "            print(\"‚ùå No embedding to save! Generate an embedding first.\")\n",
    "            return\n",
    "        \n",
    "        # Get first 4 non-padding tokens (excluding special tokens)\n",
    "        filename_tokens = []\n",
    "        for token in current_tokens:\n",
    "            # Skip padding tokens and clean up special characters\n",
    "            cleaned_token = token.strip().replace('‚ñÅ', '').replace('</s>', '')\n",
    "            if cleaned_token and cleaned_token != '<pad>':\n",
    "                filename_tokens.append(cleaned_token)\n",
    "            if len(filename_tokens) >= 4:\n",
    "                break\n",
    "        \n",
    "        # Create filename from first 4 tokens\n",
    "        filename = \"_\".join(filename_tokens) + \".json\"\n",
    "        filepath = EMBEDDINGS_DIR / filename\n",
    "        \n",
    "        # Save embedding\n",
    "        embedding_data = {\n",
    "            \"prompt\": prompt_input.value,\n",
    "            \"embedding\": current_embedding.tolist(),\n",
    "            \"shape\": list(current_embedding.shape)\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(embedding_data, f)\n",
    "        \n",
    "        print(f\"‚úì Embedding saved to:\")\n",
    "        print(f\"  {filepath}\")\n",
    "        print(f\"  Size: {os.path.getsize(filepath) / 1024:.2f} KB\")\n",
    "\n",
    "save_button.on_click(save_embedding)\n",
    "\n",
    "display(save_button, save_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8e5d96a-8a04-4069-807d-1269468cd061",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T11:44:03.191482Z",
     "iopub.status.busy": "2026-01-15T11:44:03.191283Z",
     "iopub.status.idle": "2026-01-15T11:44:03.207774Z",
     "shell.execute_reply": "2026-01-15T11:44:03.207326Z",
     "shell.execute_reply.started": "2026-01-15T11:44:03.191466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Batch T5 Embedding Generator - File-Based\n",
      "======================================================================\n",
      "üìÑ Reading prompts from: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/misc/example_prompts.txt\n",
      "üíæ Output directory: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/data/embeddings/T5/examples\n",
      "\n",
      "Select prompt type and click 'Batch Generate & Save'\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1028dc70b3a475ba8bf2734f734662e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Prompt Type:', options=('short', 't5xxl', 'clip'), style=DescriptionStyle(description_wi‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324530cab5a14d4f95435a603e962028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='warning', description='Batch Generate & Save', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695bb9c6e03e408f8cdd911d34759195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Batch generate embeddings for multiple prompts\n",
    "BATCH_DIR = EMBEDDINGS_DIR / \"examples\"\n",
    "os.makedirs(BATCH_DIR, exist_ok=True)\n",
    "\n",
    "# Path to prompts file\n",
    "current_dir = Path.cwd()\n",
    "prompts_file = current_dir.parent / \"misc/example_prompts.txt\"\n",
    "\n",
    "# Create dropdown for prompt type selection\n",
    "prompt_type_selector = widgets.Dropdown(\n",
    "    options=['short', 't5xxl', 'clip'],\n",
    "    value='short',\n",
    "    description='Prompt Type:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "batch_generate_button = widgets.Button(\n",
    "    description='Batch Generate & Save',\n",
    "    button_style='warning'\n",
    ")\n",
    "\n",
    "batch_output_area = widgets.Output()\n",
    "\n",
    "def load_prompts_from_file(filepath, prompt_type='short'):\n",
    "    \"\"\"\n",
    "    Load prompts from text file based on selected type.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the prompts file\n",
    "        prompt_type: 'short', 't5xxl', or 'clip'\n",
    "    \n",
    "    Returns:\n",
    "        List of prompts\n",
    "    \"\"\"\n",
    "    if not filepath.exists():\n",
    "        return []\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split by section headers\n",
    "    sections = content.split('#')\n",
    "    \n",
    "    prompts = []\n",
    "    target_section = None\n",
    "    \n",
    "    # Find the right section\n",
    "    for section in sections:\n",
    "        if not section.strip():\n",
    "            continue\n",
    "            \n",
    "        if prompt_type == 'short' and 'Short prompts' in section:\n",
    "            target_section = section\n",
    "            break\n",
    "        elif prompt_type == 't5xxl' and 'T5-XXL prompts' in section:\n",
    "            target_section = section\n",
    "            break\n",
    "        elif prompt_type == 'clip' and 'CLIP prompts' in section:\n",
    "            target_section = section\n",
    "            break\n",
    "    \n",
    "    if target_section:\n",
    "        # Extract prompts (non-empty lines that don't start with #)\n",
    "        lines = target_section.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#'):\n",
    "                prompts.append(line)\n",
    "    \n",
    "    return prompts\n",
    "\n",
    "def batch_generate_embeddings(b):\n",
    "    with batch_output_area:\n",
    "        batch_output_area.clear_output()\n",
    "        \n",
    "        # Load prompts from file\n",
    "        prompt_type = prompt_type_selector.value\n",
    "        prompts = load_prompts_from_file(prompts_file, prompt_type)\n",
    "        \n",
    "        if not prompts:\n",
    "            print(f\"‚ùå No prompts found in {prompts_file} for type '{prompt_type}'!\")\n",
    "            print(f\"Expected file location: {prompts_file}\")\n",
    "            return\n",
    "        \n",
    "        # Limit to 10 prompts\n",
    "        prompts = prompts[:10]\n",
    "        \n",
    "        print(f\"üìÅ Loaded {len(prompts)} prompts from file (type: {prompt_type})\")\n",
    "        print(f\"Generating T5 embeddings...\\n\")\n",
    "        \n",
    "        for i, prompt in enumerate(prompts, 1):\n",
    "            print(f\"[{i}/{len(prompts)}] '{prompt[:60]}{'...' if len(prompt) > 60 else ''}'\")\n",
    "            \n",
    "            # Tokenize\n",
    "            tokens = tokenizer(\n",
    "                prompt,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Get token strings for filename\n",
    "            token_ids = tokens['input_ids'][0].tolist()\n",
    "            token_strings = [tokenizer.decode([tid]) for tid in token_ids]\n",
    "            \n",
    "            # Generate embedding\n",
    "            with torch.no_grad():\n",
    "                tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "                outputs = t5_model(**tokens)\n",
    "                embedding = outputs.last_hidden_state.float().cpu().numpy()[0]\n",
    "            \n",
    "            # Create filename from first 4 meaningful tokens\n",
    "            filename_tokens = []\n",
    "            for token in token_strings:\n",
    "                cleaned = token.strip().replace('‚ñÅ', '').replace('</s>', '')\n",
    "                if cleaned and cleaned != '<pad>':\n",
    "                    filename_tokens.append(cleaned)\n",
    "                if len(filename_tokens) >= 4:\n",
    "                    break\n",
    "            \n",
    "            # Add prompt type suffix to distinguish files\n",
    "            filename = \"_\".join(filename_tokens) + f\"_{prompt_type}.json\"\n",
    "            filepath = BATCH_DIR / filename\n",
    "            \n",
    "            # Save embedding\n",
    "            embedding_data = {\n",
    "                \"prompt\": prompt,\n",
    "                \"prompt_type\": prompt_type,\n",
    "                \"embedding\": embedding.tolist(),\n",
    "                \"shape\": list(embedding.shape)\n",
    "            }\n",
    "            \n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(embedding_data, f)\n",
    "            \n",
    "            print(f\"   ‚úì Saved: {filename}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ All {len(prompts)} embeddings saved to:\")\n",
    "        print(f\"   {BATCH_DIR}\")\n",
    "\n",
    "batch_generate_button.on_click(batch_generate_embeddings)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Batch T5 Embedding Generator - File-Based\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üìÑ Reading prompts from: {prompts_file}\")\n",
    "print(f\"üíæ Output directory: {BATCH_DIR}\")\n",
    "print(\"\\nSelect prompt type and click 'Batch Generate & Save'\\n\")\n",
    "\n",
    "display(prompt_type_selector, batch_generate_button, batch_output_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1xtry2p1cxy",
   "metadata": {},
   "source": [
    "---\n",
    "<sub>Latent Vandalism Workshop ‚Ä¢ Laura Wagner, 2026 ‚Ä¢ [laurajul.github.io](https://laurajul.github.io/)</sub>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flux uv",
   "language": "python",
   "name": "uv_flux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
