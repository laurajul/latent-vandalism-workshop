{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# T5 & CLIP Embedding Manipulation for Stable Diffusion 3 Medium\n\nThis notebook lets you:\n1. Load manipulated **T5 embeddings** (positive and negative)\n2. Load manipulated **CLIP-L + CLIP-G combined embeddings** (positive and negative)\n3. Generate images with SD3 Medium using custom embeddings\n4. Control guidance scale for positive/negative balance\n5. Compare results with FLUX\n\n**Model Used:** `stabilityai/stable-diffusion-3-medium-diffusers`\n\n**Architecture:** SD3 uses 3 text encoders:\n- **T5-XXL** [512, 4096] - You can use custom manipulated embeddings\n- **CLIP-L** [768-dim] + **CLIP-G** [1280-dim] = **Combined [2048-dim]** - You can use custom manipulated embeddings OR auto-generate from prompt\n\n**CLIP Generation:** Use `003_CLIP_L_and_G_embeddings.ipynb` to create CLIP embeddings"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:15:26.861516Z",
     "iopub.status.busy": "2026-01-14T13:15:26.861352Z",
     "iopub.status.idle": "2026-01-14T13:15:26.866263Z",
     "shell.execute_reply": "2026-01-14T13:15:26.865643Z",
     "shell.execute_reply.started": "2026-01-14T13:15:26.861503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Models directory: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/data/models\n",
      "SD3 Medium TensorRT path: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/data/models/stable-diffusion-3-medium-tensorrt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Image as IPImage\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create models directory\n",
    "current_dir = Path.cwd()\n",
    "MODELS_DIR = current_dir.parent / \"data/models\"\n",
    "SD3_MODEL_PATH = os.path.join(MODELS_DIR, \"stable-diffusion-3-medium-tensorrt\")\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "print(f\"Models directory: {os.path.abspath(MODELS_DIR)}\")\n",
    "print(f\"SD3 Medium TensorRT path: {os.path.abspath(SD3_MODEL_PATH)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Download Stable Diffusion 3 Medium from Hugging Face"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:15:28.906539Z",
     "iopub.status.busy": "2026-01-14T13:15:28.906403Z",
     "iopub.status.idle": "2026-01-14T13:15:29.040258Z",
     "shell.execute_reply": "2026-01-14T13:15:29.039693Z",
     "shell.execute_reply.started": "2026-01-14T13:15:28.906526Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for HF token at: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/misc/credentials/hf.txt\n",
      "✓ Logged in to Hugging Face\n"
     ]
    }
   ],
   "source": [
    "# Load Hugging Face token from file\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the token file path\n",
    "current_dir = Path.cwd()\n",
    "token_file = current_dir.parent / \"misc/credentials/hf.txt\"\n",
    "\n",
    "print(f\"Looking for HF token at: {token_file}\")\n",
    "\n",
    "if token_file.exists():\n",
    "    with open(token_file, 'r') as f:\n",
    "        hf_token = f.read().strip()\n",
    "    \n",
    "    # Set the token as an environment variable\n",
    "    os.environ['HF_TOKEN'] = hf_token\n",
    "    \n",
    "    # Also login using huggingface_hub\n",
    "    from huggingface_hub import login\n",
    "    login(token=hf_token)\n",
    "    print(\"✓ Logged in to Hugging Face\")\n",
    "else:\n",
    "    print(\"⚠️ No HF token found - you may need to authenticate manually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:28:35.497203Z",
     "iopub.status.busy": "2026-01-14T13:28:35.496990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Stable Diffusion 3 Medium from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b517f2f16e1d45ae8d49a5979d505bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/706 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66de416044554c25975e61828027c210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18faccd1761d4b998e353ee844df4dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/740 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4104bd3a39d749eab2798b7ed36c79ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfee24816e744af08c02fae6ed3b3396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder_3/model-00002-of-00002.safe(…):   0%|          | 0.00/4.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2592cccc9c041cda7e1c61f1d9828c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/model.safetensors:   0%|          | 0.00/247M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99b13b6d1e14d2682498d53136fc538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder_2/model.safetensors:   0%|          | 0.00/1.39G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96dac096ac04b5c98b0b41d2b278228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/574 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78538d1504d6406a967cebd5cac8d231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84535336f79844cdb35f801d42907cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder_3/model-00001-of-00002.safe(…):   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597bf50c725c434b986fdd13f90312cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5e36976b944af4a7cf2ecf19da887d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/19.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdb97cf6de04508a1d2cba3014bad68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/705 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033529be633e4e8e9631845ed016b063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/588 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e9880c6d6441f59a76ad7813443f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6abd5fdcf747bf8c3e2fe5a35d46ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/856 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfd1345f41e483389b5b0defcf6f0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/576 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05bbc83cc34e4f62977183cb663885df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_3/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24e9f67b45f4ff79e33f421d2858ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01732caa13284a7babc3378f930f592c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/20.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e389e30b36343a8893430a925ba1952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b4ba83cb1a46aa8d5989d13db69832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/372 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d93f31312c45b99e88e85ce781c898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transformer/diffusion_pytorch_model.safe(…):   0%|          | 0.00/4.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7d6f60d474444cb8f7cf93be2977f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/739 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e441567ba9444facf31e6344c873dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/diffusion_pytorch_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f324eeaf98945a9a6ecaf811da0f1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04fe395d09a74b6d9cf86f9e86cb214d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Stable Diffusion 3 Medium (standard version)\n",
    "try:\n",
    "    MODEL_ID = \"stabilityai/stable-diffusion-3-medium-diffusers\"\n",
    "    \n",
    "    if not os.path.exists(SD3_MODEL_PATH):\n",
    "        print(\"Downloading Stable Diffusion 3 Medium from Hugging Face...\")\n",
    "        sd_pipe = StableDiffusion3Pipeline.from_pretrained(\n",
    "            MODEL_ID,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        sd_pipe.save_pretrained(SD3_MODEL_PATH)\n",
    "        print(f\"✓ Model downloaded and saved to {SD3_MODEL_PATH}\")\n",
    "    else:\n",
    "        print(\"Loading Stable Diffusion 3 Medium from local path...\")\n",
    "        sd_pipe = StableDiffusion3Pipeline.from_pretrained(\n",
    "            SD3_MODEL_PATH,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            local_files_only=True\n",
    "        )\n",
    "    \n",
    "    sd_pipe = sd_pipe.to(device)\n",
    "    print(\"✓ Stable Diffusion 3 Medium loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading SD3 Medium: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load Embeddings\n\nSelect **positive** and **negative** embeddings for both T5 and CLIP.\n\n**SD3 uses 3 text encoders:**\n- **T5-XXL** (4096-dim per token, 512 tokens) - Load manipulated embeddings from `data/embeddings/T5/`\n- **CLIP-L + CLIP-G** (768 + 1280 = 2048-dim pooled) - Load from `data/embeddings/CLIP_SD3/` OR auto-generate from prompt\n\n**Generate CLIP embeddings:** Use notebook `003_CLIP_L_and_G_embeddings.ipynb`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup directories\nT5_EMBEDDINGS_DIR = current_dir.parent / \"data/embeddings/T5\"\nCLIP_SD3_EMBEDDINGS_DIR = current_dir.parent / \"data/embeddings/CLIP_SD3\"\n\n# Global variables for loaded embeddings\nloaded_t5_pos_embedding = None\nloaded_t5_neg_embedding = None\nloaded_t5_pos_prompt = None\nloaded_t5_neg_prompt = None\n\nloaded_clip_pos_pooled = None\nloaded_clip_neg_pooled = None\nloaded_clip_pos_prompt = None\nloaded_clip_neg_prompt = None\n\n# Get available embedding files\nt5_files = []\nclip_sd3_files = []\n\nif T5_EMBEDDINGS_DIR.exists():\n    t5_files = sorted([f.name for f in T5_EMBEDDINGS_DIR.glob('*.json')])\n\nif CLIP_SD3_EMBEDDINGS_DIR.exists():\n    clip_sd3_files = sorted([f.name for f in CLIP_SD3_EMBEDDINGS_DIR.glob('*.json')])\n\n# Add 'None' option for negative embeddings\nt5_files_with_none = ['(None)'] + t5_files\nclip_files_with_none = ['(None)'] + clip_sd3_files\n\nprint(f\"Found {len(t5_files)} T5 embeddings\")\nprint(f\"Found {len(clip_sd3_files)} CLIP-SD3 embeddings (CLIP-L + CLIP-G combined)\")\nprint(f\"\\nNote: If no CLIP embeddings loaded, they will be auto-generated from prompts.\")"
  },
  {
   "cell_type": "code",
   "source": "# ==================== T5 EMBEDDING WIDGETS ====================\n\n# T5 POSITIVE embedding selection\nt5_pos_dropdown = widgets.Dropdown(\n    options=t5_files,\n    description='T5 Positive:',\n    style={'description_width': 'initial'},\n    layout=widgets.Layout(width='600px')\n)\n\nload_t5_pos_button = widgets.Button(\n    description='Load T5 Positive',\n    button_style='success'\n)\n\nt5_pos_output = widgets.Output()\n\n# T5 NEGATIVE embedding selection\nt5_neg_dropdown = widgets.Dropdown(\n    options=t5_files_with_none,\n    description='T5 Negative:',\n    style={'description_width': 'initial'},\n    layout=widgets.Layout(width='600px')\n)\n\nload_t5_neg_button = widgets.Button(\n    description='Load T5 Negative',\n    button_style='warning'\n)\n\nt5_neg_output = widgets.Output()\n\ndef load_t5_pos_embedding(b):\n    global loaded_t5_pos_embedding, loaded_t5_pos_prompt\n    \n    with t5_pos_output:\n        t5_pos_output.clear_output()\n        \n        filename = t5_pos_dropdown.value\n        if not filename:\n            print(\"❌ Please select a T5 embedding file\")\n            return\n        \n        filepath = T5_EMBEDDINGS_DIR / filename\n        \n        try:\n            with open(filepath, 'r') as f:\n                data = json.load(f)\n            \n            loaded_t5_pos_embedding = np.array(data['embedding'])\n            loaded_t5_pos_prompt = data.get('prompt', 'Unknown')\n            \n            print(f\"✓ Loaded T5 POSITIVE embedding!\")\n            print(f\"  File: {filename}\")\n            print(f\"  Prompt: '{loaded_t5_pos_prompt}'\")\n            print(f\"  Shape: {loaded_t5_pos_embedding.shape}\")\n            \n        except Exception as e:\n            print(f\"❌ Error loading T5 positive embedding: {e}\")\n\ndef load_t5_neg_embedding(b):\n    global loaded_t5_neg_embedding, loaded_t5_neg_prompt\n    \n    with t5_neg_output:\n        t5_neg_output.clear_output()\n        \n        filename = t5_neg_dropdown.value\n        if not filename or filename == '(None)':\n            loaded_t5_neg_embedding = None\n            loaded_t5_neg_prompt = None\n            print(\"✓ No negative T5 embedding (will use zero tensor)\")\n            return\n        \n        filepath = T5_EMBEDDINGS_DIR / filename\n        \n        try:\n            with open(filepath, 'r') as f:\n                data = json.load(f)\n            \n            loaded_t5_neg_embedding = np.array(data['embedding'])\n            loaded_t5_neg_prompt = data.get('prompt', 'Unknown')\n            \n            print(f\"✓ Loaded T5 NEGATIVE embedding!\")\n            print(f\"  File: {filename}\")\n            print(f\"  Prompt: '{loaded_t5_neg_prompt}'\")\n            print(f\"  Shape: {loaded_t5_neg_embedding.shape}\")\n            \n        except Exception as e:\n            print(f\"❌ Error loading T5 negative embedding: {e}\")\n\nload_t5_pos_button.on_click(load_t5_pos_embedding)\nload_t5_neg_button.on_click(load_t5_neg_embedding)\n\n# ==================== CLIP EMBEDDING WIDGETS ====================\n\n# CLIP POSITIVE embedding selection\nclip_pos_dropdown = widgets.Dropdown(\n    options=clip_files_with_none,\n    description='CLIP Positive:',\n    style={'description_width': 'initial'},\n    layout=widgets.Layout(width='600px')\n)\n\nload_clip_pos_button = widgets.Button(\n    description='Load CLIP Positive',\n    button_style='success'\n)\n\nclip_pos_output = widgets.Output()\n\n# CLIP NEGATIVE embedding selection\nclip_neg_dropdown = widgets.Dropdown(\n    options=clip_files_with_none,\n    description='CLIP Negative:',\n    style={'description_width': 'initial'},\n    layout=widgets.Layout(width='600px')\n)\n\nload_clip_neg_button = widgets.Button(\n    description='Load CLIP Negative',\n    button_style='warning'\n)\n\nclip_neg_output = widgets.Output()\n\ndef load_clip_pos_embedding(b):\n    global loaded_clip_pos_pooled, loaded_clip_pos_prompt\n    \n    with clip_pos_output:\n        clip_pos_output.clear_output()\n        \n        filename = clip_pos_dropdown.value\n        if not filename or filename == '(None)':\n            loaded_clip_pos_pooled = None\n            loaded_clip_pos_prompt = None\n            print(\"✓ No positive CLIP embedding (will auto-generate from T5 prompt)\")\n            return\n        \n        filepath = CLIP_SD3_EMBEDDINGS_DIR / filename\n        \n        try:\n            with open(filepath, 'r') as f:\n                data = json.load(f)\n            \n            # Load the combined pooled embedding (2048-dim)\n            loaded_clip_pos_pooled = np.array(data['combined_pooled'])\n            loaded_clip_pos_prompt = data.get('prompt', 'Unknown')\n            \n            print(f\"✓ Loaded CLIP POSITIVE pooled embedding!\")\n            print(f\"  File: {filename}\")\n            print(f\"  Prompt: '{loaded_clip_pos_prompt}'\")\n            print(f\"  Shape: {loaded_clip_pos_pooled.shape} (CLIP-L + CLIP-G)\")\n            \n        except Exception as e:\n            print(f\"❌ Error loading CLIP positive embedding: {e}\")\n\ndef load_clip_neg_embedding(b):\n    global loaded_clip_neg_pooled, loaded_clip_neg_prompt\n    \n    with clip_neg_output:\n        clip_neg_output.clear_output()\n        \n        filename = clip_neg_dropdown.value\n        if not filename or filename == '(None)':\n            loaded_clip_neg_pooled = None\n            loaded_clip_neg_prompt = None\n            print(\"✓ No negative CLIP embedding (will auto-generate from T5 negative prompt)\")\n            return\n        \n        filepath = CLIP_SD3_EMBEDDINGS_DIR / filename\n        \n        try:\n            with open(filepath, 'r') as f:\n                data = json.load(f)\n            \n            # Load the combined pooled embedding (2048-dim)\n            loaded_clip_neg_pooled = np.array(data['combined_pooled'])\n            loaded_clip_neg_prompt = data.get('prompt', 'Unknown')\n            \n            print(f\"✓ Loaded CLIP NEGATIVE pooled embedding!\")\n            print(f\"  File: {filename}\")\n            print(f\"  Prompt: '{loaded_clip_neg_prompt}'\")\n            print(f\"  Shape: {loaded_clip_neg_pooled.shape} (CLIP-L + CLIP-G)\")\n            \n        except Exception as e:\n            print(f\"❌ Error loading CLIP negative embedding: {e}\")\n\nload_clip_pos_button.on_click(load_clip_pos_embedding)\nload_clip_neg_button.on_click(load_clip_neg_embedding)\n\n# ==================== GENERATION CONTROLS ====================\n\nseed_input = widgets.IntText(\n    value=42,\n    description='Seed:',\n    style={'description_width': 'initial'}\n)\n\nguidance_scale_slider = widgets.FloatSlider(\n    value=7.0,\n    min=0.0,\n    max=20.0,\n    step=0.5,\n    description='Guidance Scale:',\n    style={'description_width': 'initial'},\n    layout=widgets.Layout(width='500px')\n)\n\ngenerate_button = widgets.Button(\n    description='Generate Image',\n    button_style='primary',\n    layout=widgets.Layout(width='300px', height='50px')\n)\n\nimage_output = widgets.Output()\n\ndef generate_from_loaded_embeddings(b):\n    with image_output:\n        image_output.clear_output()\n        \n        # Check that T5 positive is loaded\n        if loaded_t5_pos_embedding is None:\n            print(\"❌ Please load a T5 positive embedding first!\")\n            return\n        \n        print(\"Generating SD3 image from loaded embeddings...\")\n        print(f\"  Seed: {seed_input.value}\")\n        print(f\"  Guidance Scale: {guidance_scale_slider.value}\")\n        \n        # Process T5 embeddings\n        print(\"\\nT5 POSITIVE:\")\n        print(f\"  Prompt: '{loaded_t5_pos_prompt}'\")\n        print(f\"  Shape: {loaded_t5_pos_embedding.shape}\")\n        t5_pos_tensor = torch.from_numpy(loaded_t5_pos_embedding.astype(np.float32)).to(\n            device=device, dtype=torch.bfloat16\n        ).unsqueeze(0)\n        \n        # Handle negative T5 embedding\n        if loaded_t5_neg_embedding is not None:\n            print(\"\\nT5 NEGATIVE:\")\n            print(f\"  Prompt: '{loaded_t5_neg_prompt}'\")\n            print(f\"  Shape: {loaded_t5_neg_embedding.shape}\")\n            t5_neg_tensor = torch.from_numpy(loaded_t5_neg_embedding.astype(np.float32)).to(\n                device=device, dtype=torch.bfloat16\n            ).unsqueeze(0)\n        else:\n            # CRITICAL: Create negative T5 embedding with SAME shape as positive\n            print(\"\\nT5 NEGATIVE: Creating zeros with same shape as positive\")\n            t5_neg_tensor = torch.zeros_like(t5_pos_tensor)\n        \n        # Process CLIP pooled embeddings\n        if loaded_clip_pos_pooled is not None:\n            print(\"\\nCLIP POSITIVE: Using loaded pooled embedding\")\n            print(f\"  Prompt: '{loaded_clip_pos_prompt}'\")\n            print(f\"  Shape: {loaded_clip_pos_pooled.shape}\")\n            pooled_pos = torch.from_numpy(loaded_clip_pos_pooled.astype(np.float32)).to(\n                device=device, dtype=torch.bfloat16\n            ).unsqueeze(0)\n        else:\n            print(\"\\nCLIP POSITIVE: Auto-generating from T5 prompt\")\n            _, pooled_pos, _ = sd_pipe.encode_prompt(\n                prompt=loaded_t5_pos_prompt,\n                device=device,\n                num_images_per_prompt=1,\n            )\n        \n        if loaded_clip_neg_pooled is not None:\n            print(\"\\nCLIP NEGATIVE: Using loaded pooled embedding\")\n            print(f\"  Prompt: '{loaded_clip_neg_prompt}'\")\n            print(f\"  Shape: {loaded_clip_neg_pooled.shape}\")\n            pooled_neg = torch.from_numpy(loaded_clip_neg_pooled.astype(np.float32)).to(\n                device=device, dtype=torch.bfloat16\n            ).unsqueeze(0)\n        else:\n            if loaded_t5_neg_prompt:\n                print(\"\\nCLIP NEGATIVE: Auto-generating from T5 negative prompt\")\n                _, pooled_neg, _ = sd_pipe.encode_prompt(\n                    prompt=loaded_t5_neg_prompt,\n                    device=device,\n                    num_images_per_prompt=1,\n                )\n            else:\n                print(\"\\nCLIP NEGATIVE: Using zero tensor\")\n                pooled_neg = torch.zeros_like(pooled_pos)\n        \n        print(f\"\\n{'='*60}\")\n        print(\"Generating image with SD3 Medium...\")\n        print(f\"{'='*60}\\n\")\n        \n        # Generate image\n        image = sd_pipe(\n            prompt_embeds=t5_pos_tensor,\n            negative_prompt_embeds=t5_neg_tensor,\n            pooled_prompt_embeds=pooled_pos,\n            negative_pooled_prompt_embeds=pooled_neg,\n            num_inference_steps=28,\n            guidance_scale=guidance_scale_slider.value,\n            height=1024,\n            width=1024,\n            generator=torch.manual_seed(seed_input.value)\n        ).images[0]\n        \n        print(\"✓ Image generated!\\n\")\n        display(image)\n\ngenerate_button.on_click(generate_from_loaded_embeddings)\n\n# ==================== DISPLAY UNIFIED INTERFACE ====================\n\ndisplay(widgets.VBox([\n    widgets.HTML(\"<h2>Embedding Selection & Image Generation</h2>\"),\n    \n    # T5 Embeddings Section\n    widgets.HTML(\"<h3>1. T5 Embeddings (Required)</h3>\"),\n    widgets.HTML(\"<b>Positive Embedding:</b>\"),\n    t5_pos_dropdown,\n    load_t5_pos_button,\n    t5_pos_output,\n    widgets.HTML(\"<br><b>Negative Embedding (optional):</b>\"),\n    t5_neg_dropdown,\n    load_t5_neg_button,\n    t5_neg_output,\n    \n    # CLIP Embeddings Section\n    widgets.HTML(\"<br><h3>2. CLIP Embeddings (Optional - auto-generated if not loaded)</h3>\"),\n    widgets.HTML(\"<b>Positive Embedding:</b>\"),\n    clip_pos_dropdown,\n    load_clip_pos_button,\n    clip_pos_output,\n    widgets.HTML(\"<br><b>Negative Embedding (optional):</b>\"),\n    clip_neg_dropdown,\n    load_clip_neg_button,\n    clip_neg_output,\n    \n    # Generation Controls Section\n    widgets.HTML(\"<br><h3>3. Generation Controls</h3>\"),\n    seed_input,\n    guidance_scale_slider,\n    widgets.HTML(\"<br>\"),\n    generate_button,\n    \n    # Output Section\n    widgets.HTML(\"<br><h3>Generated Image</h3>\"),\n    image_output\n]))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n### SD3 Architecture (3 Text Encoders):\n\nUnlike FLUX which uses **T5-XXL + CLIP**, SD3 uses **THREE** text encoders:\n\n1. **T5-XXL**: Produces 512 tokens × 4096-dim embeddings (sequence embeddings)\n2. **CLIP-L** (OpenAI): Produces 768-dim pooled embedding\n3. **CLIP-G** (OpenCLIP): Produces 1280-dim pooled embedding\n\nThe final inputs to SD3:\n- **prompt_embeds**: T5-XXL embeddings [512, 4096] - **You can manipulate these!**\n- **pooled_prompt_embeds**: CLIP-L + CLIP-G concatenated [2048] - Auto-generated from text\n\n### Key Differences from FLUX:\n\n1. **3 Text Encoders vs 2**: SD3 uses T5 + CLIP-L + CLIP-G, FLUX uses T5 + CLIP\n2. **Negative Embeddings**: SD3 supports negative T5 and pooled embeddings, FLUX doesn't\n3. **Guidance Scale**: SD3 uses CFG (default 7.0), FLUX.1-schnell doesn't (guidance=0)\n4. **More Inference Steps**: SD3 uses 28 steps, FLUX uses 4 steps\n5. **Different Architecture**: SD3 uses MMDiT with CFG, FLUX uses rectified flow\n\n### What You Can Manipulate:\n\n✅ **T5 Positive Embeddings** - Load your custom manipulated embeddings  \n✅ **T5 Negative Embeddings** - Load different manipulated embeddings to avoid  \n❌ **CLIP Pooled Embeddings** - Auto-generated from prompt text (need CLIP-L + CLIP-G)\n\n### Workflow:\n\n1. Load **positive** T5 embedding (required) - with your manipulations!\n2. Load **negative** T5 embedding (optional) - with different manipulations\n3. **Pooled embeddings** auto-generated from the prompts stored in the embedding files\n4. Adjust **guidance scale** (7.0 default, higher = stronger adherence to positive/avoidance of negative)\n5. Set **seed** for reproducibility\n6. Generate and compare!\n\n### Experiment Ideas:\n\n- **Positive normal + Negative zeroed**: See what gets removed when you zero embeddings\n- **Different guidance scales**: Test 1.0, 3.5, 7.0, 15.0 with same embeddings\n- **Manipulated positive only**: Use scaled/inverted/zeroed as positive prompt\n- **Compare with FLUX**: Same T5 embedding, different models, different results!\n- **T5 positive A + T5 negative B**: Blend concepts (e.g., \"red cat\" positive, \"blue dog\" negative)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flux uv",
   "language": "python",
   "name": "uv_flux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}