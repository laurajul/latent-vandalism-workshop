{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photo to CLIP Embedding Converter\n",
    "\n",
    "This notebook allows you to select images from a folder and convert them to CLIP embeddings.\n",
    "The embeddings are saved as JSON files for use in your Flux pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T13:21:29.089462Z",
     "iopub.status.busy": "2026-01-13T13:21:29.089292Z",
     "iopub.status.idle": "2026-01-13T13:21:29.091366Z",
     "shell.execute_reply": "2026-01-13T13:21:29.090929Z",
     "shell.execute_reply.started": "2026-01-13T13:21:29.089447Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install torch torchvision transformers pillow ipywidgets --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T13:56:00.287432Z",
     "iopub.status.busy": "2026-01-13T13:56:00.287224Z",
     "iopub.status.idle": "2026-01-13T13:56:00.290209Z",
     "shell.execute_reply": "2026-01-13T13:56:00.289697Z",
     "shell.execute_reply.started": "2026-01-13T13:56:00.287417Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import json\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "Current_dir = Path.cwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T13:56:01.486144Z",
     "iopub.status.busy": "2026-01-13T13:56:01.485941Z",
     "iopub.status.idle": "2026-01-13T13:56:01.489220Z",
     "shell.execute_reply": "2026-01-13T13:56:01.488716Z",
     "shell.execute_reply.started": "2026-01-13T13:56:01.486128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/data/embeddings/CLIP\n",
      "Current directory: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/notebooks\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "current_dir = Path(os.getcwd())\n",
    "output_dir = current_dir.parent / \"data\" / \"embeddings\" / \"CLIP\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Current directory: {current_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T13:56:02.577794Z",
     "iopub.status.busy": "2026-01-13T13:56:02.577603Z",
     "iopub.status.idle": "2026-01-13T13:56:10.855098Z",
     "shell.execute_reply": "2026-01-13T13:56:10.854478Z",
     "shell.execute_reply.started": "2026-01-13T13:56:02.577779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e0ae8d66424724a3ae97bab6a5451d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5023a49de540e4b11e8ebc67cb467c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48c5befe14f467e83c79cd095be7c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1819ea263504335a1bada97e555da21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099c4a1d290740c0b56bea21fc1b5ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2193410f6424962bc597c5f8e42bfdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e061245a4a9437bbee2c826b9fdf209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a76e8caa46456fa3ac3c6bac301a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load CLIP model\n",
    "print(\"Loading CLIP model...\")\n",
    "model_name = \"openai/clip-vit-large-patch14\"\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T13:56:25.574362Z",
     "iopub.status.busy": "2026-01-13T13:56:25.574105Z",
     "iopub.status.idle": "2026-01-13T13:56:25.580701Z",
     "shell.execute_reply": "2026-01-13T13:56:25.580204Z",
     "shell.execute_reply.started": "2026-01-13T13:56:25.574342Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_image_files(directory):\n",
    "    \"\"\"Get all image files from a directory\"\"\"\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}\n",
    "    directory = Path(directory)\n",
    "    \n",
    "    if not directory.exists():\n",
    "        return []\n",
    "    \n",
    "    image_files = []\n",
    "    for file in directory.iterdir():\n",
    "        if file.is_file() and file.suffix.lower() in image_extensions:\n",
    "            image_files.append(file)\n",
    "    \n",
    "    return sorted(image_files)\n",
    "\n",
    "\n",
    "def extract_clip_image_embedding(image_path, return_tokens=True):\n",
    "    \"\"\"\n",
    "    Extract CLIP image embeddings from an image file.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        return_tokens: If True, return token embeddings [257, 768].\n",
    "                       If False, return pooled embedding [768]\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with embedding data\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Process the image\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    # Get image embeddings\n",
    "    with torch.no_grad():\n",
    "        if return_tokens:\n",
    "            # Get full token embeddings\n",
    "            vision_outputs = model.vision_model(**inputs)\n",
    "            image_embeds = vision_outputs.last_hidden_state  # [1, 257, 768]\n",
    "            image_embeds = image_embeds.squeeze(0)  # [257, 768]\n",
    "            shape = list(image_embeds.shape)\n",
    "        else:\n",
    "            # Get pooled embedding\n",
    "            image_features = model.get_image_features(**inputs)  # [1, 768]\n",
    "            # Normalize\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            image_embeds = image_features.squeeze(0)  # [768]\n",
    "            shape = list(image_embeds.shape)\n",
    "    \n",
    "    # Convert to list for JSON serialization\n",
    "    embedding_list = image_embeds.cpu().numpy().tolist()\n",
    "    \n",
    "    return {\n",
    "        \"source_image\": str(image_path.name),\n",
    "        \"embedding\": embedding_list,\n",
    "        \"shape\": shape,\n",
    "        \"type\": \"clip_image_tokens\" if return_tokens else \"clip_image_pooled\",\n",
    "        \"model\": model_name\n",
    "    }\n",
    "\n",
    "\n",
    "def save_embedding_json(embedding_data, output_path):\n",
    "    \"\"\"Save embedding data to JSON file\"\"\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(embedding_data, f)\n",
    "    print(f\"Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T14:02:49.369151Z",
     "iopub.status.busy": "2026-01-13T14:02:49.368962Z",
     "iopub.status.idle": "2026-01-13T14:02:49.383890Z",
     "shell.execute_reply": "2026-01-13T14:02:49.383499Z",
     "shell.execute_reply.started": "2026-01-13T14:02:49.369137Z"
    }
   },
   "outputs": [],
   "source": [
    "# Widget for selecting image directory\n",
    "image_dir_input = widgets.Text(\n",
    "    value=str(current_dir.parent / \"data\" / \"input_img\"),\n",
    "    placeholder='Enter path to image folder',\n",
    "    description='Image Folder:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='600px')\n",
    ")\n",
    "\n",
    "load_button = widgets.Button(\n",
    "    description='Load Images',\n",
    "    button_style='info',\n",
    "    tooltip='Load images from the specified folder'\n",
    ")\n",
    "\n",
    "image_selector = widgets.Dropdown(\n",
    "    options=[],\n",
    "    description='Select Image:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='600px'),\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "embedding_type = widgets.RadioButtons(\n",
    "    options=[\n",
    "        ('Token embeddings [257, 768] - Full detail', True),\n",
    "        ('Pooled embedding [768] - Single vector', False)\n",
    "    ],\n",
    "    description='Embedding Type:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "preview_image = widgets.Image(\n",
    "    format='png',\n",
    "    width=400,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "convert_button = widgets.Button(\n",
    "    description='Convert to CLIP Embedding',\n",
    "    button_style='success',\n",
    "    tooltip='Extract CLIP embedding and save as JSON',\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "output_text = widgets.Output()\n",
    "\n",
    "# Current state\n",
    "state = {\n",
    "    'image_files': [],\n",
    "    'current_image': None\n",
    "}\n",
    "\n",
    "\n",
    "def on_load_images(b):\n",
    "    \"\"\"Load images from the specified directory\"\"\"\n",
    "    with output_text:\n",
    "        clear_output()\n",
    "        image_dir = Path(image_dir_input.value)\n",
    "        \n",
    "        if not image_dir.exists():\n",
    "            print(f\"âŒ Directory not found: {image_dir}\")\n",
    "            image_selector.options = []\n",
    "            image_selector.disabled = True\n",
    "            convert_button.disabled = True\n",
    "            return\n",
    "        \n",
    "        image_files = get_image_files(image_dir)\n",
    "        \n",
    "        if not image_files:\n",
    "            print(f\"âŒ No images found in: {image_dir}\")\n",
    "            image_selector.options = []\n",
    "            image_selector.disabled = True\n",
    "            convert_button.disabled = True\n",
    "            return\n",
    "        \n",
    "        state['image_files'] = image_files\n",
    "        image_selector.options = [(f.name, f) for f in image_files]\n",
    "        image_selector.disabled = False\n",
    "        convert_button.disabled = False\n",
    "        \n",
    "        print(f\"âœ… Loaded {len(image_files)} images from {image_dir}\")\n",
    "\n",
    "\n",
    "def on_image_selected(change):\n",
    "    \"\"\"Update preview when image is selected\"\"\"\n",
    "    if change['new'] is None:\n",
    "        return\n",
    "    \n",
    "    image_path = change['new']\n",
    "    state['current_image'] = image_path\n",
    "    \n",
    "    # Load and display preview\n",
    "    with open(image_path, 'rb') as f:\n",
    "        preview_image.value = f.read()\n",
    "    \n",
    "    with output_text:\n",
    "        print(f\"Selected: {image_path.name}\")\n",
    "\n",
    "\n",
    "def on_convert(b):\n",
    "    \"\"\"Convert selected image to CLIP embedding\"\"\"\n",
    "    with output_text:\n",
    "        clear_output()\n",
    "        \n",
    "        if state['current_image'] is None:\n",
    "            print(\"âŒ No image selected\")\n",
    "            return\n",
    "        \n",
    "        image_path = state['current_image']\n",
    "        return_tokens = embedding_type.value\n",
    "        \n",
    "        print(f\"ðŸ”„ Processing: {image_path.name}\")\n",
    "        print(f\"   Type: {'Token embeddings [257, 768]' if return_tokens else 'Pooled embedding [768]'}\")\n",
    "        \n",
    "        try:\n",
    "            # Extract embedding\n",
    "            embedding_data = extract_clip_image_embedding(image_path, return_tokens)\n",
    "            \n",
    "            # Generate output filename\n",
    "            input_stem = image_path.stem  # filename without extension\n",
    "            output_filename = f\"{input_stem}_from_image.json\"\n",
    "            output_path = output_dir / output_filename\n",
    "            \n",
    "            # Save to JSON\n",
    "            save_embedding_json(embedding_data, output_path)\n",
    "            \n",
    "            print(f\"âœ… Success!\")\n",
    "            print(f\"   Shape: {embedding_data['shape']}\")\n",
    "            print(f\"   Saved to: {output_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: {e}\")\n",
    "\n",
    "\n",
    "# Connect callbacks\n",
    "load_button.on_click(on_load_images)\n",
    "image_selector.observe(on_image_selected, names='value')\n",
    "convert_button.on_click(on_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T14:02:50.912758Z",
     "iopub.status.busy": "2026-01-13T14:02:50.912543Z",
     "iopub.status.idle": "2026-01-13T14:02:50.918378Z",
     "shell.execute_reply": "2026-01-13T14:02:50.917982Z",
     "shell.execute_reply.started": "2026-01-13T14:02:50.912744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Photo to CLIP Embedding Converter\n",
      "======================================================================\n",
      "Output directory: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/data/embeddings/CLIP\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085e5e929dbf4522a40e916a39ebc2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='/shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/data/â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the interface\n",
    "print(\"=\" * 70)\n",
    "print(\"Photo to CLIP Embedding Converter\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "display(\n",
    "    widgets.VBox([\n",
    "        widgets.HBox([image_dir_input, load_button]),\n",
    "        image_selector,\n",
    "        preview_image,\n",
    "        embedding_type,\n",
    "        convert_button,\n",
    "        output_text\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "1. **Specify Image Folder**: Enter the path to your folder containing example photos\n",
    "2. **Load Images**: Click \"Load Images\" to scan the folder\n",
    "3. **Select Image**: Choose an image from the dropdown menu\n",
    "4. **Choose Embedding Type**:\n",
    "   - **Token embeddings [257, 768]**: Full token-level embeddings (like your text CLIP with 77 tokens)\n",
    "   - **Pooled embedding [768]**: Single vector representation\n",
    "5. **Convert**: Click \"Convert to CLIP Embedding\" to process and save\n",
    "\n",
    "The output JSON files will be saved in: `../data/embeddings/CLIP/`\n",
    "\n",
    "Filename format: `{original_image_name}_from_image.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Load and Use the Embedding\n",
    "\n",
    "After converting images, you can load and use the embeddings in your Flux pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load a saved embedding\n",
    "def load_embedding(json_path):\n",
    "    \"\"\"Load embedding from JSON file\"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    embedding = torch.tensor(data['embedding'])\n",
    "    print(f\"Loaded: {data['source_image']}\")\n",
    "    print(f\"Type: {data['type']}\")\n",
    "    print(f\"Shape: {data['shape']}\")\n",
    "    print(f\"Model: {data['model']}\")\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "# Uncomment to test loading:\n",
    "# embedding = load_embedding(output_dir / \"your_image_from_image.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- **Token Embeddings vs Pooled**: \n",
    "  - Token embeddings preserve spatial/patch information (good for detailed control)\n",
    "  - Pooled embeddings are more compact (good for overall style/content)\n",
    "  \n",
    "- **Compatibility**: The embeddings use the same CLIP model (`clip-vit-large-patch14`) for consistency\n",
    "\n",
    "- **File Size**: Token embeddings (~1.5 MB) are larger than pooled embeddings (~6 KB)\n",
    "\n",
    "- **Integration**: You can blend these image embeddings with text embeddings or use them directly in your Flux pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flux uv",
   "language": "python",
   "name": "uv_flux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
