{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Visual Attribute Blending: Image → Text Embeddings\n",
    "\n",
    "## The Problem\n",
    "Flux was trained on **text embeddings** from CLIP's text encoder, not **image embeddings** from CLIP's vision encoder. Even though both produce 768-dimensional vectors, they come from different encoders with different statistical distributions.\n",
    "\n",
    "## The Solution\n",
    "Instead of using image embeddings directly, we:\n",
    "1. Define a library of visual attributes as text (colors, composition, lighting)\n",
    "2. Use CLIP to measure how well each attribute describes the image\n",
    "3. Create a weighted blend of **text embeddings** based on these similarities\n",
    "4. Output a proper [77, 768] text embedding that Flux can understand\n",
    "\n",
    "## Why This Works\n",
    "- CLIP was trained to align vision and text in the same space\n",
    "- We extract visual features automatically using CLIP's similarity\n",
    "- We stay in text embedding space (where Flux was trained)\n",
    "- No manual text description needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aj758ccov4o",
   "source": "```mermaid\nflowchart LR\n    IMG[Input Image]\n    \n    subgraph CLIP Analysis\n        VIS[CLIP Vision<br/>Encoder]\n        ATTR[Attribute Library<br/>colors, mood, lighting...]\n        SIM[Similarity<br/>Calculation]\n    end\n\n    subgraph Text Embedding Generation\n        TOP[Top K Attributes]\n        TXT[CLIP Text<br/>Encoder]\n        BLEND[Weighted<br/>Blend]\n    end\n\n    EMB[Text Embedding<br/>77 × 768]\n\n    IMG --> VIS --> SIM\n    ATTR --> SIM\n    SIM --> TOP --> TXT --> BLEND --> EMB\n    \n    EMB -->|compatible with| FLUX[FLUX]\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "install",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Uncomment if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision transformers pillow ipywidgets matplotlib --break-system-packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTextModel, CLIPTokenizer\n",
    "from PIL import Image\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "current_dir = Path(os.getcwd())\n",
    "output_dir = current_dir.parent / \"data\" / \"embeddings\" / \"CLIP\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-load",
   "metadata": {},
   "source": [
    "## Load CLIP Models\n",
    "\n",
    "We need:\n",
    "- **Full CLIP model** (vision + text) for similarity calculations\n",
    "- **Text encoder** to generate the final text embeddings for Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-load-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading CLIP models...\")\n",
    "\n",
    "# Full CLIP model (for similarity calculations)\n",
    "model_name = \"openai/clip-vit-large-patch14\"\n",
    "clip_model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Text encoder and tokenizer (for generating final embeddings)\n",
    "text_model = CLIPTextModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_name)\n",
    "\n",
    "clip_model.eval()\n",
    "text_model.eval()\n",
    "\n",
    "print(\"✓ Models loaded successfully!\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Text embedding dimension: {text_model.config.hidden_size}\")\n",
    "print(f\"  Max tokens: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attributes",
   "metadata": {},
   "source": [
    "## Define Visual Attribute Library\n",
    "\n",
    "These are the \"building blocks\" we'll use to describe images.\n",
    "Each attribute will be converted to a text embedding, then blended based on how well it matches the image.\n",
    "\n",
    "You can customize this library to focus on attributes relevant to your use case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attributes-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define attribute library\n",
    "ATTRIBUTE_LIBRARY = {\n",
    "    \"colors\": [\n",
    "        \"red colors\",\n",
    "        \"orange colors\",\n",
    "        \"yellow colors\",\n",
    "        \"green colors\",\n",
    "        \"blue colors\",\n",
    "        \"purple colors\",\n",
    "        \"pink colors\",\n",
    "        \"warm colors\",\n",
    "        \"cool colors\",\n",
    "        \"vibrant colors\",\n",
    "        \"muted colors\",\n",
    "        \"pastel colors\",\n",
    "        \"dark colors\",\n",
    "        \"light colors\",\n",
    "    ],\n",
    "    \"composition\": [\n",
    "        \"centered composition\",\n",
    "        \"horizontal composition\",\n",
    "        \"vertical composition\",\n",
    "        \"diagonal composition\",\n",
    "        \"symmetrical composition\",\n",
    "        \"asymmetrical composition\",\n",
    "        \"simple composition\",\n",
    "        \"complex composition\",\n",
    "        \"minimal composition\",\n",
    "        \"busy composition\",\n",
    "    ],\n",
    "    \"lighting\": [\n",
    "        \"bright lighting\",\n",
    "        \"dark lighting\",\n",
    "        \"soft lighting\",\n",
    "        \"harsh lighting\",\n",
    "        \"high contrast\",\n",
    "        \"low contrast\",\n",
    "        \"dramatic lighting\",\n",
    "        \"natural lighting\",\n",
    "        \"backlit\",\n",
    "        \"evenly lit\",\n",
    "    ],\n",
    "    \"texture\": [\n",
    "        \"smooth texture\",\n",
    "        \"rough texture\",\n",
    "        \"soft texture\",\n",
    "        \"detailed texture\",\n",
    "        \"blurred texture\",\n",
    "        \"sharp details\",\n",
    "    ],\n",
    "    \"mood\": [\n",
    "        \"peaceful mood\",\n",
    "        \"dramatic mood\",\n",
    "        \"energetic mood\",\n",
    "        \"calm mood\",\n",
    "        \"mysterious mood\",\n",
    "        \"cheerful mood\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Flatten all attributes into a single list\n",
    "all_attributes = []\n",
    "for category, attrs in ATTRIBUTE_LIBRARY.items():\n",
    "    all_attributes.extend(attrs)\n",
    "\n",
    "print(f\"Total attributes defined: {len(all_attributes)}\")\n",
    "print(\"\\nCategories:\")\n",
    "for category, attrs in ATTRIBUTE_LIBRARY.items():\n",
    "    print(f\"  {category}: {len(attrs)} attributes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functions",
   "metadata": {},
   "source": [
    "## Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functions-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_features(image_path):\n",
    "    \"\"\"\n",
    "    Extract CLIP vision features from an image.\n",
    "    These are used for similarity calculations only.\n",
    "    \n",
    "    Returns: normalized image features [1, 768]\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(**inputs)\n",
    "        # Normalize for cosine similarity\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return image_features\n",
    "\n",
    "\n",
    "def get_text_features(texts):\n",
    "    \"\"\"\n",
    "    Extract CLIP text features for similarity calculations.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "    \n",
    "    Returns: normalized text features [len(texts), 768]\n",
    "    \"\"\"\n",
    "    inputs = clip_processor(text=texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.get_text_features(**inputs)\n",
    "        # Normalize for cosine similarity\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return text_features\n",
    "\n",
    "\n",
    "def calculate_similarities(image_features, text_features):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between image and each text attribute.\n",
    "    \n",
    "    Returns: similarity scores [len(texts)]\n",
    "    \"\"\"\n",
    "    # Cosine similarity (features are already normalized)\n",
    "    similarities = (image_features @ text_features.T).squeeze(0)\n",
    "    return similarities\n",
    "\n",
    "\n",
    "def get_text_embeddings(texts):\n",
    "    \"\"\"\n",
    "    Generate full [77, 768] text embeddings from CLIP text encoder.\n",
    "    These are the embeddings Flux expects.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "    \n",
    "    Returns: text embeddings [len(texts), 77, 768]\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        max_length=77,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = text_model(**tokens)\n",
    "        embeddings = outputs.last_hidden_state  # [batch, 77, 768]\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def blend_embeddings(embeddings, weights):\n",
    "    \"\"\"\n",
    "    Create weighted blend of text embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Tensor of shape [n_attributes, 77, 768]\n",
    "        weights: Tensor of shape [n_attributes] - similarity scores\n",
    "    \n",
    "    Returns: Blended embedding [77, 768]\n",
    "    \"\"\"\n",
    "    # Normalize weights to sum to 1\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    # Reshape weights for broadcasting: [n_attributes, 1, 1]\n",
    "    weights = weights.view(-1, 1, 1)\n",
    "    \n",
    "    # Weighted sum\n",
    "    blended = (embeddings * weights).sum(dim=0)  # [77, 768]\n",
    "    \n",
    "    return blended\n",
    "\n",
    "\n",
    "print(\"✓ Functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main-function",
   "metadata": {},
   "source": [
    "## Main Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main-function-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visual_text_embedding(image_path, top_k=15, min_similarity=0.15):\n",
    "    \"\"\"\n",
    "    Create a text embedding informed by visual content.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to input image\n",
    "        top_k: Number of top attributes to use for blending (default: 15)\n",
    "        min_similarity: Minimum similarity threshold (default: 0.15)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - embedding: [77, 768] numpy array\n",
    "        - shape: [77, 768]\n",
    "        - prompt: Description of attributes used\n",
    "        - top_attributes: List of (attribute, score) tuples\n",
    "    \"\"\"\n",
    "    print(f\"Processing: {image_path.name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Get image features\n",
    "    print(\"\\n1. Extracting image features...\")\n",
    "    image_features = get_image_features(image_path)\n",
    "    print(f\"   ✓ Image features: {image_features.shape}\")\n",
    "    \n",
    "    # Step 2: Get text features for all attributes\n",
    "    print(\"\\n2. Processing attribute library...\")\n",
    "    text_features = get_text_features(all_attributes)\n",
    "    print(f\"   ✓ Text features: {text_features.shape}\")\n",
    "    \n",
    "    # Step 3: Calculate similarities\n",
    "    print(\"\\n3. Calculating similarities...\")\n",
    "    similarities = calculate_similarities(image_features, text_features)\n",
    "    print(f\"   ✓ Similarities calculated: {similarities.shape}\")\n",
    "    print(f\"   Similarity range: [{similarities.min():.3f}, {similarities.max():.3f}]\")\n",
    "    \n",
    "    # Step 4: Select top K attributes\n",
    "    print(f\"\\n4. Selecting top {top_k} attributes (min similarity: {min_similarity})...\")\n",
    "    \n",
    "    # Get top K indices\n",
    "    top_indices = torch.argsort(similarities, descending=True)[:top_k]\n",
    "    top_similarities = similarities[top_indices]\n",
    "    \n",
    "    # Filter by minimum similarity\n",
    "    mask = top_similarities >= min_similarity\n",
    "    top_indices = top_indices[mask]\n",
    "    top_similarities = top_similarities[mask]\n",
    "    \n",
    "    # Get attribute names\n",
    "    top_attributes = [(all_attributes[idx], similarities[idx].item()) \n",
    "                      for idx in top_indices]\n",
    "    \n",
    "    print(f\"   ✓ Selected {len(top_attributes)} attributes:\")\n",
    "    for attr, score in top_attributes:\n",
    "        print(f\"      {score:.3f} - {attr}\")\n",
    "    \n",
    "    # Step 5: Get full text embeddings for selected attributes\n",
    "    print(\"\\n5. Generating text embeddings...\")\n",
    "    selected_texts = [all_attributes[idx] for idx in top_indices]\n",
    "    text_embeddings = get_text_embeddings(selected_texts)  # [n, 77, 768]\n",
    "    print(f\"   ✓ Text embeddings: {text_embeddings.shape}\")\n",
    "    \n",
    "    # Step 6: Blend embeddings\n",
    "    print(\"\\n6. Blending embeddings...\")\n",
    "    blended = blend_embeddings(text_embeddings, top_similarities)\n",
    "    print(f\"   ✓ Blended embedding: {blended.shape}\")\n",
    "    \n",
    "    # Convert to numpy\n",
    "    embedding_array = blended.cpu().numpy()\n",
    "    \n",
    "    # Create prompt description\n",
    "    prompt = \", \".join([attr for attr, _ in top_attributes[:5]])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✓ Complete!\")\n",
    "    print(f\"Prompt (top 5 attributes): {prompt}\")\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"embedding\": embedding_array.tolist(),\n",
    "        \"shape\": [77, 768],\n",
    "        \"top_attributes\": top_attributes,\n",
    "        \"method\": \"visual_attribute_blending\",\n",
    "        \"source_image\": str(image_path.name)\n",
    "    }\n",
    "\n",
    "print(\"✓ Main function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-function",
   "metadata": {},
   "source": [
    "## Visualization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-function-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attributes(top_attributes, image_path=None):\n",
    "    \"\"\"\n",
    "    Visualize the top attributes and their similarity scores.\n",
    "    \"\"\"\n",
    "    if not top_attributes:\n",
    "        print(\"No attributes to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Prepare data\n",
    "    attrs = [attr for attr, _ in top_attributes]\n",
    "    scores = [score for _, score in top_attributes]\n",
    "    \n",
    "    # Create figure\n",
    "    if image_path:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # Show image\n",
    "        img = Image.open(image_path)\n",
    "        ax1.imshow(img)\n",
    "        ax1.axis('off')\n",
    "        ax1.set_title('Input Image', fontsize=14, fontweight='bold')\n",
    "    else:\n",
    "        fig, ax2 = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    \n",
    "    # Plot bar chart\n",
    "    y_pos = np.arange(len(attrs))\n",
    "    ax2.barh(y_pos, scores, color='steelblue')\n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(attrs)\n",
    "    ax2.invert_yaxis()  # Top attribute at the top\n",
    "    ax2.set_xlabel('CLIP Similarity Score', fontsize=12)\n",
    "    ax2.set_title('Top Visual Attributes', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(scores):\n",
    "        ax2.text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Visualization function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interface",
   "metadata": {},
   "source": [
    "## Interactive Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interface-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File browser\n",
    "def get_image_files(directory):\n",
    "    \"\"\"Get all image files from a directory\"\"\"\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}\n",
    "    directory = Path(directory)\n",
    "    \n",
    "    if not directory.exists():\n",
    "        return []\n",
    "    \n",
    "    image_files = []\n",
    "    for file in directory.iterdir():\n",
    "        if file.is_file() and file.suffix.lower() in image_extensions:\n",
    "            image_files.append(file)\n",
    "    \n",
    "    return sorted(image_files)\n",
    "\n",
    "# Widgets\n",
    "image_dir_input = widgets.Text(\n",
    "    value=str(current_dir.parent / \"data\" / \"input_img\"),\n",
    "    placeholder='Enter path to image folder',\n",
    "    description='Image Folder:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='600px')\n",
    ")\n",
    "\n",
    "load_button = widgets.Button(\n",
    "    description='Load Images',\n",
    "    button_style='info'\n",
    ")\n",
    "\n",
    "image_selector = widgets.Dropdown(\n",
    "    options=[],\n",
    "    description='Select Image:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='600px'),\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "preview_image = widgets.Image(\n",
    "    format='png',\n",
    "    width=400,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "top_k_slider = widgets.IntSlider(\n",
    "    value=15,\n",
    "    min=5,\n",
    "    max=30,\n",
    "    step=1,\n",
    "    description='Top K attributes:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "min_sim_slider = widgets.FloatSlider(\n",
    "    value=0.15,\n",
    "    min=0.0,\n",
    "    max=0.5,\n",
    "    step=0.05,\n",
    "    description='Min similarity:',\n",
    "    style={'description_width': 'initial'},\n",
    "    readout_format='.2f'\n",
    ")\n",
    "\n",
    "process_button = widgets.Button(\n",
    "    description='Process Image',\n",
    "    button_style='success',\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# State\n",
    "state = {\n",
    "    'image_files': [],\n",
    "    'current_image': None,\n",
    "    'last_result': None\n",
    "}\n",
    "\n",
    "# Callbacks\n",
    "def on_load_images(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        image_dir = Path(image_dir_input.value)\n",
    "        \n",
    "        if not image_dir.exists():\n",
    "            print(f\"❌ Directory not found: {image_dir}\")\n",
    "            return\n",
    "        \n",
    "        image_files = get_image_files(image_dir)\n",
    "        \n",
    "        if not image_files:\n",
    "            print(f\"❌ No images found in: {image_dir}\")\n",
    "            return\n",
    "        \n",
    "        state['image_files'] = image_files\n",
    "        image_selector.options = [(f.name, f) for f in image_files]\n",
    "        image_selector.disabled = False\n",
    "        process_button.disabled = False\n",
    "        \n",
    "        print(f\"✓ Loaded {len(image_files)} images from {image_dir}\")\n",
    "\n",
    "def on_image_selected(change):\n",
    "    if change['new'] is None:\n",
    "        return\n",
    "    \n",
    "    image_path = change['new']\n",
    "    state['current_image'] = image_path\n",
    "    \n",
    "    # Load and display preview\n",
    "    with open(image_path, 'rb') as f:\n",
    "        preview_image.value = f.read()\n",
    "\n",
    "def on_process(b):\n",
    "    with output_area:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        if state['current_image'] is None:\n",
    "            print(\"❌ No image selected\")\n",
    "            return\n",
    "        \n",
    "        image_path = state['current_image']\n",
    "        \n",
    "        try:\n",
    "            # Process image\n",
    "            result = create_visual_text_embedding(\n",
    "                image_path,\n",
    "                top_k=top_k_slider.value,\n",
    "                min_similarity=min_sim_slider.value\n",
    "            )\n",
    "            \n",
    "            state['last_result'] = result\n",
    "            \n",
    "            # Save to JSON\n",
    "            output_filename = f\"{image_path.stem}_visual_blend.json\"\n",
    "            output_path = output_dir / output_filename\n",
    "            \n",
    "            # Remove top_attributes from saved JSON (too verbose)\n",
    "            save_data = {k: v for k, v in result.items() if k != 'top_attributes'}\n",
    "            \n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(save_data, f)\n",
    "            \n",
    "            print(f\"\\n✓ Saved to: {output_path}\")\n",
    "            print(f\"  File size: {os.path.getsize(output_path) / 1024:.2f} KB\")\n",
    "            \n",
    "            # Visualize\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            visualize_attributes(result['top_attributes'], image_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "# Connect callbacks\n",
    "load_button.on_click(on_load_images)\n",
    "image_selector.observe(on_image_selected, names='value')\n",
    "process_button.on_click(on_process)\n",
    "\n",
    "# Display interface\n",
    "print(\"=\"*70)\n",
    "print(\"Visual Attribute Blending Interface\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "display(\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<h3>1. Load Images</h3>\"),\n",
    "        widgets.HBox([image_dir_input, load_button]),\n",
    "        widgets.HTML(\"<h3>2. Select Image</h3>\"),\n",
    "        image_selector,\n",
    "        preview_image,\n",
    "        widgets.HTML(\"<h3>3. Configure Processing</h3>\"),\n",
    "        top_k_slider,\n",
    "        min_sim_slider,\n",
    "        widgets.HTML(\"<h3>4. Process</h3>\"),\n",
    "        process_button,\n",
    "        output_area\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "1. **Load Images**: Enter the path to your image folder and click \"Load Images\"\n",
    "2. **Select Image**: Choose an image from the dropdown\n",
    "3. **Configure Processing**:\n",
    "   - **Top K attributes**: How many top-scoring attributes to blend (more = more comprehensive)\n",
    "   - **Min similarity**: Minimum threshold for including an attribute (higher = more selective)\n",
    "4. **Process**: Click \"Process Image\" to create the blended text embedding\n",
    "\n",
    "The output will show:\n",
    "- Processing steps and selected attributes\n",
    "- Visualization showing similarity scores\n",
    "- Saved JSON file path\n",
    "\n",
    "## Output Format\n",
    "\n",
    "The generated JSON files will have:\n",
    "- `prompt`: Comma-separated list of top 5 attributes\n",
    "- `embedding`: [77, 768] array - compatible with Flux!\n",
    "- `shape`: [77, 768]\n",
    "- `method`: \"visual_attribute_blending\"\n",
    "- `source_image`: Original image filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "notes",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "### The Process:\n",
    "\n",
    "1. **Extract Image Features**: Get vision embedding from CLIP (used only for similarity)\n",
    "2. **Calculate Similarities**: Compare image to each attribute text\n",
    "3. **Select Top Attributes**: Keep only the highest-scoring attributes\n",
    "4. **Generate Text Embeddings**: Get full [77, 768] embeddings for each selected attribute\n",
    "5. **Weighted Blend**: Combine embeddings weighted by similarity scores\n",
    "6. **Output**: Final [77, 768] text embedding that Flux can use\n",
    "\n",
    "### Key Insight:\n",
    "\n",
    "Instead of trying to convert image embeddings to text embeddings (impossible!), we:\n",
    "- Use CLIP's vision-text alignment to measure visual properties\n",
    "- Stay entirely in text embedding space for the final output\n",
    "- Automatically extract visual features without manual description\n",
    "\n",
    "### Customization:\n",
    "\n",
    "- **Attribute Library**: Modify `ATTRIBUTE_LIBRARY` to focus on specific visual aspects\n",
    "- **Top K**: Higher values = more attributes blended = more comprehensive\n",
    "- **Min Similarity**: Higher threshold = more selective = stronger attributes only\n",
    "\n",
    "### Comparison with Traditional Approaches:\n",
    "\n",
    "- **CLIP Interrogation**: Generates a single text description → single embedding\n",
    "- **This Method**: Blends multiple visual attributes → weighted combination\n",
    "- **Direct Image Embedding**: Uses vision encoder → incompatible with Flux ❌\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "- Won't capture specific objects or scenes (\"elephant\", \"sunset\")\n",
    "- Focuses on visual attributes (colors, composition, mood)\n",
    "- Results depend on the attribute library\n",
    "- Best for style transfer and visual property extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch",
   "metadata": {},
   "source": [
    "## Batch Processing (Optional)\n",
    "\n",
    "Process multiple images at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_images(image_dir, top_k=15, min_similarity=0.15):\n",
    "    \"\"\"\n",
    "    Process all images in a directory.\n",
    "    \"\"\"\n",
    "    image_dir = Path(image_dir)\n",
    "    image_files = get_image_files(image_dir)\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images in {image_dir}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        print(f\"\\n[{i}/{len(image_files)}] Processing {image_path.name}...\")\n",
    "        \n",
    "        try:\n",
    "            result = create_visual_text_embedding(\n",
    "                image_path,\n",
    "                top_k=top_k,\n",
    "                min_similarity=min_similarity\n",
    "            )\n",
    "            \n",
    "            # Save\n",
    "            output_filename = f\"{image_path.stem}_visual_blend.json\"\n",
    "            output_path = output_dir / output_filename\n",
    "            \n",
    "            save_data = {k: v for k, v in result.items() if k != 'top_attributes'}\n",
    "            \n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(save_data, f)\n",
    "            \n",
    "            print(f\"✓ Saved to: {output_path}\")\n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {image_path.name}: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"✓ Batch processing complete! Processed {len(results)}/{len(image_files)} images\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Uncomment to run batch processing:\n",
    "# results = batch_process_images(\n",
    "#     image_dir=current_dir.parent / \"data\" / \"input_img\",\n",
    "#     top_k=15,\n",
    "#     min_similarity=0.15\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test",
   "metadata": {},
   "source": [
    "## Test with Flux\n",
    "\n",
    "To verify the embeddings work with Flux, you can load them in your `Image_generation.ipynb` notebook.\n",
    "\n",
    "The embeddings are saved in the same format and location as your other CLIP embeddings, so they should load directly into your existing pipeline!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}