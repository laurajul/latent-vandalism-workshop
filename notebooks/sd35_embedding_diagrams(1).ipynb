{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Vandalism: The Joy of Productive Damage to Text-to-Image Synthesis Pipelines\n",
    "\n",
    "**Workshop by Laura Wagner**\n",
    "\n",
    "ðŸ”— [laurajul.github.io](https://laurajul.github.io/)  \n",
    "ðŸ“¦ [Workshop Repository](https://github.com/laurajul/latent-vandalism-workshop)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Text-to-image models have evolved into sophisticated engines of **template culture** (Grund and Scherffig), systems trained to reproduce standardized aesthetics. Fatigued by the constant flood of polished results and the arms race for images benchmarked on visual coherence, commercial value and consumer-friendliness, this workshop explores once again the charm of **AI weirdness** (Shane) - the failure in generative AI and the epistemic value of **productive damage**.\n",
    "\n",
    "Drawing inspiration from **glitch studies** (Menkman), we embrace glitches and artifacts as revelatory moments. Through gently violating the consumer-friendly, polished norms meant to please, we surface the model's implicit assumptions about how things are *supposed* to look. Participants will work directly with **Diffusion Transformers (DiT)**, focusing on the role of embeddings in image-text correlation from embedding space to latent space back into pixel space. Through hands-on meddling with the pipeline, we'll systematically damage and reconfigure the semantic substrate that guides image generation, deliberately perturbing inputs to understand this system's sensitivity and dynamics.\n",
    "\n",
    "This **counterfactual, gently adversarial approach**, positions productive damage as a research method. Through **iatrogenic techniques** performed on text-to-image models, we probe the layers of **technological inscription** (Latour) embedded in these systems. Values, design choices, and visual norms inscribed become legible where the system breaks down. By deliberately coaxing the model into failure, we trace the contours of what has been encoded into them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Workshop Overview\n",
    "\n",
    "### The Problem with Perfect\n",
    "\n",
    "Modern text-to-image models have become exceptionally good at producing exactly what they're supposed to: coherent, aesthetically pleasing, commercially viable images. But this polish comes at a cost. These models have been trained and tuned to reproduce **template culture**â€”standardized visual languages that feel increasingly homogeneous.\n",
    "\n",
    "When every generated image is optimized for visual coherence and consumer appeal, we lose something valuable: the **epistemic weirdness** that reveals how these systems actually work.\n",
    "\n",
    "### Productive Damage as Method\n",
    "\n",
    "This workshop takes a different approach. Instead of trying to get the \"best\" results, we're interested in **productive damage**â€”deliberate interventions that make the system fail in interesting ways. By breaking things carefully, we can:\n",
    "\n",
    "- **Surface hidden assumptions** about what images \"should\" look like\n",
    "- **Reveal the training data's bias** toward certain visual patterns\n",
    "- **Understand the semantic structure** of embedding space\n",
    "- **Trace technological inscription**â€”the values and design choices baked into these systems\n",
    "\n",
    "As Rosa Menkman argues in glitch studies, glitches aren't just errorsâ€”they're **revelatory moments** that expose the normally invisible structures underlying digital systems.\n",
    "\n",
    "### Our Approach: Iatrogenic Techniques\n",
    "\n",
    "We adopt what we call **iatrogenic techniques**â€”a term borrowed from medicine meaning \"caused by treatment itself.\" Rather than trying to optimize the pipeline, we deliberately introduce perturbations:\n",
    "\n",
    "- **Scaling embeddings** beyond reasonable ranges\n",
    "- **Inverting semantic directions** to explore negative space\n",
    "- **Mixing incompatible concepts** that text prompts can't express\n",
    "- **Zeroing out encoders** to see what each contributes\n",
    "- **Creating impossible combinations** that violate training distribution\n",
    "\n",
    "These interventions are **gently adversarial**â€”we're not trying to break the system maliciously, but to understand it through carefully designed failures.\n",
    "\n",
    "### What We're Doing Today\n",
    "\n",
    "In this workshop, we're going to peek under the hood of modern text-to-image diffusion models. Specifically, we'll explore how text prompts are transformed into numerical embeddings, and how those embeddings guide the image generation process.\n",
    "\n",
    "**The Key Insight:** Most users interact with these models through text prompts, but the models themselves don't \"see\" text. They work with **numerical embeddings**â€”high-dimensional vectors that capture semantic meaning. By directly manipulating these embeddings, we can:\n",
    "\n",
    "- Create images that are **impossible to generate from text alone**\n",
    "- Understand how **different text encoders** contribute distinct semantic information\n",
    "- Discover **emergent behaviors** in embedding space that text can't access\n",
    "- Learn exactly **where and how embeddings influence** generation\n",
    "- **Make visible the invisible**â€”the assumptions and inscriptions built into these systems\n",
    "\n",
    "### Technical Approach\n",
    "\n",
    "We've modified both the **SD 3.5** and **FLUX-Schnell** pipelines to accept raw embeddings directly, bypassing the text encoding step. Here's our workflow:\n",
    "\n",
    "1. **Generate embeddings** from text prompts using T5-XXL, CLIP-L, and CLIP-G encoders\n",
    "2. **Save embeddings** as JSON files so we can inspect and modify the raw numbers\n",
    "3. **Vandalize embeddings** by scaling, inverting, mixing, or zeroing them\n",
    "4. **Inject damaged embeddings** directly into the diffusion pipeline\n",
    "5. **Observe the productive failures** and understand what they reveal\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "By working directly with embeddings, we gain:\n",
    "\n",
    "- **Transparency**: See exactly what numerical representations drive image generation\n",
    "- **Control**: Manipulate semantic space in ways impossible through text\n",
    "- **Understanding**: Learn how different encoders contribute and interact\n",
    "- **Creativity**: Discover novel visual territory outside the training distribution\n",
    "- **Critical Insight**: Expose the **technological inscription** (Latour)â€”the values, norms, and aesthetic preferences encoded into these systems\n",
    "\n",
    "When a system breaks down, it reveals its construction. By systematically damaging these pipelines, we make visible the **layers of inscription** that are normally hidden behind polished outputs.\n",
    "\n",
    "### Models We're Vandalizing\n",
    "\n",
    "- **Stable Diffusion 3.5**: Uses three text encoders (T5-XXL, CLIP-L, CLIP-G) with an MMDiT transformer architecture\n",
    "- **FLUX-Schnell**: Uses two text encoders (T5-XXL, CLIP-L) with a different architectural approach\n",
    "\n",
    "Both models follow a similar high-level pattern: **Text â†’ Embeddings â†’ Latent Diffusion â†’ Image**, but they differ in how embeddings are processed and used. These differences become visible when we start breaking things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big Picture: From Text to Pixels\n",
    "\n",
    "Before diving into the details, let's understand the fundamental flow in modern diffusion models:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Text Prompt] --> B[Embedding Space]\n",
    "    B --> C[Latent Space]\n",
    "    C --> D[Pixel Space]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style B fill:#fff4e1\n",
    "    style C fill:#ffe1f5\n",
    "    style D fill:#d4edda\n",
    "```\n",
    "\n",
    "### Three Key Spaces:\n",
    "\n",
    "1. **Embedding Space** (High-dimensional semantic vectors)\n",
    "   - Where text meaning is encoded numerically\n",
    "   - Typically thousands of dimensions (768-4096 per token)\n",
    "   - **This is our focus today**\n",
    "\n",
    "2. **Latent Space** (Compressed image representation)\n",
    "   - Where diffusion actually happens\n",
    "   - Much smaller than pixel space (e.g., 64Ã—64Ã—16 instead of 1024Ã—1024Ã—3)\n",
    "   - Embeddings guide the denoising process here\n",
    "\n",
    "3. **Pixel Space** (Final RGB image)\n",
    "   - The actual image you see\n",
    "   - Decoded from latent space by VAE\n",
    "\n",
    "### Our Workshop Focus:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Text Prompt] --> B[Text Encoders]\n",
    "    B --> C[Embedding Space]\n",
    "    \n",
    "    C -.->|We manipulate here| D[Modified Embeddings]\n",
    "    \n",
    "    D --> E[Diffusion in Latent Space]\n",
    "    E --> F[VAE Decoder]\n",
    "    F --> G[Pixel Space / Image]\n",
    "    \n",
    "    H[Random Noise] --> E\n",
    "    \n",
    "    style C fill:#fff4e1\n",
    "    style D fill:#ffcccc\n",
    "    style E fill:#ffe1f5\n",
    "    style G fill:#d4edda\n",
    "```\n",
    "\n",
    "**Today's Goal**: Understand how changes in embedding space propagate through latent space to affect the final image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing SD 3.5 and FLUX-Schnell Architectures\n",
    "\n",
    "### SD 3.5: Complete Pipeline Flow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph \"EMBEDDING SPACE\"\n",
    "        A[Text Prompt] --> B1[T5-XXL Encoder]\n",
    "        A --> B2[CLIP-L Encoder]\n",
    "        A --> B3[CLIP-G Encoder]\n",
    "        \n",
    "        B1 --> C1[T5 Embeddings<br/>77 Ã— 4096]\n",
    "        B2 --> C2[CLIP-L Embeddings<br/>77 Ã— 768]\n",
    "        B2 --> C3[CLIP-L Pooled<br/>768]\n",
    "        B3 --> C4[CLIP-G Embeddings<br/>77 Ã— 1280]\n",
    "        B3 --> C5[CLIP-G Pooled<br/>1280]\n",
    "        \n",
    "        C1 --> D1[Concatenate]\n",
    "        C2 --> D1\n",
    "        C4 --> D1\n",
    "        \n",
    "        C3 --> D2[Concatenate]\n",
    "        C5 --> D2\n",
    "        \n",
    "        D1 --> E1[Context Embeddings<br/>77 Ã— 6144]\n",
    "        D2 --> E2[Pooled Embeddings<br/>2048]\n",
    "    end\n",
    "    \n",
    "    subgraph \"LATENT SPACE\"\n",
    "        F[Random Noise<br/>Latent Tensor] --> G[MMDiT Transformer]\n",
    "        E1 --> |Cross-Attention| G\n",
    "        E2 --> |AdaLN Modulation| G\n",
    "        \n",
    "        G --> H[Denoising Steps<br/>Iterative Refinement]\n",
    "        H --> I[Clean Latent<br/>Representation]\n",
    "    end\n",
    "    \n",
    "    subgraph \"PIXEL SPACE\"\n",
    "        I --> J[VAE Decoder]\n",
    "        J --> K[RGB Image<br/>1024Ã—1024Ã—3]\n",
    "    end\n",
    "    \n",
    "    style E1 fill:#fff4e1\n",
    "    style E2 fill:#fff4e1\n",
    "    style G fill:#ffe1f5\n",
    "    style H fill:#ffe1f5\n",
    "    style K fill:#d4edda\n",
    "```\n",
    "\n",
    "### FLUX-Schnell: Complete Pipeline Flow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph \"EMBEDDING SPACE\"\n",
    "        A[Text Prompt] --> B1[T5-XXL Encoder]\n",
    "        A --> B2[CLIP-L Encoder]\n",
    "        \n",
    "        B1 --> C1[T5 Embeddings<br/>512 Ã— 4096]\n",
    "        B2 --> C2[CLIP-L Embeddings<br/>77 Ã— 768]\n",
    "        B2 --> C3[CLIP-L Pooled<br/>768]\n",
    "        \n",
    "        C1 --> D1[Concatenate]\n",
    "        C2 --> D1\n",
    "        \n",
    "        D1 --> E1[Context Embeddings<br/>Variable length Ã— 4864]\n",
    "        C3 --> E2[Guidance Vector<br/>768]\n",
    "    end\n",
    "    \n",
    "    subgraph \"LATENT SPACE\"\n",
    "        F[Random Noise<br/>Latent Tensor] --> G[FLUX Transformer]\n",
    "        E1 --> |Attention| G\n",
    "        E2 --> |Guidance Embedding| G\n",
    "        \n",
    "        G --> H[Flow Matching<br/>4 Steps Only]\n",
    "        H --> I[Clean Latent<br/>Representation]\n",
    "    end\n",
    "    \n",
    "    subgraph \"PIXEL SPACE\"\n",
    "        I --> J[VAE Decoder]\n",
    "        J --> K[RGB Image<br/>1024Ã—1024Ã—3]\n",
    "    end\n",
    "    \n",
    "    style E1 fill:#fff4e1\n",
    "    style E2 fill:#fff4e1\n",
    "    style G fill:#ffe1f5\n",
    "    style H fill:#ffe1f5\n",
    "    style K fill:#d4edda\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Differences: SD 3.5 vs FLUX-Schnell\n",
    "\n",
    "| Aspect | SD 3.5 | FLUX-Schnell |\n",
    "|--------|--------|-------------|\n",
    "| **Text Encoders** | T5-XXL, CLIP-L, CLIP-G | T5-XXL, CLIP-L |\n",
    "| **T5 Sequence Length** | 77 tokens | 512 tokens (longer context!) |\n",
    "| **Pooled Embeddings** | CLIP-L + CLIP-G (2048 dims) | CLIP-L only (768 dims) |\n",
    "| **Architecture** | MMDiT (Multimodal Diffusion Transformer) | FLUX Transformer |\n",
    "| **Denoising Process** | Standard diffusion (20-50 steps) | Flow matching (4 steps) |\n",
    "| **Embedding Usage** | Cross-attention + AdaLN modulation | Attention + guidance embedding |\n",
    "|**Speed** | Slower (more steps) | Faster (fewer steps) |\n",
    "\n",
    "### Why These Differences Matter:\n",
    "\n",
    "- **FLUX's longer T5 context** (512 vs 77 tokens) allows more detailed prompts\n",
    "- **SD 3.5's third encoder** (CLIP-G) provides additional style/aesthetic control\n",
    "- **Different architectures** mean embeddings influence generation in different ways\n",
    "- **Flow matching vs diffusion** affects how quickly embeddings guide the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Embeddings Control Latent Diffusion (And How We'll Break It)\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph \"Latent Vandalism: Embedding Manipulation\"\n",
    "        A[Original Prompt] --> B[Generate Embeddings]\n",
    "        B --> C[Save to JSON]\n",
    "        C --> D{Vandalism Techniques}\n",
    "        \n",
    "        D --> E[Scaling Beyond Reason<br/>Ã— 5.0 or Ã— 0.1]\n",
    "        D --> F[Semantic Inversion<br/>Ã— -1 negative space]\n",
    "        D --> G[Impossible Mixing<br/>Blend incompatible prompts]\n",
    "        D --> H[Encoder Ablation<br/>Zero out components]\n",
    "        D --> I[Nonsense Interpolation<br/>Invalid semantic paths]\n",
    "        \n",
    "        E --> J[Damaged Embeddings]\n",
    "        F --> J\n",
    "        G --> J\n",
    "        H --> J\n",
    "        I --> J\n",
    "    end\n",
    "    \n",
    "    subgraph \"Effect on Latent Diffusion: Productive Failures\"\n",
    "        J --> K[Inject into Pipeline]\n",
    "        \n",
    "        K --> L[Transformer Attention<br/>with corrupted guidance]\n",
    "        L --> M{How Damage Manifests}\n",
    "        \n",
    "        M --> N[Extreme influence<br/>Oversaturated semantics]\n",
    "        M --> O[Weak/absent influence<br/>Lost coherence]\n",
    "        M --> P[Inverted semantics<br/>Opposite concepts]\n",
    "        M --> Q[Chimeric concepts<br/>Impossible combinations]\n",
    "        \n",
    "        N --> R[Glitches & Artifacts]\n",
    "        O --> R[Template breakdown]\n",
    "        P --> R[Semantic inversions]\n",
    "        Q --> R[AI weirdness emerges]\n",
    "    end\n",
    "    \n",
    "    R --> S[Corrupted Latent]\n",
    "    S --> T[VAE Decode]\n",
    "    T --> U[Revelatory Failure Image]\n",
    "    \n",
    "    U --> V{What Does This Reveal?}\n",
    "    V --> W[Training data biases]\n",
    "    V --> X[Encoder dependencies]\n",
    "    V --> Y[Inscribed assumptions]\n",
    "    V --> Z[Aesthetic boundaries]\n",
    "    \n",
    "    style J fill:#ffcccc\n",
    "    style L fill:#ffe1f5\n",
    "    style R fill:#fff4e1\n",
    "    style U fill:#d4edda\n",
    "    style V fill:#e1f5ff\n",
    "```\n",
    "\n",
    "### The Epistemology of Productive Damage\n",
    "\n",
    "When we damage embeddings systematically, we're not just making \"bad\" imagesâ€”we're conducting **counterfactual experiments**:\n",
    "\n",
    "- **\"What if this encoder didn't exist?\"** â†’ Zero it out\n",
    "- **\"What if the semantic direction reversed?\"** â†’ Invert it\n",
    "- **\"What if two incompatible concepts merged?\"** â†’ Mix embeddings\n",
    "- **\"What if the signal was too strong/weak?\"** â†’ Scale it\n",
    "\n",
    "Each failure mode reveals **technological inscription**: the assumptions, preferences, and constraints built into the system that are normally invisible in successful generations.\n",
    "\n",
    "As Latour argues, technology embeds social and design choices into material form. When we make these systems fail, we **make the inscription visible**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop Roadmap: From Understanding to Vandalism\n",
    "\n",
    "### Part 1: Understanding the Architecture\n",
    "**Before we break it, we need to know what we're breaking**\n",
    "- Detailed SD 3.5 pipeline breakdown\n",
    "- Detailed FLUX-Schnell pipeline breakdown\n",
    "- Where each embedding type is used and how\n",
    "- The \"normal\" path from text to pixels\n",
    "\n",
    "### Part 2: Generating and Saving Embeddings\n",
    "**Extracting the semantic substrate**\n",
    "- Extract embeddings from prompts\n",
    "- Save as JSON for inspection and manipulation\n",
    "- Understand the numerical structure of semantic space\n",
    "- Observe what \"normal\" embeddings look like\n",
    "\n",
    "### Part 3: Techniques of Productive Damage\n",
    "**Systematic vandalism experiments**\n",
    "\n",
    "- **Experiment 1: Scaling Beyond Reason**  \n",
    "  Push encoders to extremes (10Ã— T5, 0.1Ã— CLIP). What breaks first?\n",
    "  \n",
    "- **Experiment 2: Negative Space Exploration**  \n",
    "  Invert embeddings to explore semantic opposites. What does negative \"cat\" look like?\n",
    "  \n",
    "- **Experiment 3: Impossible Mixtures**  \n",
    "  Combine embeddings from incompatible prompts. Create chimeras text can't express.\n",
    "  \n",
    "- **Experiment 4: Encoder Ablation**  \n",
    "  Zero out specific encoders. What does each one actually contribute?\n",
    "  \n",
    "- **Experiment 5: Interpolation Through Nonsense**  \n",
    "  Morph between concepts via paths that violate semantic coherence.\n",
    "\n",
    "### Part 4: Reading the Glitches\n",
    "**What do failures reveal?**\n",
    "- Same damage, different modelsâ€”comparing SD 3.5 vs FLUX responses\n",
    "- Identifying inscribed assumptions (what the model \"wants\" to generate)\n",
    "- Finding aesthetic boundaries (where template culture stops)\n",
    "- Understanding model-specific sensitivities\n",
    "- Documenting **AI weirdness** as epistemic resource\n",
    "\n",
    "### Part 5: Discussion\n",
    "**Productive damage as critical method**\n",
    "- What did breaking things teach us?\n",
    "- How do glitches expose technological inscription?\n",
    "- Can we create a taxonomy of revealing failures?\n",
    "- What are the ethics of adversarial exploration?\n",
    "\n",
    "---\n",
    "\n",
    "**Let's begin by understanding what we're about to vandalize!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: SD 3.5 Pipeline Deep Dive\n",
    "\n",
    "## 1. Standard SD 3.5 Pipeline (Normal Text-to-Image)\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Text Prompt] --> B[T5-XXL Encoder]\n",
    "    A --> C[CLIP-L Encoder]\n",
    "    A --> D[CLIP-G Encoder]\n",
    "    \n",
    "    B --> E[T5 Text Embeddings<br/>77 tokens Ã— 4096 dims]\n",
    "    C --> F[CLIP-L Text Embeddings<br/>77 tokens Ã— 768 dims]\n",
    "    C --> G[CLIP-L Pooled<br/>Single vector: 768 dims]\n",
    "    D --> H[CLIP-G Text Embeddings<br/>77 tokens Ã— 1280 dims]\n",
    "    D --> I[CLIP-G Pooled<br/>Single vector: 1280 dims]\n",
    "    \n",
    "    E --> J[Concatenate Text Embeddings]\n",
    "    F --> J\n",
    "    H --> J\n",
    "    \n",
    "    J --> K[Combined Context<br/>77 tokens Ã— 6144 dims<br/>4096+768+1280]\n",
    "    \n",
    "    G --> L[Concatenate Pooled]\n",
    "    I --> L\n",
    "    \n",
    "    L --> M[Pooled Embeddings<br/>2048 dims<br/>768+1280]\n",
    "    \n",
    "    K --> N[MMDiT Transformer]\n",
    "    M --> N\n",
    "    \n",
    "    O[Random Latent Noise] --> N\n",
    "    \n",
    "    N --> P[Denoising Process<br/>Multiple Steps]\n",
    "    P --> Q[Final Latent]\n",
    "    Q --> R[VAE Decoder]\n",
    "    R --> S[Output Image]\n",
    "    \n",
    "    style K fill:#e1f5ff\n",
    "    style M fill:#ffe1f5\n",
    "    style N fill:#fff4e1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Our Modified Pipeline (Direct Embedding Injection)\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[T5 Embeddings JSON<br/>77 Ã— 4096] --> B[Load & Parse]\n",
    "    C[CLIP-L Embeddings JSON<br/>77 Ã— 768] --> D[Load & Parse]\n",
    "    E[CLIP-L Pooled JSON<br/>768 dims] --> F[Load & Parse]\n",
    "    G[CLIP-G Embeddings JSON<br/>77 Ã— 1280] --> H[Load & Parse]\n",
    "    I[CLIP-G Pooled JSON<br/>1280 dims] --> J[Load & Parse]\n",
    "    \n",
    "    B --> K[Optional: Scale/Modify<br/>e.g., Ã— 1.5, invert, etc.]\n",
    "    D --> L[Optional: Scale/Modify]\n",
    "    F --> M[Optional: Scale/Modify]\n",
    "    H --> N[Optional: Scale/Modify]\n",
    "    J --> O[Optional: Scale/Modify]\n",
    "    \n",
    "    K --> P[Concatenate Text Embeddings]\n",
    "    L --> P\n",
    "    N --> P\n",
    "    \n",
    "    P --> Q[Combined Context<br/>77 Ã— 6144 dims]\n",
    "    \n",
    "    M --> R[Concatenate Pooled]\n",
    "    O --> R\n",
    "    \n",
    "    R --> S[Pooled Embeddings<br/>2048 dims]\n",
    "    \n",
    "    Q --> T[MMDiT Transformer]\n",
    "    S --> T\n",
    "    \n",
    "    U[Random Latent Noise] --> T\n",
    "    \n",
    "    T --> V[Denoising Process]\n",
    "    V --> W[Final Latent]\n",
    "    W --> X[VAE Decoder]\n",
    "    X --> Y[Output Image]\n",
    "    \n",
    "    style K fill:#ffcccc\n",
    "    style L fill:#ffcccc\n",
    "    style M fill:#ffcccc\n",
    "    style N fill:#ffcccc\n",
    "    style O fill:#ffcccc\n",
    "    style Q fill:#e1f5ff\n",
    "    style S fill:#ffe1f5\n",
    "    style T fill:#fff4e1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detailed View: How Embeddings Flow Into MMDiT\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    subgraph \"Text Embeddings (Context)\"\n",
    "        A[T5-XXL<br/>77 Ã— 4096]\n",
    "        B[CLIP-L<br/>77 Ã— 768]\n",
    "        C[CLIP-G<br/>77 Ã— 1280]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Pooled Embeddings (Global Conditioning)\"\n",
    "        D[CLIP-L Pooled<br/>768]\n",
    "        E[CLIP-G Pooled<br/>1280]\n",
    "    end\n",
    "    \n",
    "    A --> F[Concat]\n",
    "    B --> F\n",
    "    C --> F\n",
    "    \n",
    "    F --> G[Context Vector<br/>77 Ã— 6144]\n",
    "    \n",
    "    D --> H[Concat]\n",
    "    E --> H\n",
    "    \n",
    "    H --> I[Pooled Vector<br/>2048]\n",
    "    \n",
    "    subgraph \"MMDiT Transformer Block\"\n",
    "        G --> J[Cross-Attention<br/>Keys & Values]\n",
    "        K[Noisy Latent] --> L[Self-Attention<br/>Queries]\n",
    "        L --> M[Attention]\n",
    "        J --> M\n",
    "        M --> N[Feed Forward]\n",
    "        \n",
    "        I --> O[AdaLN Modulation<br/>Scale & Shift]\n",
    "        O --> L\n",
    "        O --> N\n",
    "    end\n",
    "    \n",
    "    N --> P[Denoised Output]\n",
    "    \n",
    "    style G fill:#e1f5ff\n",
    "    style I fill:#ffe1f5\n",
    "    style M fill:#d4edda\n",
    "    style O fill:#fff3cd\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Key Roles of Each Embedding Type\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph \"T5-XXL Embeddings\"\n",
    "        A[Rich Semantic Understanding<br/>4096 dimensions per token]\n",
    "        A --> A1[Detailed descriptions]\n",
    "        A --> A2[Complex relationships]\n",
    "        A --> A3[Nuanced concepts]\n",
    "    end\n",
    "    \n",
    "    subgraph \"CLIP-L Embeddings\"\n",
    "        B[Visual-Text Alignment<br/>768 dimensions per token]\n",
    "        B --> B1[Object recognition]\n",
    "        B --> B2[Style understanding]\n",
    "        B --> B3[Composition hints]\n",
    "    end\n",
    "    \n",
    "    subgraph \"CLIP-G Embeddings\"\n",
    "        C[High-Level Visual Concepts<br/>1280 dimensions per token]\n",
    "        C --> C1[Artistic style]\n",
    "        C --> C2[Overall aesthetics]\n",
    "        C --> C3[Global structure]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Pooled Embeddings\"\n",
    "        D[Global Image Characteristics]\n",
    "        D --> D1[Overall style modulation]\n",
    "        D --> D2[Quality/aesthetic level]\n",
    "        D --> D3[Conditioning strength]\n",
    "    end\n",
    "    \n",
    "    A1 --> E[Cross-Attention]\n",
    "    A2 --> E\n",
    "    A3 --> E\n",
    "    B1 --> E\n",
    "    B2 --> E\n",
    "    B3 --> E\n",
    "    C1 --> E\n",
    "    C2 --> E\n",
    "    C3 --> E\n",
    "    \n",
    "    D1 --> F[AdaLN Modulation]\n",
    "    D2 --> F\n",
    "    D3 --> F\n",
    "    \n",
    "    E --> G[Image Generation]\n",
    "    F --> G\n",
    "    \n",
    "    style E fill:#e1f5ff\n",
    "    style F fill:#ffe1f5\n",
    "    style G fill:#d4edda\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vandalism Techniques: From Manipulation to Revelation\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Saved Embeddings JSON] --> B{Vandalism Technique}\n",
    "    \n",
    "    B --> C[Extreme Scaling]\n",
    "    B --> D[Semantic Inversion]\n",
    "    B --> E[Impossible Mixing]\n",
    "    B --> F[Encoder Ablation]\n",
    "    B --> G[Nonsense Interpolation]\n",
    "    \n",
    "    C --> C1[\"T5 Ã— 10, CLIP Ã— 0.01<br/>Push encoders to extremes\"]\n",
    "    D --> D1[\"Ã— -1 all embeddings<br/>Explore negative semantic space\"]\n",
    "    E --> E1[\"Blend 'sunset' + 'theorem'<br/>Force incompatible concepts\"]\n",
    "    F --> F1[\"Zero T5, keep CLIP<br/>Isolate encoder roles\"]\n",
    "    G --> G1[\"Interpolate via invalid paths<br/>Break semantic continuity\"]\n",
    "    \n",
    "    C1 --> H[Damaged Embeddings]\n",
    "    D1 --> H\n",
    "    E1 --> H\n",
    "    F1 --> H\n",
    "    G1 --> H\n",
    "    \n",
    "    H --> I[Inject into Pipeline]\n",
    "    I --> J[Generate Image]\n",
    "    J --> K{Productive Failures}\n",
    "    \n",
    "    K --> L[Glitch Aesthetics<br/>Visual artifacts reveal structure]\n",
    "    K --> M[Template Breakdown<br/>Where polished norms collapse]\n",
    "    K --> N[Inscribed Assumptions<br/>What the model 'wants' to do]\n",
    "    K --> O[AI Weirdness<br/>Epistemic value of failure]\n",
    "    \n",
    "    L --> P[Research Insights]\n",
    "    M --> P\n",
    "    N --> P\n",
    "    O --> P\n",
    "    \n",
    "    P --> Q[\"Understanding through damage:<br/>What becomes visible when systems break?\"]\n",
    "    \n",
    "    style H fill:#ffcccc\n",
    "    style J fill:#ffe1f5\n",
    "    style K fill:#fff4e1\n",
    "    style L fill:#e1f5ff\n",
    "    style M fill:#e1f5ff\n",
    "    style N fill:#e1f5ff\n",
    "    style O fill:#e1f5ff\n",
    "    style Q fill:#d4edda\n",
    "```\n",
    "\n",
    "### Reading Glitches as Data\n",
    "\n",
    "Each type of damage produces characteristic failures:\n",
    "\n",
    "- **Extreme scaling** â†’ Oversaturation or complete loss of semantic guidance  \n",
    "  *Reveals: Sensitivity thresholds, encoder balance requirements*\n",
    "\n",
    "- **Semantic inversion** â†’ Opposite concepts, anti-patterns  \n",
    "  *Reveals: Whether semantic directions are truly bidirectional*\n",
    "\n",
    "- **Impossible mixing** â†’ Chimeric forms, visual contradictions  \n",
    "  *Reveals: How models handle semantic conflicts, training data gaps*\n",
    "\n",
    "- **Encoder ablation** â†’ Missing styles, lost coherence, altered aesthetics  \n",
    "  *Reveals: Individual encoder contributions, dependencies*\n",
    "\n",
    "- **Nonsense interpolation** â†’ Unexpected transitional states  \n",
    "  *Reveals: Non-linear structure of embedding space*\n",
    "\n",
    "### The Charm of AI Weirdness\n",
    "\n",
    "As Janelle Shane observes, when AI systems fail, they often fail in **informative ways**. The strange, the broken, the glitchedâ€”these aren't just mistakes. They're **windows into the system's implicit knowledge**.\n",
    "\n",
    "A perfectly coherent image tells you the system works. A **revelatory failure** tells you *how* it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Embedding Dimensions:\n",
    "- **T5-XXL**: 77 tokens Ã— 4096 dimensions (SD 3.5) / 512 tokens Ã— 4096 (FLUX)\n",
    "- **CLIP-L**: 77 tokens Ã— 768 dimensions + 768 pooled\n",
    "- **CLIP-G**: 77 tokens Ã— 1280 dimensions + 1280 pooled (SD 3.5 only)\n",
    "\n",
    "### How They're Used:\n",
    "1. **Text embeddings** (concatenated) â†’ Cross-attention in transformer\n",
    "2. **Pooled embeddings** (concatenated) â†’ Global conditioning (AdaLN/guidance)\n",
    "\n",
    "### Workshop Method:\n",
    "- **Productive damage** as epistemological tool\n",
    "- **Iatrogenic techniques** to probe system boundaries\n",
    "- **Glitch aesthetics** as revelatory moments\n",
    "- **Counterfactual experiments** to understand inscription\n",
    "\n",
    "### What Vandalism Reveals:\n",
    "- Direct manipulation bypasses text encoding limitations\n",
    "- Systematic damage exposes training data biases\n",
    "- Failures make visible the inscribed norms and assumptions\n",
    "- AI weirdness provides epistemic value beyond polish\n",
    "- Each encoder contributes distinct, separable semantic information\n",
    "- Template culture's boundaries become legible where it breaks\n",
    "\n",
    "---\n",
    "\n",
    "**Remember:** The goal isn't to make \"better\" imagesâ€”it's to understand what \"better\" means to these systems, and to find creative freedom in the spaces where that definition breaks down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Framework: References\n",
    "\n",
    "This workshop draws on several theoretical traditions:\n",
    "\n",
    "### Glitch Studies\n",
    "- **Menkman, Rosa.** *The Glitch Moment(um)*. Network Notebooks, 2011.\n",
    "  - Glitches as revelatory moments that expose normally invisible structures\n",
    "  - Productive failures as aesthetic and epistemic resources\n",
    "\n",
    "### Template Culture\n",
    "- **Grund, Katja and Scherffig, Lasse.** Work on template culture and standardized aesthetics in generative AI\n",
    "  - How models reproduce homogeneous visual languages\n",
    "  - The political economy of aesthetic standardization\n",
    "\n",
    "### AI Weirdness\n",
    "- **Shane, Janelle.** Research on AI failures and unexpected behaviors\n",
    "  - The epistemic value of AI mistakes\n",
    "  - How failures reveal system structure\n",
    "\n",
    "### Science and Technology Studies\n",
    "- **Latour, Bruno.** \"Technology is society made durable.\" *Sociological Review*, 1990.\n",
    "  - Technological inscription: How values and choices become embedded in systems\n",
    "  - Making visible the social and political dimensions of technical artifacts\n",
    "\n",
    "### Iatrogenic Methods\n",
    "- Medical concept of harm caused by treatment itself, repurposed as deliberate intervention\n",
    "  - Systematic damage as research methodology\n",
    "  - Counterfactual reasoning through controlled failures\n",
    "\n",
    "---\n",
    "\n",
    "### About This Workshop\n",
    "\n",
    "**Workshop by Laura Wagner**\n",
    "\n",
    "ðŸ”— Website: [laurajul.github.io](https://laurajul.github.io/)  \n",
    "ðŸ“¦ Repository: [github.com/laurajul/latent-vandalism-workshop](https://github.com/laurajul/latent-vandalism-workshop)\n",
    "\n",
    "For questions, feedback, or collaborations on productive damage to generative AI systems, please reach out via the website or repository.\n",
    "\n",
    "---\n",
    "\n",
    "*\"The charm of AI weirdness is not just in the strange outputs, but in what those outputs reveal about the system that produced them.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
