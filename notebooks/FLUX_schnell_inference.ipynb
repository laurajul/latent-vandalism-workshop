{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLUX-Schnell Inference\n",
    "\n",
    "This notebook lets you:\n",
    "\n",
    "2. Save/load embeddings\n",
    "5. Generate images with FLUX using modified embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Embeddings\n",
    "        T5[T5 Embedding<br/>JSON<br/>512 × 4096]\n",
    "        CLIP[CLIP Embedding<br/>JSON<br/>77 × 768]\n",
    "    end\n",
    "\n",
    "    subgraph FLUX Pipeline\n",
    "        T5E[T5 Prompt<br/>Embeds]\n",
    "        PE[Pooled<br/>Embeds]\n",
    "        DT[Diffusion<br/>Transformer]\n",
    "        VAE[VAE<br/>Decoder]\n",
    "    end\n",
    "\n",
    "    T5 --> T5E\n",
    "    CLIP -->|EOS token| PE\n",
    "    \n",
    "    T5E -->|cross-attention| DT\n",
    "    PE -->|global conditioning| DT\n",
    "    \n",
    "    DT --> VAE --> IMG[Generated Image]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Run this cell first to install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T11:48:59.649120Z",
     "iopub.status.busy": "2026-01-15T11:48:59.648714Z",
     "iopub.status.idle": "2026-01-15T11:48:59.655265Z",
     "shell.execute_reply": "2026-01-15T11:48:59.654521Z",
     "shell.execute_reply.started": "2026-01-15T11:48:59.649101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Models directory: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/data/models\n",
      "T5 path: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/data/models/t5-v1_1-xxl\n",
      "FLUX path: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/data/models/FLUX.1-schnell\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "from diffusers import FluxPipeline\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Image as IPImage\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load models path from config\n",
    "current_dir = Path.cwd()\n",
    "models_path_file = current_dir.parent / \"misc/paths/models.txt\"\n",
    "with open(models_path_file, 'r') as f:\n",
    "    models_path = f.read().strip()\n",
    "MODELS_DIR = current_dir.parent / models_path\n",
    "\n",
    "T5_MODEL_PATH = os.path.join(MODELS_DIR, \"t5-v1_1-xxl\")\n",
    "CLIP_MODEL_PATH = os.path.join(MODELS_DIR, \"clip\")\n",
    "FLUX_MODEL_PATH = os.path.join(MODELS_DIR, \"FLUX.1-schnell\")\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "print(f\"Models directory: {os.path.abspath(MODELS_DIR)}\")\n",
    "print(f\"T5 path: {os.path.abspath(T5_MODEL_PATH)}\")\n",
    "print(f\"FLUX path: {os.path.abspath(FLUX_MODEL_PATH)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T12:46:06.535933Z",
     "iopub.status.busy": "2026-01-13T12:46:06.535715Z",
     "iopub.status.idle": "2026-01-13T12:46:06.538143Z",
     "shell.execute_reply": "2026-01-13T12:46:06.537652Z",
     "shell.execute_reply.started": "2026-01-13T12:46:06.535918Z"
    }
   },
   "source": [
    "### Download Flux schnell from huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T12:45:13.570160Z",
     "iopub.status.busy": "2026-01-13T12:45:13.569947Z",
     "iopub.status.idle": "2026-01-13T12:45:13.572499Z",
     "shell.execute_reply": "2026-01-13T12:45:13.572011Z",
     "shell.execute_reply.started": "2026-01-13T12:45:13.570145Z"
    }
   },
   "source": [
    "FYI: if you don't have the model locally in ../data/models/FLUX.1-schnell, you have to download it manually from huggingface, because it is in a gated repository, put your huggingface access token in ../misc/credentials/hf.txt to download the model from the repository. THis process only works if you have previously requested access to the repository from huggingface and have generated the access token with the right permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T11:49:02.745960Z",
     "iopub.status.busy": "2026-01-15T11:49:02.745723Z",
     "iopub.status.idle": "2026-01-15T11:49:03.079100Z",
     "shell.execute_reply": "2026-01-15T11:49:03.078428Z",
     "shell.execute_reply.started": "2026-01-15T11:49:02.745944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for HF token at: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/misc/credentials/hf.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Load Hugging Face token from file\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the token file path\n",
    "current_dir = Path.cwd()\n",
    "token_file = current_dir.parent / \"misc/credentials/hf.txt\"\n",
    "\n",
    "print(f\"Looking for HF token at: {token_file}\")\n",
    "\n",
    "if token_file.exists():\n",
    "    with open(token_file, 'r') as f:\n",
    "        hf_token = f.read().strip()\n",
    "    \n",
    "    # Set the token as an environment variable\n",
    "    os.environ['HF_TOKEN'] = hf_token\n",
    "    \n",
    "    # Also login using huggingface_hub\n",
    "    from huggingface_hub import login\n",
    "    login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T11:49:10.586416Z",
     "iopub.status.busy": "2026-01-15T11:49:10.586153Z",
     "iopub.status.idle": "2026-01-15T11:52:14.688911Z",
     "shell.execute_reply": "2026-01-15T11:52:14.612831Z",
     "shell.execute_reply.started": "2026-01-15T11:49:10.586399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e718b2f63a4ad683212a2dc518b574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bb247348a842b3bc93365d798a8e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ccdb1844bf247148ccd2619b02fa380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load FLUX\n",
    "try:\n",
    "    if not os.path.exists(FLUX_MODEL_PATH):\n",
    "        flux_pipe = FluxPipeline.from_pretrained(\n",
    "            \"black-forest-labs/FLUX.1-schnell\",\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        flux_pipe.save_pretrained(FLUX_MODEL_PATH)\n",
    "    else:\n",
    "        flux_pipe = FluxPipeline.from_pretrained(\n",
    "            FLUX_MODEL_PATH,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            local_files_only=True\n",
    "        )\n",
    "    \n",
    "    flux_pipe = flux_pipe.to(device)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading FLUX: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Image from embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T11:52:57.431502Z",
     "iopub.status.busy": "2026-01-15T11:52:57.431284Z",
     "iopub.status.idle": "2026-01-15T11:52:57.435408Z",
     "shell.execute_reply": "2026-01-15T11:52:57.434686Z",
     "shell.execute_reply.started": "2026-01-15T11:52:57.431486Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup directories\n",
    "T5_EMBEDDINGS_DIR = current_dir.parent / \"data/embeddings/T5\"\n",
    "CLIP_EMBEDDINGS_DIR = current_dir.parent / \"data/embeddings/CLIP\"\n",
    "\n",
    "# Global variables for loaded embeddings\n",
    "loaded_t5_embedding = None\n",
    "loaded_clip_embedding = None\n",
    "loaded_t5_prompt = None\n",
    "loaded_clip_prompt = None\n",
    "\n",
    "# Setup output directory\n",
    "OUTPUT_IMAGES_DIR = current_dir.parent / \"output/images\"\n",
    "os.makedirs(OUTPUT_IMAGES_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:39:45.512371Z",
     "iopub.status.busy": "2026-01-15T12:39:45.512188Z",
     "iopub.status.idle": "2026-01-15T12:39:45.550972Z",
     "shell.execute_reply": "2026-01-15T12:39:45.550562Z",
     "shell.execute_reply.started": "2026-01-15T12:39:45.512357Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee587c6032847a5af2399ce8ceb7b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>Embedding Selection & Image Generation</h2>'), HTML(value='<h3>1. T5 Embedding …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==================== HELPER FUNCTIONS ====================\n",
    "\n",
    "def parse_embedding_filename(filename):\n",
    "    \"\"\"\n",
    "    Parse embedding filename to extract tokens and manipulation.\n",
    "    Returns (tokens_string, manipulation_string)\n",
    "    \n",
    "    Examples:\n",
    "    - 'a_red_cat_sitting.json' -> ('aredcatsitting', '')\n",
    "    - 'a_red_cat_sitting_scaled_2.0x.json' -> ('aredcatsitting', 'scaled')\n",
    "    - 'an_elephant_inverted.json' -> ('anelephant', 'inverted')\n",
    "    - 'an_elephant_zeroed_30pct.json' -> ('anelephant', 'zeroed')\n",
    "    - 'examples/a_red_cat.json' -> ('aredcat', '')  # handles subdirectories\n",
    "    \"\"\"\n",
    "    if not filename:\n",
    "        return ('unknown', '')\n",
    "    \n",
    "    # Extract just the filename (strip directory path)\n",
    "    basename = os.path.basename(filename)\n",
    "    \n",
    "    # Remove .json extension\n",
    "    base_name = basename.rsplit('.', 1)[0]\n",
    "    \n",
    "    # Split by underscore\n",
    "    parts = base_name.split('_')\n",
    "    \n",
    "    # First 4 parts are the tokens - join without underscores\n",
    "    if len(parts) <= 4:\n",
    "        tokens = ''.join(parts)\n",
    "        return (tokens, '')\n",
    "    \n",
    "    tokens = ''.join(parts[:4])\n",
    "    \n",
    "    # Get manipulation type (simplified)\n",
    "    # Just take the first part after the tokens\n",
    "    manipulation_parts = parts[4:]\n",
    "    if manipulation_parts:\n",
    "        # Get the manipulation type (first word after tokens)\n",
    "        manipulation = manipulation_parts[0]\n",
    "    else:\n",
    "        manipulation = ''\n",
    "    \n",
    "    return (tokens, manipulation)\n",
    "\n",
    "# ==================== T5 EMBEDDING WIDGETS ====================\n",
    "\n",
    "# T5 embedding selection\n",
    "t5_file_dropdown = widgets.Dropdown(\n",
    "    options=[],\n",
    "    description='T5 Embedding:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='600px')\n",
    ")\n",
    "\n",
    "load_t5_button = widgets.Button(\n",
    "    description='Load T5',\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "t5_load_output = widgets.Output()\n",
    "\n",
    "# Populate T5 dropdown options (including subfolders)\n",
    "if T5_EMBEDDINGS_DIR.exists():\n",
    "    t5_files = sorted([str(f.relative_to(T5_EMBEDDINGS_DIR)) for f in T5_EMBEDDINGS_DIR.glob('**/*.json')])\n",
    "    t5_file_dropdown.options = t5_files\n",
    "\n",
    "def load_t5_embedding_file(b):\n",
    "    global loaded_t5_embedding, loaded_t5_prompt\n",
    "    \n",
    "    with t5_load_output:\n",
    "        t5_load_output.clear_output()\n",
    "        \n",
    "        filename = t5_file_dropdown.value\n",
    "        if not filename:\n",
    "            print(\"❌ No file selected!\")\n",
    "            return\n",
    "        \n",
    "        filepath = T5_EMBEDDINGS_DIR / filename\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            loaded_t5_embedding = np.array(data['embedding'])\n",
    "            loaded_t5_prompt = data.get('prompt', 'Unknown')\n",
    "            \n",
    "            print(f\"✓ Loaded T5 embedding!\")\n",
    "            print(f\"  File: {filename}\")\n",
    "            print(f\"  Prompt: '{loaded_t5_prompt}'\")\n",
    "            print(f\"  Shape: {loaded_t5_embedding.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading T5 embedding: {e}\")\n",
    "\n",
    "load_t5_button.on_click(load_t5_embedding_file)\n",
    "\n",
    "# ==================== CLIP EMBEDDING WIDGETS ====================\n",
    "\n",
    "# CLIP embedding selection\n",
    "clip_file_dropdown = widgets.Dropdown(\n",
    "    options=[],\n",
    "    description='CLIP Embedding:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='600px')\n",
    ")\n",
    "\n",
    "load_clip_button = widgets.Button(\n",
    "    description='Load CLIP',\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "clip_load_output = widgets.Output()\n",
    "\n",
    "# Populate CLIP dropdown options (including subfolders)\n",
    "if CLIP_EMBEDDINGS_DIR.exists():\n",
    "    clip_files = sorted([str(f.relative_to(CLIP_EMBEDDINGS_DIR)) for f in CLIP_EMBEDDINGS_DIR.glob('**/*.json')])\n",
    "    clip_file_dropdown.options = clip_files\n",
    "\n",
    "def load_clip_embedding_file(b):\n",
    "    global loaded_clip_embedding, loaded_clip_prompt\n",
    "    \n",
    "    with clip_load_output:\n",
    "        clip_load_output.clear_output()\n",
    "        \n",
    "        filename = clip_file_dropdown.value\n",
    "        if not filename:\n",
    "            print(\"❌ No file selected!\")\n",
    "            return\n",
    "        \n",
    "        filepath = CLIP_EMBEDDINGS_DIR / filename\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            loaded_clip_embedding = np.array(data['embedding'])\n",
    "            loaded_clip_prompt = data.get('prompt', 'Unknown')\n",
    "            \n",
    "            print(f\"✓ Loaded CLIP embedding!\")\n",
    "            print(f\"  File: {filename}\")\n",
    "            print(f\"  Prompt: '{loaded_clip_prompt}'\")\n",
    "            print(f\"  Shape: {loaded_clip_embedding.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading CLIP embedding: {e}\")\n",
    "\n",
    "load_clip_button.on_click(load_clip_embedding_file)\n",
    "\n",
    "# ==================== GENERATION CONTROLS ====================\n",
    "\n",
    "seed_input = widgets.IntText(\n",
    "    value=42,\n",
    "    description='Seed:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "steps_input = widgets.IntSlider(\n",
    "    value=4,\n",
    "    min=1,\n",
    "    max=50,\n",
    "    step=1,\n",
    "    description='Inference Steps:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "width_input = widgets.IntText(\n",
    "    value=1024,\n",
    "    description='Width:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "height_input = widgets.IntText(\n",
    "    value=1024,\n",
    "    description='Height:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "generate_button = widgets.Button(\n",
    "    description='Generate Image',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='300px', height='50px')\n",
    ")\n",
    "\n",
    "generation_output = widgets.Output()\n",
    "\n",
    "def generate_from_loaded_embeddings(b):\n",
    "    \"\"\"\n",
    "    Generate image using loaded T5 and CLIP embeddings.\n",
    "    \"\"\"\n",
    "    with generation_output:\n",
    "        generation_output.clear_output()\n",
    "        \n",
    "        if 'flux_pipe' not in globals():\n",
    "            print(\"❌ FLUX not loaded!\")\n",
    "            return\n",
    "        \n",
    "        if loaded_t5_embedding is None:\n",
    "            print(\"❌ No T5 embedding loaded! Load a T5 embedding first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Generating image from loaded embeddings...\")\n",
    "        print(f\"  Seed: {seed_input.value}\")\n",
    "        print(f\"  Inference Steps: {steps_input.value}\")\n",
    "        print(f\"  Dimensions: {width_input.value}x{height_input.value}\")\n",
    "        print(f\"  T5 embedding shape: {loaded_t5_embedding.shape}\")\n",
    "        print(f\"  T5 prompt: '{loaded_t5_prompt}'\")\n",
    "        \n",
    "        # Convert T5 numpy to torch tensor with bfloat16\n",
    "        t5_tensor = torch.from_numpy(loaded_t5_embedding.astype(np.float32)).to(\n",
    "            device=device,\n",
    "            dtype=torch.bfloat16\n",
    "        )\n",
    "        \n",
    "        # Add batch dimension: [512, 4096] -> [1, 512, 4096]\n",
    "        t5_tensor = t5_tensor.unsqueeze(0)\n",
    "        \n",
    "        # Process CLIP embeddings\n",
    "        print(\"\\nProcessing CLIP pooled embeddings...\")\n",
    "        \n",
    "        if loaded_clip_embedding is not None:\n",
    "            # Use loaded CLIP embedding\n",
    "            print(f\"  Using loaded CLIP embedding\")\n",
    "            print(f\"  CLIP embedding shape: {loaded_clip_embedding.shape}\")\n",
    "            print(f\"  CLIP prompt: '{loaded_clip_prompt}'\")\n",
    "            \n",
    "            # Use last token embedding as pooled embedding (EOS token)\n",
    "            pooled_embeds = torch.from_numpy(\n",
    "                loaded_clip_embedding[-1:].astype(np.float32)\n",
    "            ).to(device=device, dtype=torch.bfloat16)\n",
    "            \n",
    "        else:\n",
    "            # Fall back to generating from T5 prompt using CLIP\n",
    "            print(f\"  No CLIP embedding loaded, generating from T5 prompt using CLIP model...\")\n",
    "            \n",
    "            if loaded_t5_prompt:\n",
    "                with torch.no_grad():\n",
    "                    (\n",
    "                        _,\n",
    "                        pooled_embeds,\n",
    "                        _,\n",
    "                    ) = flux_pipe.encode_prompt(\n",
    "                        prompt=loaded_t5_prompt,\n",
    "                        prompt_2=None,\n",
    "                        device=device,\n",
    "                        num_images_per_prompt=1,\n",
    "                        prompt_embeds=None,\n",
    "                        pooled_prompt_embeds=None,\n",
    "                        max_sequence_length=512,\n",
    "                    )\n",
    "            else:\n",
    "                print(\"❌ No CLIP embedding and no prompt available!\")\n",
    "                return\n",
    "        \n",
    "        print(f\"  Pooled embeddings shape: {pooled_embeds.shape}\")\n",
    "        \n",
    "        # Construct filename from embeddings\n",
    "        t5_tokens, t5_manip = parse_embedding_filename(t5_file_dropdown.value)\n",
    "        clip_tokens, clip_manip = parse_embedding_filename(clip_file_dropdown.value)\n",
    "        \n",
    "        # Build filename: t5tokens_t5manip_cliptokens_clipmanip.png\n",
    "        filename_parts = [t5_tokens]\n",
    "        if t5_manip:\n",
    "            filename_parts.append(t5_manip)\n",
    "        filename_parts.append(clip_tokens)\n",
    "        if clip_manip:\n",
    "            filename_parts.append(clip_manip)\n",
    "        \n",
    "        filename = '_'.join(filename_parts) + '.png'\n",
    "        output_filepath = OUTPUT_IMAGES_DIR / filename\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        output_filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Generate image\n",
    "        try:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"Running FLUX diffusion...\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "            \n",
    "            image = flux_pipe(\n",
    "                prompt_embeds=t5_tensor,\n",
    "                pooled_prompt_embeds=pooled_embeds,\n",
    "                num_inference_steps=steps_input.value,\n",
    "                guidance_scale=0.0,\n",
    "                height=height_input.value,\n",
    "                width=width_input.value,\n",
    "                generator=torch.manual_seed(seed_input.value)\n",
    "            ).images[0]\n",
    "            \n",
    "            image.save(output_filepath)\n",
    "            print(f\"✓ Image generated and saved!\")\n",
    "            print(f\"  Path: {output_filepath}\")\n",
    "            print(f\"  Filename: {filename}\\n\")\n",
    "            \n",
    "            display(image)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error generating image: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "generate_button.on_click(generate_from_loaded_embeddings)\n",
    "\n",
    "# ==================== DISPLAY UNIFIED INTERFACE ====================\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h2>Embedding Selection & Image Generation</h2>\"),\n",
    "    \n",
    "    # T5 Embedding Section\n",
    "    widgets.HTML(\"<h3>1. T5 Embedding (Required)</h3>\"),\n",
    "    t5_file_dropdown,\n",
    "    load_t5_button,\n",
    "    t5_load_output,\n",
    "    \n",
    "    # CLIP Embedding Section\n",
    "    widgets.HTML(\"<br><h3>2. CLIP Embedding (Optional - auto-generated if not loaded)</h3>\"),\n",
    "    clip_file_dropdown,\n",
    "    load_clip_button,\n",
    "    clip_load_output,\n",
    "    \n",
    "    # Generation Controls Section\n",
    "    widgets.HTML(\"<br><h3>3. Generation Controls</h3>\"),\n",
    "    seed_input,\n",
    "    steps_input,\n",
    "    widgets.HTML(\"<br><b>Image Dimensions:</b>\"),\n",
    "    widgets.HBox([width_input, height_input]),\n",
    "    widgets.HTML(\"<br>\"),\n",
    "    generate_button,\n",
    "    \n",
    "    # Output Section\n",
    "    widgets.HTML(\"<br><h3>Generated Image</h3>\"),\n",
    "    generation_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generation\n",
    "\n",
    "Generate multiple images with different embedding combinations and organize them into subfolders based on token length:\n",
    "- `short/` - Generated from short text prompts (encoded at runtime)\n",
    "- `512_tokens/` - Generated from pre-saved T5 embeddings (512 token sequences)\n",
    "- `77_tokens/` - Generated from pre-saved CLIP embeddings (77 token sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==================== BATCH GENERATION WITH SUBFOLDER ORGANIZATION ====================\n",
    "\n",
    "# # Create output subdirectories\n",
    "# OUTPUT_SHORT_DIR = OUTPUT_IMAGES_DIR / \"short\"\n",
    "# OUTPUT_512_DIR = OUTPUT_IMAGES_DIR / \"512_tokens\"\n",
    "# OUTPUT_77_DIR = OUTPUT_IMAGES_DIR / \"77_tokens\"\n",
    "\n",
    "# for dir_path in [OUTPUT_SHORT_DIR, OUTPUT_512_DIR, OUTPUT_77_DIR]:\n",
    "#     os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# # ==================== T5 EMBEDDING SOURCE SELECTION ====================\n",
    "\n",
    "# t5_source_radio = widgets.RadioButtons(\n",
    "#     options=[\n",
    "#         ('Use T5 embedding file (512 tokens)', '512_tokens'),\n",
    "#         ('Generate from short prompt', 'short')\n",
    "#     ],\n",
    "#     value='512_tokens',\n",
    "#     description='T5 Source:',\n",
    "#     style={'description_width': 'initial'},\n",
    "#     layout=widgets.Layout(width='400px')\n",
    "# )\n",
    "\n",
    "# # Short prompt input (for generating T5 at runtime)\n",
    "# batch_short_prompt = widgets.Text(\n",
    "#     value='',\n",
    "#     placeholder='Enter short prompt (e.g., \"a red cat\")',\n",
    "#     description='Short Prompt:',\n",
    "#     style={'description_width': 'initial'},\n",
    "#     layout=widgets.Layout(width='600px')\n",
    "# )\n",
    "\n",
    "# # T5 file selection (for pre-saved embeddings)\n",
    "# batch_t5_dropdown = widgets.Dropdown(\n",
    "#     options=[],\n",
    "#     description='T5 File:',\n",
    "#     style={'description_width': 'initial'},\n",
    "#     layout=widgets.Layout(width='600px')\n",
    "# )\n",
    "\n",
    "# # Populate T5 dropdown\n",
    "# if T5_EMBEDDINGS_DIR.exists():\n",
    "#     t5_files = sorted([str(f.relative_to(T5_EMBEDDINGS_DIR)) for f in T5_EMBEDDINGS_DIR.glob('**/*.json')])\n",
    "#     batch_t5_dropdown.options = t5_files\n",
    "\n",
    "# # ==================== CLIP EMBEDDING SOURCE SELECTION ====================\n",
    "\n",
    "# clip_source_radio = widgets.RadioButtons(\n",
    "#     options=[\n",
    "#         ('Use CLIP embedding file (77 tokens)', '77_tokens'),\n",
    "#         ('Generate from short prompt', 'short'),\n",
    "#         ('Auto-generate from T5 prompt', 'auto')\n",
    "#     ],\n",
    "#     value='77_tokens',\n",
    "#     description='CLIP Source:',\n",
    "#     style={'description_width': 'initial'},\n",
    "#     layout=widgets.Layout(width='400px')\n",
    "# )\n",
    "\n",
    "# # Short prompt input for CLIP\n",
    "# batch_clip_prompt = widgets.Text(\n",
    "#     value='',\n",
    "#     placeholder='Enter short prompt for CLIP (e.g., \"a red cat\")',\n",
    "#     description='CLIP Prompt:',\n",
    "#     style={'description_width': 'initial'},\n",
    "#     layout=widgets.Layout(width='600px')\n",
    "# )\n",
    "\n",
    "# # CLIP file selection\n",
    "# batch_clip_dropdown = widgets.Dropdown(\n",
    "#     options=[],\n",
    "#     description='CLIP File:',\n",
    "#     style={'description_width': 'initial'},\n",
    "#     layout=widgets.Layout(width='600px')\n",
    "# )\n",
    "\n",
    "# # Populate CLIP dropdown\n",
    "# if CLIP_EMBEDDINGS_DIR.exists():\n",
    "#     clip_files = sorted([str(f.relative_to(CLIP_EMBEDDINGS_DIR)) for f in CLIP_EMBEDDINGS_DIR.glob('**/*.json')])\n",
    "#     batch_clip_dropdown.options = clip_files\n",
    "\n",
    "# # ==================== GENERATION CONTROLS ====================\n",
    "\n",
    "# batch_seed = widgets.IntText(value=42, description='Seed:', style={'description_width': 'initial'})\n",
    "# batch_steps = widgets.IntSlider(value=4, min=1, max=50, description='Steps:', style={'description_width': 'initial'}, layout=widgets.Layout(width='400px'))\n",
    "# batch_width = widgets.IntText(value=1024, description='Width:', style={'description_width': 'initial'})\n",
    "# batch_height = widgets.IntText(value=1024, description='Height:', style={'description_width': 'initial'})\n",
    "\n",
    "# batch_generate_button = widgets.Button(\n",
    "#     description='Generate Batch Image',\n",
    "#     button_style='primary',\n",
    "#     layout=widgets.Layout(width='300px', height='50px')\n",
    "# )\n",
    "\n",
    "# batch_output = widgets.Output()\n",
    "\n",
    "# # ==================== VISIBILITY HANDLERS ====================\n",
    "\n",
    "# def update_t5_visibility(change):\n",
    "#     if change['new'] == '512_tokens':\n",
    "#         batch_t5_dropdown.layout.display = 'flex'\n",
    "#         batch_short_prompt.layout.display = 'none'\n",
    "#     else:\n",
    "#         batch_t5_dropdown.layout.display = 'none'\n",
    "#         batch_short_prompt.layout.display = 'flex'\n",
    "\n",
    "# def update_clip_visibility(change):\n",
    "#     if change['new'] == '77_tokens':\n",
    "#         batch_clip_dropdown.layout.display = 'flex'\n",
    "#         batch_clip_prompt.layout.display = 'none'\n",
    "#     elif change['new'] == 'short':\n",
    "#         batch_clip_dropdown.layout.display = 'none'\n",
    "#         batch_clip_prompt.layout.display = 'flex'\n",
    "#     else:  # auto\n",
    "#         batch_clip_dropdown.layout.display = 'none'\n",
    "#         batch_clip_prompt.layout.display = 'none'\n",
    "\n",
    "# t5_source_radio.observe(update_t5_visibility, names='value')\n",
    "# clip_source_radio.observe(update_clip_visibility, names='value')\n",
    "\n",
    "# # Initialize visibility\n",
    "# batch_short_prompt.layout.display = 'none'\n",
    "# batch_clip_prompt.layout.display = 'none'\n",
    "\n",
    "# # ==================== BATCH GENERATION FUNCTION ====================\n",
    "\n",
    "# def determine_output_subfolder(t5_source, clip_source):\n",
    "#     \"\"\"\n",
    "#     Determine output subfolder based on embedding sources.\n",
    "#     Priority: short > 512_tokens > 77_tokens\n",
    "#     \"\"\"\n",
    "#     if t5_source == 'short' or clip_source == 'short':\n",
    "#         return OUTPUT_SHORT_DIR, 'short'\n",
    "#     elif t5_source == '512_tokens':\n",
    "#         return OUTPUT_512_DIR, '512_tokens'\n",
    "#     else:\n",
    "#         return OUTPUT_77_DIR, '77_tokens'\n",
    "\n",
    "# def sanitize_prompt_for_filename(prompt):\n",
    "#     \"\"\"Convert prompt to filename-safe string.\"\"\"\n",
    "#     # Replace spaces with underscores, keep only alphanumeric and underscores\n",
    "#     safe = ''.join(c if c.isalnum() else '_' for c in prompt.lower())\n",
    "#     # Collapse multiple underscores\n",
    "#     while '__' in safe:\n",
    "#         safe = safe.replace('__', '_')\n",
    "#     return safe.strip('_')[:50]  # Limit length\n",
    "\n",
    "# def generate_batch_image(b):\n",
    "#     \"\"\"Generate image with subfolder organization based on embedding sources.\"\"\"\n",
    "#     with batch_output:\n",
    "#         batch_output.clear_output()\n",
    "        \n",
    "#         if 'flux_pipe' not in globals():\n",
    "#             print(\"FLUX not loaded!\")\n",
    "#             return\n",
    "        \n",
    "#         t5_source = t5_source_radio.value\n",
    "#         clip_source = clip_source_radio.value\n",
    "        \n",
    "#         print(f\"T5 source: {t5_source}\")\n",
    "#         print(f\"CLIP source: {clip_source}\")\n",
    "        \n",
    "#         # ==================== PREPARE T5 EMBEDDINGS ====================\n",
    "        \n",
    "#         if t5_source == 'short':\n",
    "#             # Generate T5 embedding from short prompt\n",
    "#             prompt = batch_short_prompt.value.strip()\n",
    "#             if not prompt:\n",
    "#                 print(\"Please enter a short prompt for T5!\")\n",
    "#                 return\n",
    "            \n",
    "#             print(f\"\\nGenerating T5 embedding from short prompt: '{prompt}'\")\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 (\n",
    "#                     t5_tensor,\n",
    "#                     _,\n",
    "#                     _,\n",
    "#                 ) = flux_pipe.encode_prompt(\n",
    "#                     prompt=prompt,\n",
    "#                     prompt_2=None,\n",
    "#                     device=device,\n",
    "#                     num_images_per_prompt=1,\n",
    "#                     prompt_embeds=None,\n",
    "#                     pooled_prompt_embeds=None,\n",
    "#                     max_sequence_length=512,\n",
    "#                 )\n",
    "            \n",
    "#             t5_name = sanitize_prompt_for_filename(prompt)\n",
    "#             t5_prompt_text = prompt\n",
    "#             print(f\"  T5 tensor shape: {t5_tensor.shape}\")\n",
    "            \n",
    "#         else:\n",
    "#             # Load from file\n",
    "#             filename = batch_t5_dropdown.value\n",
    "#             if not filename:\n",
    "#                 print(\"Please select a T5 embedding file!\")\n",
    "#                 return\n",
    "            \n",
    "#             filepath = T5_EMBEDDINGS_DIR / filename\n",
    "#             print(f\"\\nLoading T5 embedding from: {filename}\")\n",
    "            \n",
    "#             with open(filepath, 'r') as f:\n",
    "#                 data = json.load(f)\n",
    "            \n",
    "#             t5_embedding = np.array(data['embedding'])\n",
    "#             t5_prompt_text = data.get('prompt', 'Unknown')\n",
    "            \n",
    "#             t5_tensor = torch.from_numpy(t5_embedding.astype(np.float32)).to(\n",
    "#                 device=device,\n",
    "#                 dtype=torch.bfloat16\n",
    "#             ).unsqueeze(0)\n",
    "            \n",
    "#             t5_tokens, t5_manip = parse_embedding_filename(filename)\n",
    "#             t5_name = t5_tokens + ('_' + t5_manip if t5_manip else '')\n",
    "#             print(f\"  T5 tensor shape: {t5_tensor.shape}\")\n",
    "#             print(f\"  T5 prompt: '{t5_prompt_text}'\")\n",
    "        \n",
    "#         # ==================== PREPARE CLIP EMBEDDINGS ====================\n",
    "        \n",
    "#         if clip_source == '77_tokens':\n",
    "#             # Load from file\n",
    "#             filename = batch_clip_dropdown.value\n",
    "#             if not filename:\n",
    "#                 print(\"Please select a CLIP embedding file!\")\n",
    "#                 return\n",
    "            \n",
    "#             filepath = CLIP_EMBEDDINGS_DIR / filename\n",
    "#             print(f\"\\nLoading CLIP embedding from: {filename}\")\n",
    "            \n",
    "#             with open(filepath, 'r') as f:\n",
    "#                 data = json.load(f)\n",
    "            \n",
    "#             clip_embedding = np.array(data['embedding'])\n",
    "            \n",
    "#             # Use last token (EOS) as pooled embedding\n",
    "#             pooled_embeds = torch.from_numpy(\n",
    "#                 clip_embedding[-1:].astype(np.float32)\n",
    "#             ).to(device=device, dtype=torch.bfloat16)\n",
    "            \n",
    "#             clip_tokens, clip_manip = parse_embedding_filename(filename)\n",
    "#             clip_name = clip_tokens + ('_' + clip_manip if clip_manip else '')\n",
    "#             print(f\"  Pooled embeddings shape: {pooled_embeds.shape}\")\n",
    "            \n",
    "#         elif clip_source == 'short':\n",
    "#             # Generate from short prompt\n",
    "#             prompt = batch_clip_prompt.value.strip()\n",
    "#             if not prompt:\n",
    "#                 print(\"Please enter a short prompt for CLIP!\")\n",
    "#                 return\n",
    "            \n",
    "#             print(f\"\\nGenerating CLIP pooled embedding from short prompt: '{prompt}'\")\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 (\n",
    "#                     _,\n",
    "#                     pooled_embeds,\n",
    "#                     _,\n",
    "#                 ) = flux_pipe.encode_prompt(\n",
    "#                     prompt=prompt,\n",
    "#                     prompt_2=None,\n",
    "#                     device=device,\n",
    "#                     num_images_per_prompt=1,\n",
    "#                     prompt_embeds=None,\n",
    "#                     pooled_prompt_embeds=None,\n",
    "#                     max_sequence_length=512,\n",
    "#                 )\n",
    "            \n",
    "#             clip_name = sanitize_prompt_for_filename(prompt)\n",
    "#             print(f\"  Pooled embeddings shape: {pooled_embeds.shape}\")\n",
    "            \n",
    "#         else:  # auto - generate from T5 prompt\n",
    "#             print(f\"\\nAuto-generating CLIP pooled embedding from T5 prompt: '{t5_prompt_text}'\")\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 (\n",
    "#                     _,\n",
    "#                     pooled_embeds,\n",
    "#                     _,\n",
    "#                 ) = flux_pipe.encode_prompt(\n",
    "#                     prompt=t5_prompt_text,\n",
    "#                     prompt_2=None,\n",
    "#                     device=device,\n",
    "#                     num_images_per_prompt=1,\n",
    "#                     prompt_embeds=None,\n",
    "#                     pooled_prompt_embeds=None,\n",
    "#                     max_sequence_length=512,\n",
    "#                 )\n",
    "            \n",
    "#             clip_name = 'auto'\n",
    "#             print(f\"  Pooled embeddings shape: {pooled_embeds.shape}\")\n",
    "        \n",
    "#         # ==================== DETERMINE OUTPUT PATH ====================\n",
    "        \n",
    "#         output_dir, subfolder_name = determine_output_subfolder(t5_source, clip_source)\n",
    "        \n",
    "#         # Build filename\n",
    "#         filename = f\"{t5_name}_{clip_name}.png\"\n",
    "#         output_path = output_dir / filename\n",
    "        \n",
    "#         print(f\"\\nOutput subfolder: {subfolder_name}/\")\n",
    "#         print(f\"Output filename: {filename}\")\n",
    "        \n",
    "#         # ==================== GENERATE IMAGE ====================\n",
    "        \n",
    "#         try:\n",
    "#             print(f\"\\n{'='*60}\")\n",
    "#             print(\"Running FLUX diffusion...\")\n",
    "#             print(f\"{'='*60}\\n\")\n",
    "            \n",
    "#             image = flux_pipe(\n",
    "#                 prompt_embeds=t5_tensor,\n",
    "#                 pooled_prompt_embeds=pooled_embeds,\n",
    "#                 num_inference_steps=batch_steps.value,\n",
    "#                 guidance_scale=0.0,\n",
    "#                 height=batch_height.value,\n",
    "#                 width=batch_width.value,\n",
    "#                 generator=torch.manual_seed(batch_seed.value)\n",
    "#             ).images[0]\n",
    "            \n",
    "#             image.save(output_path)\n",
    "#             print(f\"Image saved to: {output_path}\")\n",
    "            \n",
    "#             display(image)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error generating image: {e}\")\n",
    "#             import traceback\n",
    "#             traceback.print_exc()\n",
    "\n",
    "# batch_generate_button.on_click(generate_batch_image)\n",
    "\n",
    "# # ==================== DISPLAY BATCH INTERFACE ====================\n",
    "\n",
    "# display(widgets.VBox([\n",
    "#     widgets.HTML(\"<h2>Batch Generation with Subfolder Organization</h2>\"),\n",
    "#     widgets.HTML(\"<p>Images are saved to subfolders based on embedding source:</p>\"),\n",
    "#     widgets.HTML(\"<ul><li><b>short/</b> - From short text prompts</li><li><b>512_tokens/</b> - From T5 embedding files</li><li><b>77_tokens/</b> - From CLIP embedding files</li></ul>\"),\n",
    "    \n",
    "#     widgets.HTML(\"<h3>T5 Embedding Source</h3>\"),\n",
    "#     t5_source_radio,\n",
    "#     batch_t5_dropdown,\n",
    "#     batch_short_prompt,\n",
    "    \n",
    "#     widgets.HTML(\"<br><h3>CLIP Embedding Source</h3>\"),\n",
    "#     clip_source_radio,\n",
    "#     batch_clip_dropdown,\n",
    "#     batch_clip_prompt,\n",
    "    \n",
    "#     widgets.HTML(\"<br><h3>Generation Settings</h3>\"),\n",
    "#     batch_seed,\n",
    "#     batch_steps,\n",
    "#     widgets.HBox([batch_width, batch_height]),\n",
    "#     widgets.HTML(\"<br>\"),\n",
    "#     batch_generate_button,\n",
    "    \n",
    "#     widgets.HTML(\"<br><h3>Output</h3>\"),\n",
    "#     batch_output\n",
    "# ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T14:54:17.592999Z",
     "iopub.status.busy": "2026-01-14T14:54:17.592871Z",
     "iopub.status.idle": "2026-01-14T14:54:21.078285Z",
     "shell.execute_reply": "2026-01-14T14:54:21.077807Z",
     "shell.execute_reply.started": "2026-01-14T14:54:17.592987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Could not delete /home/lauwag/data/.cache: Cannot call rmtree on a symbolic link\n",
      "✓ Deleted /home/lauwag/.cache/huggingface: 51.00 GB\n",
      "✓ Deleted /home/lauwag/.cache/uv: 0.05 GB\n",
      "\n",
      "Total freed: 51.05 GB\n"
     ]
    }
   ],
   "source": [
    "# Clean up cache to free home directory space\n",
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def cleanup_cache():\n",
    "    \"\"\"Delete cache directories to free up home quota\"\"\"\n",
    "    cache_dirs = [\n",
    "        Path.home() / \"data\" / \".cache\",\n",
    "        Path.home() / \".cache\" / \"huggingface\",\n",
    "        Path.home() / \".cache\" / \"torch\",\n",
    "        Path.home() / \".cache\" / \"uv\",\n",
    "    ]\n",
    "    \n",
    "    total_freed = 0\n",
    "    for cache_dir in cache_dirs:\n",
    "        if cache_dir.exists():\n",
    "            try:\n",
    "                # Get size before deletion (approximate)\n",
    "                size = sum(f.stat().st_size for f in cache_dir.rglob('*') if f.is_file())\n",
    "                shutil.rmtree(cache_dir)\n",
    "                total_freed += size\n",
    "                print(f\"\\u2713 Deleted {cache_dir}: {size / 1024**3:.2f} GB\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\u2717 Could not delete {cache_dir}: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal freed: {total_freed / 1024**3:.2f} GB\")\n",
    "\n",
    "# Run cleanup\n",
    "cleanup_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<sub>Latent Vandalism Workshop • Laura Wagner, 2026 • [laurajul.github.io](https://laurajul.github.io/)</sub>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flux uv",
   "language": "python",
   "name": "uv_flux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
