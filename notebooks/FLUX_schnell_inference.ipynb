{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 Embedding Manipulation for FLUX\n",
    "\n",
    "This notebook lets you:\n",
    "1. Generate T5 embeddings from text prompts\n",
    "2. Save/load embeddings as JSON\n",
    "3. Manually edit embedding values\n",
    "4. Apply custom attention masks\n",
    "5. Generate images with FLUX using modified embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Run this cell first to install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T14:45:30.824714Z",
     "iopub.status.busy": "2026-01-14T14:45:30.824564Z",
     "iopub.status.idle": "2026-01-14T14:48:47.873008Z",
     "shell.execute_reply": "2026-01-14T14:48:47.872273Z",
     "shell.execute_reply.started": "2026-01-14T14:45:30.824701Z"
    }
   },
   "outputs": [],
   "source": "import torch\nimport json\nimport numpy as np\nfrom transformers import T5EncoderModel, T5Tokenizer\nfrom diffusers import FluxPipeline\nimport ipywidgets as widgets\nfrom IPython.display import display, Image as IPImage\nfrom PIL import Image\nimport os\nfrom pathlib import Path\n\n# Set device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# Load models path from config\ncurrent_dir = Path.cwd()\nmodels_path_file = current_dir.parent / \"misc/paths/models.txt\"\nwith open(models_path_file, 'r') as f:\n    models_path = f.read().strip()\nMODELS_DIR = current_dir.parent / models_path\n\nT5_MODEL_PATH = os.path.join(MODELS_DIR, \"t5-v1_1-xxl\")\nCLIP_MODEL_PATH = os.path.join(MODELS_DIR, \"clip\")\nFLUX_MODEL_PATH = os.path.join(MODELS_DIR, \"FLUX.1-schnell\")\n\nos.makedirs(MODELS_DIR, exist_ok=True)\nprint(f\"Models directory: {os.path.abspath(MODELS_DIR)}\")\nprint(f\"T5 path: {os.path.abspath(T5_MODEL_PATH)}\")\nprint(f\"FLUX path: {os.path.abspath(FLUX_MODEL_PATH)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T12:46:06.535933Z",
     "iopub.status.busy": "2026-01-13T12:46:06.535715Z",
     "iopub.status.idle": "2026-01-13T12:46:06.538143Z",
     "shell.execute_reply": "2026-01-13T12:46:06.537652Z",
     "shell.execute_reply.started": "2026-01-13T12:46:06.535918Z"
    }
   },
   "source": [
    "### Download Flux schnell from huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T12:45:13.570160Z",
     "iopub.status.busy": "2026-01-13T12:45:13.569947Z",
     "iopub.status.idle": "2026-01-13T12:45:13.572499Z",
     "shell.execute_reply": "2026-01-13T12:45:13.572011Z",
     "shell.execute_reply.started": "2026-01-13T12:45:13.570145Z"
    }
   },
   "source": [
    "FYI: if you don't have the model locally in ../data/models/FLUX.1-schnell, you have to download it manually from huggingface, because it is in a gated repository, put your huggingface access token in ../misc/credentials/hf.txt to download the model from the repository. THis process only works if you have previously requested access to the repository from huggingface and have generated the access token with the right permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T14:49:21.303452Z",
     "iopub.status.busy": "2026-01-14T14:49:21.303071Z",
     "iopub.status.idle": "2026-01-14T14:49:21.543570Z",
     "shell.execute_reply": "2026-01-14T14:49:21.543023Z",
     "shell.execute_reply.started": "2026-01-14T14:49:21.303432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for HF token at: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/misc/credentials/hf.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Load Hugging Face token from file\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the token file path\n",
    "current_dir = Path.cwd()\n",
    "token_file = current_dir.parent / \"misc/credentials/hf.txt\"\n",
    "\n",
    "print(f\"Looking for HF token at: {token_file}\")\n",
    "\n",
    "if token_file.exists():\n",
    "    with open(token_file, 'r') as f:\n",
    "        hf_token = f.read().strip()\n",
    "    \n",
    "    # Set the token as an environment variable\n",
    "    os.environ['HF_TOKEN'] = hf_token\n",
    "    \n",
    "    # Also login using huggingface_hub\n",
    "    from huggingface_hub import login\n",
    "    login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T14:49:22.515779Z",
     "iopub.status.busy": "2026-01-14T14:49:22.515571Z",
     "iopub.status.idle": "2026-01-14T14:52:31.874440Z",
     "shell.execute_reply": "2026-01-14T14:52:31.869267Z",
     "shell.execute_reply.started": "2026-01-14T14:49:22.515766Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e932d59b5d284ae796f3ed756a005a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973efe74e2fa45bbb3607d0ec038336e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3131e2db9a46402786b0e471389b55ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load FLUX\n",
    "try:\n",
    "    if not os.path.exists(FLUX_MODEL_PATH):\n",
    "        flux_pipe = FluxPipeline.from_pretrained(\n",
    "            \"black-forest-labs/FLUX.1-schnell\",\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        flux_pipe.save_pretrained(FLUX_MODEL_PATH)\n",
    "    else:\n",
    "        flux_pipe = FluxPipeline.from_pretrained(\n",
    "            FLUX_MODEL_PATH,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            local_files_only=True\n",
    "        )\n",
    "    \n",
    "    flux_pipe = flux_pipe.to(device)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading FLUX: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Manual Attention Masking\n",
    "\n",
    "Control which tokens get how much attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T14:53:42.027971Z",
     "iopub.status.busy": "2026-01-14T14:53:42.027779Z",
     "iopub.status.idle": "2026-01-14T14:53:42.082257Z",
     "shell.execute_reply": "2026-01-14T14:53:42.081686Z",
     "shell.execute_reply.started": "2026-01-14T14:53:42.027956Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'current_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     56\u001b[39m     update_total()\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sliders, normalize_button, total_label, real_tokens\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m attention_controls = \u001b[43mcreate_attention_sliders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_controls:\n\u001b[32m     63\u001b[39m     sliders, normalize_btn, total_lbl, real_tokens = attention_controls\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mcreate_attention_sliders\u001b[39m\u001b[34m(num_tokens)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_attention_sliders\u001b[39m(num_tokens=\u001b[32m10\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcurrent_tokens\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      4\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m❌ Generate an embedding first!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'current_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "# Display tokens and create sliders\n",
    "def create_attention_sliders(num_tokens=10):\n",
    "    if current_tokens is None:\n",
    "        print(\"❌ Generate an embedding first!\")\n",
    "        return None\n",
    "    \n",
    "    # Find non-padding tokens\n",
    "    real_tokens = []\n",
    "    for i, token in enumerate(current_tokens):\n",
    "        if token.strip() and token != '<pad>' and i < num_tokens:\n",
    "            real_tokens.append((i, token))\n",
    "    \n",
    "    print(f\"Creating sliders for first {len(real_tokens)} tokens:\")\n",
    "    print(real_tokens)\n",
    "    print()\n",
    "    \n",
    "    sliders = []\n",
    "    for idx, token in real_tokens:\n",
    "        slider = widgets.FloatSlider(\n",
    "            value=1.0 / len(real_tokens),  # Equal distribution\n",
    "            min=0.0,\n",
    "            max=1.0,\n",
    "            step=0.01,\n",
    "            description=f'{idx}: {token[:15]}',\n",
    "            readout_format='.0%',\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "        sliders.append(slider)\n",
    "    \n",
    "    # Normalize button\n",
    "    normalize_button = widgets.Button(\n",
    "        description='Normalize to 100%',\n",
    "        button_style='info'\n",
    "    )\n",
    "    \n",
    "    total_label = widgets.Label(value='Total: 100%')\n",
    "    \n",
    "    def update_total(*args):\n",
    "        total = sum(s.value for s in sliders)\n",
    "        total_label.value = f'Total: {total*100:.1f}%'\n",
    "        if abs(total - 1.0) > 0.01:\n",
    "            total_label.value += ' ⚠️ Should sum to 100%'\n",
    "    \n",
    "    def normalize(*args):\n",
    "        total = sum(s.value for s in sliders)\n",
    "        if total > 0:\n",
    "            for s in sliders:\n",
    "                s.value = s.value / total\n",
    "        update_total()\n",
    "    \n",
    "    for slider in sliders:\n",
    "        slider.observe(update_total, 'value')\n",
    "    \n",
    "    normalize_button.on_click(normalize)\n",
    "    \n",
    "    update_total()\n",
    "    \n",
    "    return sliders, normalize_button, total_label, real_tokens\n",
    "\n",
    "attention_controls = create_attention_sliders(num_tokens=10)\n",
    "\n",
    "if attention_controls:\n",
    "    sliders, normalize_btn, total_lbl, real_tokens = attention_controls\n",
    "    display(widgets.VBox(sliders + [normalize_btn, total_lbl]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Attention Mask Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-14T14:52:32.046816Z",
     "iopub.status.idle": "2026-01-14T14:52:32.047130Z",
     "shell.execute_reply": "2026-01-14T14:52:32.046919Z",
     "shell.execute_reply.started": "2026-01-14T14:52:32.046911Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_attention_mask():\n",
    "    if attention_controls is None:\n",
    "        print(\"❌ Create attention sliders first!\")\n",
    "        return None\n",
    "    \n",
    "    sliders, _, _, real_tokens = attention_controls\n",
    "    \n",
    "    # Create mask array (512 tokens)\n",
    "    mask = np.zeros(512)\n",
    "    \n",
    "    # Set weights from sliders\n",
    "    for slider, (idx, token) in zip(sliders, real_tokens):\n",
    "        mask[idx] = slider.value\n",
    "    \n",
    "    print(\"Attention Mask created:\")\n",
    "    print(f\"Non-zero weights: {np.count_nonzero(mask)}\")\n",
    "    for slider, (idx, token) in zip(sliders, real_tokens):\n",
    "        print(f\"  Token {idx} '{token}': {slider.value*100:.1f}%\")\n",
    "    \n",
    "    return mask\n",
    "\n",
    "mask_button = widgets.Button(description='Create Mask', button_style='success')\n",
    "mask_output = widgets.Output()\n",
    "\n",
    "attention_mask = None\n",
    "\n",
    "def on_mask_click(b):\n",
    "    global attention_mask\n",
    "    with mask_output:\n",
    "        mask_output.clear_output()\n",
    "        attention_mask = create_attention_mask()\n",
    "\n",
    "mask_button.on_click(on_mask_click)\n",
    "display(mask_button, mask_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Image from embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T14:54:13.604578Z",
     "iopub.status.busy": "2026-01-14T14:54:13.604391Z",
     "iopub.status.idle": "2026-01-14T14:54:13.607475Z",
     "shell.execute_reply": "2026-01-14T14:54:13.607065Z",
     "shell.execute_reply.started": "2026-01-14T14:54:13.604563Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup directories\n",
    "T5_EMBEDDINGS_DIR = current_dir.parent / \"data/embeddings/T5\"\n",
    "CLIP_EMBEDDINGS_DIR = current_dir.parent / \"data/embeddings/CLIP\"\n",
    "\n",
    "# Global variables for loaded embeddings\n",
    "loaded_t5_embedding = None\n",
    "loaded_clip_embedding = None\n",
    "loaded_t5_prompt = None\n",
    "loaded_clip_prompt = None\n",
    "\n",
    "# Setup output directory\n",
    "OUTPUT_IMAGES_DIR = current_dir.parent / \"output/images\"\n",
    "os.makedirs(OUTPUT_IMAGES_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T14:54:15.660207Z",
     "iopub.status.busy": "2026-01-14T14:54:15.660024Z",
     "iopub.status.idle": "2026-01-14T14:54:15.698682Z",
     "shell.execute_reply": "2026-01-14T14:54:15.698211Z",
     "shell.execute_reply.started": "2026-01-14T14:54:15.660194Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85dcd115c99b4edaa64dba8aeba0fbf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>Embedding Selection & Image Generation</h2>'), HTML(value='<h3>1. T5 Embedding …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==================== HELPER FUNCTIONS ====================\n",
    "\n",
    "def parse_embedding_filename(filename):\n",
    "    \"\"\"\n",
    "    Parse embedding filename to extract tokens and manipulation.\n",
    "    Returns (tokens_string, manipulation_string)\n",
    "    \n",
    "    Examples:\n",
    "    - 'a_red_cat_sitting.json' -> ('aredcatsitting', '')\n",
    "    - 'a_red_cat_sitting_scaled_2.0x.json' -> ('aredcatsitting', 'scaled')\n",
    "    - 'an_elephant_inverted.json' -> ('anelephant', 'inverted')\n",
    "    - 'an_elephant_zeroed_30pct.json' -> ('anelephant', 'zeroed')\n",
    "    \"\"\"\n",
    "    if not filename:\n",
    "        return ('unknown', '')\n",
    "    \n",
    "    # Remove .json extension\n",
    "    base_name = filename.rsplit('.', 1)[0]\n",
    "    \n",
    "    # Split by underscore\n",
    "    parts = base_name.split('_')\n",
    "    \n",
    "    # First 4 parts are the tokens - join without underscores\n",
    "    if len(parts) <= 4:\n",
    "        tokens = ''.join(parts)\n",
    "        return (tokens, '')\n",
    "    \n",
    "    tokens = ''.join(parts[:4])\n",
    "    \n",
    "    # Get manipulation type (simplified)\n",
    "    # Just take the first part after the tokens\n",
    "    manipulation_parts = parts[4:]\n",
    "    if manipulation_parts:\n",
    "        # Get the manipulation type (first word after tokens)\n",
    "        manipulation = manipulation_parts[0]\n",
    "    else:\n",
    "        manipulation = ''\n",
    "    \n",
    "    return (tokens, manipulation)\n",
    "\n",
    "# ==================== T5 EMBEDDING WIDGETS ====================\n",
    "\n",
    "# T5 embedding selection\n",
    "t5_file_dropdown = widgets.Dropdown(\n",
    "    options=[],\n",
    "    description='T5 Embedding:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='600px')\n",
    ")\n",
    "\n",
    "load_t5_button = widgets.Button(\n",
    "    description='Load T5',\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "t5_load_output = widgets.Output()\n",
    "\n",
    "# Populate T5 dropdown options\n",
    "if T5_EMBEDDINGS_DIR.exists():\n",
    "    t5_files = sorted([f.name for f in T5_EMBEDDINGS_DIR.glob('*.json')])\n",
    "    t5_file_dropdown.options = t5_files\n",
    "\n",
    "def load_t5_embedding_file(b):\n",
    "    global loaded_t5_embedding, loaded_t5_prompt\n",
    "    \n",
    "    with t5_load_output:\n",
    "        t5_load_output.clear_output()\n",
    "        \n",
    "        filename = t5_file_dropdown.value\n",
    "        if not filename:\n",
    "            print(\"❌ No file selected!\")\n",
    "            return\n",
    "        \n",
    "        filepath = T5_EMBEDDINGS_DIR / filename\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            loaded_t5_embedding = np.array(data['embedding'])\n",
    "            loaded_t5_prompt = data.get('prompt', 'Unknown')\n",
    "            \n",
    "            print(f\"✓ Loaded T5 embedding!\")\n",
    "            print(f\"  File: {filename}\")\n",
    "            print(f\"  Prompt: '{loaded_t5_prompt}'\")\n",
    "            print(f\"  Shape: {loaded_t5_embedding.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading T5 embedding: {e}\")\n",
    "\n",
    "load_t5_button.on_click(load_t5_embedding_file)\n",
    "\n",
    "# ==================== CLIP EMBEDDING WIDGETS ====================\n",
    "\n",
    "# CLIP embedding selection\n",
    "clip_file_dropdown = widgets.Dropdown(\n",
    "    options=[],\n",
    "    description='CLIP Embedding:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='600px')\n",
    ")\n",
    "\n",
    "load_clip_button = widgets.Button(\n",
    "    description='Load CLIP',\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "clip_load_output = widgets.Output()\n",
    "\n",
    "# Populate CLIP dropdown options\n",
    "if CLIP_EMBEDDINGS_DIR.exists():\n",
    "    clip_files = sorted([f.name for f in CLIP_EMBEDDINGS_DIR.glob('*.json')])\n",
    "    clip_file_dropdown.options = clip_files\n",
    "\n",
    "def load_clip_embedding_file(b):\n",
    "    global loaded_clip_embedding, loaded_clip_prompt\n",
    "    \n",
    "    with clip_load_output:\n",
    "        clip_load_output.clear_output()\n",
    "        \n",
    "        filename = clip_file_dropdown.value\n",
    "        if not filename:\n",
    "            print(\"❌ No file selected!\")\n",
    "            return\n",
    "        \n",
    "        filepath = CLIP_EMBEDDINGS_DIR / filename\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            loaded_clip_embedding = np.array(data['embedding'])\n",
    "            loaded_clip_prompt = data.get('prompt', 'Unknown')\n",
    "            \n",
    "            print(f\"✓ Loaded CLIP embedding!\")\n",
    "            print(f\"  File: {filename}\")\n",
    "            print(f\"  Prompt: '{loaded_clip_prompt}'\")\n",
    "            print(f\"  Shape: {loaded_clip_embedding.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading CLIP embedding: {e}\")\n",
    "\n",
    "load_clip_button.on_click(load_clip_embedding_file)\n",
    "\n",
    "# ==================== GENERATION CONTROLS ====================\n",
    "\n",
    "seed_input = widgets.IntText(\n",
    "    value=42,\n",
    "    description='Seed:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "steps_input = widgets.IntSlider(\n",
    "    value=4,\n",
    "    min=1,\n",
    "    max=50,\n",
    "    step=1,\n",
    "    description='Inference Steps:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "width_input = widgets.IntText(\n",
    "    value=1024,\n",
    "    description='Width:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "height_input = widgets.IntText(\n",
    "    value=1024,\n",
    "    description='Height:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "generate_button = widgets.Button(\n",
    "    description='Generate Image',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='300px', height='50px')\n",
    ")\n",
    "\n",
    "generation_output = widgets.Output()\n",
    "\n",
    "def generate_from_loaded_embeddings(b):\n",
    "    \"\"\"\n",
    "    Generate image using loaded T5 and CLIP embeddings.\n",
    "    \"\"\"\n",
    "    with generation_output:\n",
    "        generation_output.clear_output()\n",
    "        \n",
    "        if 'flux_pipe' not in globals():\n",
    "            print(\"❌ FLUX not loaded!\")\n",
    "            return\n",
    "        \n",
    "        if loaded_t5_embedding is None:\n",
    "            print(\"❌ No T5 embedding loaded! Load a T5 embedding first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Generating image from loaded embeddings...\")\n",
    "        print(f\"  Seed: {seed_input.value}\")\n",
    "        print(f\"  Inference Steps: {steps_input.value}\")\n",
    "        print(f\"  Dimensions: {width_input.value}x{height_input.value}\")\n",
    "        print(f\"  T5 embedding shape: {loaded_t5_embedding.shape}\")\n",
    "        print(f\"  T5 prompt: '{loaded_t5_prompt}'\")\n",
    "        \n",
    "        # Convert T5 numpy to torch tensor with bfloat16\n",
    "        t5_tensor = torch.from_numpy(loaded_t5_embedding.astype(np.float32)).to(\n",
    "            device=device,\n",
    "            dtype=torch.bfloat16\n",
    "        )\n",
    "        \n",
    "        # Add batch dimension: [512, 4096] -> [1, 512, 4096]\n",
    "        t5_tensor = t5_tensor.unsqueeze(0)\n",
    "        \n",
    "        # Process CLIP embeddings\n",
    "        print(\"\\nProcessing CLIP pooled embeddings...\")\n",
    "        \n",
    "        if loaded_clip_embedding is not None:\n",
    "            # Use loaded CLIP embedding\n",
    "            print(f\"  Using loaded CLIP embedding\")\n",
    "            print(f\"  CLIP embedding shape: {loaded_clip_embedding.shape}\")\n",
    "            print(f\"  CLIP prompt: '{loaded_clip_prompt}'\")\n",
    "            \n",
    "            # Use last token embedding as pooled embedding (EOS token)\n",
    "            pooled_embeds = torch.from_numpy(\n",
    "                loaded_clip_embedding[-1:].astype(np.float32)\n",
    "            ).to(device=device, dtype=torch.bfloat16)\n",
    "            \n",
    "        else:\n",
    "            # Fall back to generating from T5 prompt using CLIP\n",
    "            print(f\"  No CLIP embedding loaded, generating from T5 prompt using CLIP model...\")\n",
    "            \n",
    "            if loaded_t5_prompt:\n",
    "                with torch.no_grad():\n",
    "                    (\n",
    "                        _,\n",
    "                        pooled_embeds,\n",
    "                        _,\n",
    "                    ) = flux_pipe.encode_prompt(\n",
    "                        prompt=loaded_t5_prompt,\n",
    "                        prompt_2=None,\n",
    "                        device=device,\n",
    "                        num_images_per_prompt=1,\n",
    "                        prompt_embeds=None,\n",
    "                        pooled_prompt_embeds=None,\n",
    "                        max_sequence_length=512,\n",
    "                    )\n",
    "            else:\n",
    "                print(\"❌ No CLIP embedding and no prompt available!\")\n",
    "                return\n",
    "        \n",
    "        print(f\"  Pooled embeddings shape: {pooled_embeds.shape}\")\n",
    "        \n",
    "        # Construct filename from embeddings\n",
    "        t5_tokens, t5_manip = parse_embedding_filename(t5_file_dropdown.value)\n",
    "        clip_tokens, clip_manip = parse_embedding_filename(clip_file_dropdown.value)\n",
    "        \n",
    "        # Build filename: t5tokens_t5manip_cliptokens_clipmanip.png\n",
    "        filename_parts = [t5_tokens]\n",
    "        if t5_manip:\n",
    "            filename_parts.append(t5_manip)\n",
    "        filename_parts.append(clip_tokens)\n",
    "        if clip_manip:\n",
    "            filename_parts.append(clip_manip)\n",
    "        \n",
    "        filename = '_'.join(filename_parts) + '.png'\n",
    "        output_filepath = OUTPUT_IMAGES_DIR / filename\n",
    "        \n",
    "        # Generate image\n",
    "        try:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"Running FLUX diffusion...\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "            \n",
    "            image = flux_pipe(\n",
    "                prompt_embeds=t5_tensor,\n",
    "                pooled_prompt_embeds=pooled_embeds,\n",
    "                num_inference_steps=steps_input.value,\n",
    "                guidance_scale=0.0,\n",
    "                height=height_input.value,\n",
    "                width=width_input.value,\n",
    "                generator=torch.manual_seed(seed_input.value)\n",
    "            ).images[0]\n",
    "            \n",
    "            image.save(output_filepath)\n",
    "            print(f\"✓ Image generated and saved!\")\n",
    "            print(f\"  Path: {output_filepath}\")\n",
    "            print(f\"  Filename: {filename}\\n\")\n",
    "            \n",
    "            display(image)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error generating image: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "generate_button.on_click(generate_from_loaded_embeddings)\n",
    "\n",
    "# ==================== DISPLAY UNIFIED INTERFACE ====================\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h2>Embedding Selection & Image Generation</h2>\"),\n",
    "    \n",
    "    # T5 Embedding Section\n",
    "    widgets.HTML(\"<h3>1. T5 Embedding (Required)</h3>\"),\n",
    "    t5_file_dropdown,\n",
    "    load_t5_button,\n",
    "    t5_load_output,\n",
    "    \n",
    "    # CLIP Embedding Section\n",
    "    widgets.HTML(\"<br><h3>2. CLIP Embedding (Optional - auto-generated if not loaded)</h3>\"),\n",
    "    clip_file_dropdown,\n",
    "    load_clip_button,\n",
    "    clip_load_output,\n",
    "    \n",
    "    # Generation Controls Section\n",
    "    widgets.HTML(\"<br><h3>3. Generation Controls</h3>\"),\n",
    "    seed_input,\n",
    "    steps_input,\n",
    "    widgets.HTML(\"<br><b>Image Dimensions:</b>\"),\n",
    "    widgets.HBox([width_input, height_input]),\n",
    "    widgets.HTML(\"<br>\"),\n",
    "    generate_button,\n",
    "    \n",
    "    # Output Section\n",
    "    widgets.HTML(\"<br><h3>Generated Image</h3>\"),\n",
    "    generation_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "What you've learned:\n",
    "\n",
    "1. ✓ Generate T5-XXL embeddings from text (512 tokens × 4096 dimensions = 2,097,152 numbers)\n",
    "2. ✓ Save/load embeddings as JSON files\n",
    "3. ✓ Manually manipulate embedding values\n",
    "4. ✓ Create custom attention masks with sliders\n",
    "5. ✓ Generate images directly from custom embeddings using FLUX\n",
    "6. ✓ Compare original vs modified embeddings side-by-side\n",
    "7. ✓ Understand the complete pipeline: text → T5-XXL → embeddings → FLUX → image\n",
    "\n",
    "## Workflow:\n",
    "\n",
    "1. **Generate embedding** from your text prompt\n",
    "2. **Save to JSON** for backup\n",
    "3. **Apply manipulations** (zero out values, etc.)\n",
    "4. **Create attention masks** (optional - for future use)\n",
    "5. **Generate images** from both original and modified embeddings\n",
    "6. **Compare results** to see what your changes did!\n",
    "\n",
    "## Next Steps:\n",
    "\n",
    "- Experiment with different zero percentages (10%, 30%, 50%, 90%)\n",
    "- Try zeroing specific token embeddings instead of random values\n",
    "- Compare results with different prompts\n",
    "- Implement attention mask injection (requires modifying cross-attention layers)\n",
    "- Explore interpolating between two different embeddings\n",
    "- Try adding noise to embeddings and see the effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T14:54:17.592999Z",
     "iopub.status.busy": "2026-01-14T14:54:17.592871Z",
     "iopub.status.idle": "2026-01-14T14:54:21.078285Z",
     "shell.execute_reply": "2026-01-14T14:54:21.077807Z",
     "shell.execute_reply.started": "2026-01-14T14:54:17.592987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Could not delete /home/lauwag/data/.cache: Cannot call rmtree on a symbolic link\n",
      "✓ Deleted /home/lauwag/.cache/huggingface: 51.00 GB\n",
      "✓ Deleted /home/lauwag/.cache/uv: 0.05 GB\n",
      "\n",
      "Total freed: 51.05 GB\n"
     ]
    }
   ],
   "source": [
    "# Clean up cache to free home directory space\n",
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def cleanup_cache():\n",
    "    \"\"\"Delete cache directories to free up home quota\"\"\"\n",
    "    cache_dirs = [\n",
    "        Path.home() / \"data\" / \".cache\",\n",
    "        Path.home() / \".cache\" / \"huggingface\",\n",
    "        Path.home() / \".cache\" / \"torch\",\n",
    "        Path.home() / \".cache\" / \"uv\",\n",
    "    ]\n",
    "    \n",
    "    total_freed = 0\n",
    "    for cache_dir in cache_dirs:\n",
    "        if cache_dir.exists():\n",
    "            try:\n",
    "                # Get size before deletion (approximate)\n",
    "                size = sum(f.stat().st_size for f in cache_dir.rglob('*') if f.is_file())\n",
    "                shutil.rmtree(cache_dir)\n",
    "                total_freed += size\n",
    "                print(f\"\\u2713 Deleted {cache_dir}: {size / 1024**3:.2f} GB\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\u2717 Could not delete {cache_dir}: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal freed: {total_freed / 1024**3:.2f} GB\")\n",
    "\n",
    "# Run cleanup\n",
    "cleanup_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flux uv",
   "language": "python",
   "name": "uv_flux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}