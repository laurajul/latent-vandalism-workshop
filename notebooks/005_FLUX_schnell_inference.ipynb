{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLUX-Schnell Inference\n",
    "\n",
    "This notebook lets you:\n",
    "\n",
    "2. Save/load embeddings\n",
    "5. Generate images with FLUX using modified embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Embeddings\n",
    "        T5[T5 Embedding<br/>JSON<br/>512 × 4096]\n",
    "        CLIP[CLIP Embedding<br/>JSON<br/>77 × 768]\n",
    "    end\n",
    "\n",
    "    subgraph FLUX Pipeline\n",
    "        T5E[T5 Prompt<br/>Embeds]\n",
    "        PE[Pooled<br/>Embeds]\n",
    "        DT[Diffusion<br/>Transformer]\n",
    "        VAE[VAE<br/>Decoder]\n",
    "    end\n",
    "\n",
    "    T5 --> T5E\n",
    "    CLIP -->|EOS token| PE\n",
    "    \n",
    "    T5E -->|cross-attention| DT\n",
    "    PE -->|global conditioning| DT\n",
    "    \n",
    "    DT --> VAE --> IMG[Generated Image]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Run this cell first to install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:29:20.242838Z",
     "iopub.status.busy": "2026-01-16T00:29:20.242685Z",
     "iopub.status.idle": "2026-01-16T00:29:45.884659Z",
     "shell.execute_reply": "2026-01-16T00:29:45.884190Z",
     "shell.execute_reply.started": "2026-01-16T00:29:20.242824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Models directory: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/data/models\n",
      "T5 path: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/data/models/t5-v1_1-xxl\n",
      "FLUX path: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/data/models/FLUX.1-schnell\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "from diffusers import FluxPipeline\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Image as IPImage\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load models path from config\n",
    "current_dir = Path.cwd()\n",
    "models_path_file = current_dir.parent / \"misc/paths/models.txt\"\n",
    "with open(models_path_file, 'r') as f:\n",
    "    models_path = f.read().strip()\n",
    "MODELS_DIR = current_dir.parent / models_path\n",
    "\n",
    "T5_MODEL_PATH = os.path.join(MODELS_DIR, \"t5-v1_1-xxl\")\n",
    "CLIP_MODEL_PATH = os.path.join(MODELS_DIR, \"clip\")\n",
    "FLUX_MODEL_PATH = os.path.join(MODELS_DIR, \"FLUX.1-schnell\")\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "print(f\"Models directory: {os.path.abspath(MODELS_DIR)}\")\n",
    "print(f\"T5 path: {os.path.abspath(T5_MODEL_PATH)}\")\n",
    "print(f\"FLUX path: {os.path.abspath(FLUX_MODEL_PATH)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T12:46:06.535933Z",
     "iopub.status.busy": "2026-01-13T12:46:06.535715Z",
     "iopub.status.idle": "2026-01-13T12:46:06.538143Z",
     "shell.execute_reply": "2026-01-13T12:46:06.537652Z",
     "shell.execute_reply.started": "2026-01-13T12:46:06.535918Z"
    }
   },
   "source": [
    "### Download Flux schnell from huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T12:45:13.570160Z",
     "iopub.status.busy": "2026-01-13T12:45:13.569947Z",
     "iopub.status.idle": "2026-01-13T12:45:13.572499Z",
     "shell.execute_reply": "2026-01-13T12:45:13.572011Z",
     "shell.execute_reply.started": "2026-01-13T12:45:13.570145Z"
    }
   },
   "source": [
    "FYI: if you don't have the model locally in ../data/models/FLUX.1-schnell, you have to download it manually from huggingface, because it is in a gated repository, put your huggingface access token in ../misc/credentials/hf.txt to download the model from the repository. THis process only works if you have previously requested access to the repository from huggingface and have generated the access token with the right permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:29:45.885474Z",
     "iopub.status.busy": "2026-01-16T00:29:45.885162Z",
     "iopub.status.idle": "2026-01-16T00:29:46.208104Z",
     "shell.execute_reply": "2026-01-16T00:29:46.207447Z",
     "shell.execute_reply.started": "2026-01-16T00:29:45.885458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for HF token at: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/misc/credentials/hf.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Load Hugging Face token from file\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the token file path\n",
    "current_dir = Path.cwd()\n",
    "token_file = current_dir.parent / \"misc/credentials/hf.txt\"\n",
    "\n",
    "print(f\"Looking for HF token at: {token_file}\")\n",
    "\n",
    "if token_file.exists():\n",
    "    with open(token_file, 'r') as f:\n",
    "        hf_token = f.read().strip()\n",
    "    \n",
    "    # Set the token as an environment variable\n",
    "    os.environ['HF_TOKEN'] = hf_token\n",
    "    \n",
    "    # Also login using huggingface_hub\n",
    "    from huggingface_hub import login\n",
    "    login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:29:46.208639Z",
     "iopub.status.busy": "2026-01-16T00:29:46.208501Z",
     "iopub.status.idle": "2026-01-16T00:32:45.914696Z",
     "shell.execute_reply": "2026-01-16T00:32:45.913907Z",
     "shell.execute_reply.started": "2026-01-16T00:29:46.208625Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca873d35b30245958e26f41d92523177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4df829b155d436eaed5fe93e4efffd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb9072ac1634ba0b14e331fb35700c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load FLUX\n",
    "try:\n",
    "    if not os.path.exists(FLUX_MODEL_PATH):\n",
    "        flux_pipe = FluxPipeline.from_pretrained(\n",
    "            \"black-forest-labs/FLUX.1-schnell\",\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        flux_pipe.save_pretrained(FLUX_MODEL_PATH)\n",
    "    else:\n",
    "        flux_pipe = FluxPipeline.from_pretrained(\n",
    "            FLUX_MODEL_PATH,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            local_files_only=True\n",
    "        )\n",
    "    \n",
    "    flux_pipe = flux_pipe.to(device)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading FLUX: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Image from embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T02:47:41.671451Z",
     "iopub.status.busy": "2026-01-16T02:47:41.671260Z",
     "iopub.status.idle": "2026-01-16T02:47:41.674245Z",
     "shell.execute_reply": "2026-01-16T02:47:41.673768Z",
     "shell.execute_reply.started": "2026-01-16T02:47:41.671437Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup directories\n",
    "T5_EMBEDDINGS_DIR = current_dir.parent / \"data/embeddings/T5/\"\n",
    "CLIP_EMBEDDINGS_DIR = current_dir.parent / \"data/embeddings/CLIP/\"\n",
    "\n",
    "\n",
    "# Global variables for loaded embeddings\n",
    "loaded_t5_embedding = None\n",
    "loaded_clip_embedding = None\n",
    "loaded_t5_prompt = None\n",
    "loaded_clip_prompt = None\n",
    "\n",
    "# Setup output directory\n",
    "OUTPUT_IMAGES_DIR = current_dir.parent / \"output/images\"\n",
    "os.makedirs(OUTPUT_IMAGES_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T03:01:05.477631Z",
     "iopub.status.busy": "2026-01-16T03:01:05.477413Z",
     "iopub.status.idle": "2026-01-16T03:01:05.742740Z",
     "shell.execute_reply": "2026-01-16T03:01:05.742271Z",
     "shell.execute_reply.started": "2026-01-16T03:01:05.477612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88dfbc4562a4b52a90a0e9526dd6015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>Embedding Selection & Image Generation</h2>'), HTML(value='<h3>1. T5 Embedding …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==================== HELPER FUNCTIONS ====================\n",
    "\n",
    "def parse_embedding_filename(filename):\n",
    "    \"\"\"\n",
    "    Parse embedding filename to extract tokens and manipulation.\n",
    "    Returns (tokens_string, manipulation_string)\n",
    "    \n",
    "    Examples:\n",
    "    - 'a_red_cat_sitting.json' -> ('aredcatsitting', '')\n",
    "    - 'a_red_cat_sitting_scaled_2.0x.json' -> ('aredcatsitting', 'scaled')\n",
    "    - 'an_elephant_inverted.json' -> ('anelephant', 'inverted')\n",
    "    - 'an_elephant_zeroed_30pct.json' -> ('anelephant', 'zeroed')\n",
    "    - 'examples/a_red_cat.json' -> ('aredcat', '')  # handles subdirectories\n",
    "    \"\"\"\n",
    "    if not filename:\n",
    "        return ('unknown', '')\n",
    "    \n",
    "    # Extract just the filename (strip directory path)\n",
    "    basename = os.path.basename(filename)\n",
    "    \n",
    "    # Remove .json extension\n",
    "    base_name = basename.rsplit('.', 1)[0]\n",
    "    \n",
    "    # Split by underscore\n",
    "    parts = base_name.split('_')\n",
    "    \n",
    "    # First 4 parts are the tokens - join without underscores\n",
    "    if len(parts) <= 4:\n",
    "        tokens = ''.join(parts)\n",
    "        return (tokens, '')\n",
    "    \n",
    "    tokens = ''.join(parts[:4])\n",
    "    \n",
    "    # Get manipulation type (simplified)\n",
    "    # Just take the first part after the tokens\n",
    "    manipulation_parts = parts[4:]\n",
    "    if manipulation_parts:\n",
    "        # Get the manipulation type (first word after tokens)\n",
    "        manipulation = manipulation_parts[0]\n",
    "    else:\n",
    "        manipulation = ''\n",
    "    \n",
    "    return (tokens, manipulation)\n",
    "\n",
    "# ==================== T5 EMBEDDING WIDGETS ====================\n",
    "\n",
    "# T5 embedding selection\n",
    "t5_file_dropdown = widgets.Dropdown(\n",
    "    options=[],\n",
    "    description='T5 Embedding:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='600px')\n",
    ")\n",
    "\n",
    "load_t5_button = widgets.Button(\n",
    "    description='Load T5',\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "t5_load_output = widgets.Output()\n",
    "\n",
    "# Populate T5 dropdown options (including subfolders)\n",
    "if T5_EMBEDDINGS_DIR.exists():\n",
    "    t5_files = sorted([str(f.relative_to(T5_EMBEDDINGS_DIR)) for f in T5_EMBEDDINGS_DIR.glob('**/*.json')])\n",
    "    t5_file_dropdown.options = t5_files\n",
    "\n",
    "def load_t5_embedding_file(b):\n",
    "    global loaded_t5_embedding, loaded_t5_prompt\n",
    "    \n",
    "    with t5_load_output:\n",
    "        t5_load_output.clear_output()\n",
    "        \n",
    "        filename = t5_file_dropdown.value\n",
    "        if not filename:\n",
    "            print(\"❌ No file selected!\")\n",
    "            return\n",
    "        \n",
    "        filepath = T5_EMBEDDINGS_DIR / filename\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            loaded_t5_embedding = np.array(data['embedding'])\n",
    "            loaded_t5_prompt = data.get('prompt', 'Unknown')\n",
    "            \n",
    "            print(f\"✓ Loaded T5 embedding!\")\n",
    "            print(f\"  File: {filename}\")\n",
    "            print(f\"  Prompt: '{loaded_t5_prompt}'\")\n",
    "            print(f\"  Shape: {loaded_t5_embedding.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading T5 embedding: {e}\")\n",
    "\n",
    "load_t5_button.on_click(load_t5_embedding_file)\n",
    "\n",
    "# ==================== CLIP EMBEDDING WIDGETS ====================\n",
    "\n",
    "# CLIP embedding selection\n",
    "clip_file_dropdown = widgets.Dropdown(\n",
    "    options=[],\n",
    "    description='CLIP Embedding:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='600px')\n",
    ")\n",
    "\n",
    "load_clip_button = widgets.Button(\n",
    "    description='Load CLIP',\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "clip_load_output = widgets.Output()\n",
    "\n",
    "# Populate CLIP dropdown options (including subfolders)\n",
    "if CLIP_EMBEDDINGS_DIR.exists():\n",
    "    clip_files = sorted([str(f.relative_to(CLIP_EMBEDDINGS_DIR)) for f in CLIP_EMBEDDINGS_DIR.glob('**/*.json')])\n",
    "    clip_file_dropdown.options = clip_files\n",
    "\n",
    "def load_clip_embedding_file(b):\n",
    "    global loaded_clip_embedding, loaded_clip_prompt\n",
    "    \n",
    "    with clip_load_output:\n",
    "        clip_load_output.clear_output()\n",
    "        \n",
    "        filename = clip_file_dropdown.value\n",
    "        if not filename:\n",
    "            print(\"❌ No file selected!\")\n",
    "            return\n",
    "        \n",
    "        filepath = CLIP_EMBEDDINGS_DIR / filename\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            loaded_clip_embedding = np.array(data['embedding'])\n",
    "            loaded_clip_prompt = data.get('prompt', 'Unknown')\n",
    "            \n",
    "            print(f\"✓ Loaded CLIP embedding!\")\n",
    "            print(f\"  File: {filename}\")\n",
    "            print(f\"  Prompt: '{loaded_clip_prompt}'\")\n",
    "            print(f\"  Shape: {loaded_clip_embedding.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading CLIP embedding: {e}\")\n",
    "\n",
    "load_clip_button.on_click(load_clip_embedding_file)\n",
    "\n",
    "# ==================== GENERATION CONTROLS ====================\n",
    "\n",
    "seed_input = widgets.IntText(\n",
    "    value=42,\n",
    "    description='Seed:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "steps_input = widgets.IntSlider(\n",
    "    value=4,\n",
    "    min=1,\n",
    "    max=50,\n",
    "    step=1,\n",
    "    description='Inference Steps:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "width_input = widgets.IntText(\n",
    "    value=512,\n",
    "    description='Width:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "height_input = widgets.IntText(\n",
    "    value=512,\n",
    "    description='Height:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "generate_button = widgets.Button(\n",
    "    description='Generate Image',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='300px', height='50px')\n",
    ")\n",
    "\n",
    "generation_output = widgets.Output()\n",
    "\n",
    "def format_filename_for_display(filename):\n",
    "    \"\"\"\n",
    "    Convert filename to display format:\n",
    "    - Remove .json extension\n",
    "    - Replace underscores with spaces\n",
    "    - Clean up scaling numbers (remove trailing zeros)\n",
    "    - Capitalize first letter of each word\n",
    "    - Identify modification part for styling\n",
    "    \n",
    "    Example: \n",
    "    'a_green-winged_macaw_scaled_0.8000000000000002x.json' -> \n",
    "    ('A green-winged macaw scaled 0.8', 'scaled 0.8')\n",
    "    \"\"\"\n",
    "    if not filename:\n",
    "        return ('No filename', '')\n",
    "    \n",
    "    # Get just the basename without extension\n",
    "    basename = os.path.basename(filename)\n",
    "    name_without_ext = basename.rsplit('.', 1)[0]\n",
    "    \n",
    "    # Split by underscore\n",
    "    parts = name_without_ext.split('_')\n",
    "    \n",
    "    if not parts:\n",
    "        return ('Unknown', '')\n",
    "    \n",
    "    # Clean up scaling numbers\n",
    "    cleaned_parts = []\n",
    "    for part in parts:\n",
    "        # Check if part ends with 'x' and contains numbers (scaling factor)\n",
    "        if part.endswith('x') and any(c.isdigit() for c in part):\n",
    "            # Try to parse as float and format nicely\n",
    "            try:\n",
    "                number_part = part.rstrip('x')\n",
    "                # Convert to float and format to remove trailing zeros\n",
    "                formatted_num = f\"{float(number_part):g}\"\n",
    "                cleaned_parts.append(f\"{formatted_num}x\")\n",
    "                continue\n",
    "            except:\n",
    "                pass\n",
    "        cleaned_parts.append(part)\n",
    "    \n",
    "    # Join with spaces and capitalize first letter of each word\n",
    "    display_text = ' '.join(cleaned_parts)\n",
    "    \n",
    "    # Simple capitalization (first letter of the string and after spaces)\n",
    "    display_text = ' '.join(word[0].upper() + word[1:] if word else '' \n",
    "                          for word in display_text.split(' '))\n",
    "    \n",
    "    # Try to identify modification part (usually after first 3-4 words)\n",
    "    # For our format: description_words + modification_type + modification_value\n",
    "    words = display_text.split()\n",
    "    modification_start_idx = -1\n",
    "    \n",
    "    # Look for common modification indicators\n",
    "    modification_keywords = ['scaled', 'inverted', 'zeroed', 'shifted', 'blended', 'mixed']\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        lower_word = word.lower().rstrip('0123456789x')\n",
    "        if lower_word in modification_keywords:\n",
    "            modification_start_idx = i\n",
    "            break\n",
    "    \n",
    "    if modification_start_idx >= 0:\n",
    "        modification_text = ' '.join(words[modification_start_idx:])\n",
    "        # Keep the display text as is for now\n",
    "        return (display_text, modification_text)\n",
    "    \n",
    "    return (display_text, '')\n",
    "\n",
    "\n",
    "def generate_from_loaded_embeddings(b):\n",
    "    \"\"\"\n",
    "    Generate image using loaded T5 and CLIP embeddings with Polaroid-style filename display.\n",
    "    \"\"\"\n",
    "    with generation_output:\n",
    "        generation_output.clear_output()\n",
    "        \n",
    "        if 'flux_pipe' not in globals():\n",
    "            print(\"❌ FLUX not loaded!\")\n",
    "            return\n",
    "        \n",
    "        if loaded_t5_embedding is None:\n",
    "            print(\"❌ No T5 embedding loaded! Load a T5 embedding first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Generating image from loaded embeddings...\")\n",
    "        print(f\"  Seed: {seed_input.value}\")\n",
    "        print(f\"  Inference Steps: {steps_input.value}\")\n",
    "        print(f\"  Dimensions: {width_input.value}x{height_input.value}\")\n",
    "        print(f\"  T5 embedding shape: {loaded_t5_embedding.shape}\")\n",
    "        print(f\"  T5 prompt: '{loaded_t5_prompt}'\")\n",
    "        \n",
    "        # Convert T5 numpy to torch tensor with bfloat16\n",
    "        t5_tensor = torch.from_numpy(loaded_t5_embedding.astype(np.float32)).to(\n",
    "            device=device,\n",
    "            dtype=torch.bfloat16\n",
    "        )\n",
    "        \n",
    "        # Add batch dimension: [512, 4096] -> [1, 512, 4096]\n",
    "        t5_tensor = t5_tensor.unsqueeze(0)\n",
    "        \n",
    "        # Process CLIP embeddings\n",
    "        print(\"\\nProcessing CLIP pooled embeddings...\")\n",
    "        \n",
    "        if loaded_clip_embedding is not None:\n",
    "            # Use loaded CLIP embedding\n",
    "            print(f\"  Using loaded CLIP embedding\")\n",
    "            print(f\"  CLIP embedding shape: {loaded_clip_embedding.shape}\")\n",
    "            print(f\"  CLIP prompt: '{loaded_clip_prompt}'\")\n",
    "            \n",
    "            # Use last token embedding as pooled embedding (EOS token)\n",
    "            pooled_embeds = torch.from_numpy(\n",
    "                loaded_clip_embedding[-1:].astype(np.float32)\n",
    "            ).to(device=device, dtype=torch.bfloat16)\n",
    "            \n",
    "        else:\n",
    "            # Fall back to generating from T5 prompt using CLIP\n",
    "            print(f\"  No CLIP embedding loaded, generating from T5 prompt using CLIP model...\")\n",
    "            \n",
    "            if loaded_t5_prompt:\n",
    "                with torch.no_grad():\n",
    "                    (\n",
    "                        _,\n",
    "                        pooled_embeds,\n",
    "                        _,\n",
    "                    ) = flux_pipe.encode_prompt(\n",
    "                        prompt=loaded_t5_prompt,\n",
    "                        prompt_2=None,\n",
    "                        device=device,\n",
    "                        num_images_per_prompt=1,\n",
    "                        prompt_embeds=None,\n",
    "                        pooled_prompt_embeds=None,\n",
    "                        max_sequence_length=512,\n",
    "                    )\n",
    "            else:\n",
    "                print(\"❌ No CLIP embedding and no prompt available!\")\n",
    "                return\n",
    "        \n",
    "        print(f\"  Pooled embeddings shape: {pooled_embeds.shape}\")\n",
    "        \n",
    "        # Get base filename for saving\n",
    "        t5_tokens, t5_manip = parse_embedding_filename(t5_file_dropdown.value)\n",
    "        clip_tokens, clip_manip = parse_embedding_filename(clip_file_dropdown.value)\n",
    "        \n",
    "        # Build filename: t5tokens_t5manip_cliptokens_clipmanip.png\n",
    "        filename_parts = [t5_tokens]\n",
    "        if t5_manip:\n",
    "            filename_parts.append(t5_manip)\n",
    "        filename_parts.append(clip_tokens)\n",
    "        if clip_manip:\n",
    "            filename_parts.append(clip_manip)\n",
    "        \n",
    "        base_filename = '_'.join(filename_parts)\n",
    "        filename = base_filename + '.png'\n",
    "        output_filepath = OUTPUT_IMAGES_DIR / filename\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        output_filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Generate image\n",
    "        try:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"Running FLUX diffusion...\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "            \n",
    "            image = flux_pipe(\n",
    "                prompt_embeds=t5_tensor,\n",
    "                pooled_prompt_embeds=pooled_embeds,\n",
    "                num_inference_steps=steps_input.value,\n",
    "                guidance_scale=0.0,\n",
    "                height=height_input.value,\n",
    "                width=width_input.value,\n",
    "                generator=torch.manual_seed(seed_input.value)\n",
    "            ).images[0]\n",
    "            \n",
    "            # ==================== POLAROID-STYLE MODIFICATION ====================\n",
    "            # Create a Polaroid-style image with formatted filename at the bottom\n",
    "            \n",
    "            # Convert image to RGB if it's not\n",
    "            image = image.convert('RGB')\n",
    "            \n",
    "            # Get original dimensions\n",
    "            orig_width, orig_height = image.size\n",
    "            \n",
    "            # Calculate Polaroid dimensions - increased height for two lines\n",
    "            border_top = 20\n",
    "            border_sides = 20\n",
    "            filename_area_height = 140  # Height for T5 + CLIP + model lines\n",
    "            polaroid_width = orig_width + (2 * border_sides)\n",
    "            polaroid_height = orig_height + border_top + filename_area_height + 20\n",
    "            \n",
    "            # Create a new white image for Polaroid effect\n",
    "            polaroid = Image.new('RGB', (polaroid_width, polaroid_height), color='white')\n",
    "            \n",
    "            # Paste the original image centered with top border\n",
    "            image_position = (border_sides, border_top)\n",
    "            polaroid.paste(image, image_position)\n",
    "            \n",
    "            # Draw a rectangle for the filename area\n",
    "            draw = ImageDraw.Draw(polaroid)\n",
    "            \n",
    "            # No background rectangle - just white background\n",
    "            filename_rect_top = orig_height + border_top + 10\n",
    "            filename_rect_bottom = polaroid_height - 10\n",
    "            # No rectangle drawn here - just keep white background\n",
    "            \n",
    "            # Format the filenames for display\n",
    "            t5_filename = t5_file_dropdown.value\n",
    "            clip_filename = clip_file_dropdown.value\n",
    "            \n",
    "            # Get formatted display text for each\n",
    "            t5_display, t5_mod = format_filename_for_display(t5_filename)\n",
    "            clip_display, clip_mod = format_filename_for_display(clip_filename)\n",
    "            \n",
    "            # Create labeled text lines with FLUX-Schnell\n",
    "            if clip_display != 'No filename' and clip_display != 'Unknown':\n",
    "                # Both T5 and CLIP are loaded\n",
    "                t5_line = f\"T5: {t5_display}\"\n",
    "                clip_line = f\"CLIP: {clip_display}\"\n",
    "                model_line = \"FLUX-Schnell\"\n",
    "                text_lines = [t5_line, clip_line, model_line]\n",
    "                \n",
    "                # Track modifications for styling\n",
    "                modifications = {\n",
    "                    0: t5_mod,  # T5 line index\n",
    "                    1: clip_mod  # CLIP line index\n",
    "                }\n",
    "            else:\n",
    "                # Only T5 is loaded\n",
    "                t5_line = f\"T5: {t5_display}\"\n",
    "                model_line = \"FLUX-Schnell\"\n",
    "                text_lines = [t5_line, model_line]\n",
    "                modifications = {0: t5_mod}\n",
    "            \n",
    "            print(f\"  T5: {t5_display}\")\n",
    "            if clip_display not in ['No filename', 'Unknown']:\n",
    "                print(f\"  CLIP: {clip_display}\")\n",
    "            print(f\"  Model: FLUX-Schnell\")\n",
    "            \n",
    "            # Load fonts with different sizes for dynamic scaling\n",
    "            fonts = {}\n",
    "            label_fonts = {}  # Separate fonts for labels (T5:, CLIP:)\n",
    "            try:\n",
    "                # Try to load DejaVuSans or fallback to default\n",
    "                font_paths = [\n",
    "                    '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf',\n",
    "                    '/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf',\n",
    "                    'Arial.ttf',\n",
    "                    'Helvetica.ttf'\n",
    "                ]\n",
    "                \n",
    "                loaded_font = None\n",
    "                for font_path in font_paths:\n",
    "                    try:\n",
    "                        loaded_font = ImageFont.truetype(font_path, 20)\n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if loaded_font is None:\n",
    "                    # Create a simple bitmap font as fallback\n",
    "                    loaded_font = ImageFont.load_default()\n",
    "                \n",
    "                # Create font variations for body text\n",
    "                for size in [8, 10, 12, 14, 16, 18, 20, 22]:\n",
    "                    try:\n",
    "                        if loaded_font != ImageFont.load_default():\n",
    "                            fonts[size] = loaded_font.font_variant(size=size)\n",
    "                        else:\n",
    "                            fonts[size] = loaded_font\n",
    "                    except:\n",
    "                        fonts[size] = loaded_font\n",
    "                \n",
    "                # Create slightly smaller, bold-ish fonts for labels\n",
    "                try:\n",
    "                    for font_path in font_paths:\n",
    "                        try:\n",
    "                            bold_font = ImageFont.truetype(font_path, 18)\n",
    "                            # Create semi-bold effect by making it slightly smaller but drawing twice\n",
    "                            for size in [10, 12, 14, 16]:\n",
    "                                label_fonts[size] = bold_font.font_variant(size=size)\n",
    "                            break\n",
    "                        except:\n",
    "                            continue\n",
    "                except:\n",
    "                    # Fallback to same fonts if bold not available\n",
    "                    label_fonts = fonts\n",
    "                        \n",
    "            except:\n",
    "                # Ultimate fallback\n",
    "                fonts = {16: ImageFont.load_default()}\n",
    "                label_fonts = fonts\n",
    "            \n",
    "            # Function to calculate text dimensions\n",
    "            def get_text_dimensions(text, font):\n",
    "                if hasattr(font, 'getbbox'):\n",
    "                    bbox = font.getbbox(text)\n",
    "                    return bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "                elif hasattr(font, 'getsize'):\n",
    "                    return font.getsize(text)\n",
    "                else:\n",
    "                    # Rough estimate\n",
    "                    return len(text) * 10, 20\n",
    "            \n",
    "            # Function to wrap text with dynamic font sizing\n",
    "            def wrap_text_with_dynamic_size(text_lines, max_width, start_size=20):\n",
    "                # Try decreasing font sizes until all lines fit\n",
    "                for font_size in sorted(fonts.keys(), reverse=True):\n",
    "                    if font_size < 10:  # Minimum readable size\n",
    "                        break\n",
    "                    \n",
    "                    font = fonts[font_size]\n",
    "                    label_font = label_fonts.get(font_size - 2, font)  # Labels slightly smaller\n",
    "                    \n",
    "                    all_lines_fit = True\n",
    "                    wrapped_result = []\n",
    "                    \n",
    "                    for line in text_lines:\n",
    "                        # Split label from content\n",
    "                        if ': ' in line:\n",
    "                            label, content = line.split(': ', 1)\n",
    "                            label = label + ': '\n",
    "                        else:\n",
    "                            label = ''\n",
    "                            content = line\n",
    "                        \n",
    "                        # Check if label fits\n",
    "                        label_width, _ = get_text_dimensions(label, label_font)\n",
    "                        \n",
    "                        # Available width for content\n",
    "                        content_max_width = max_width - label_width\n",
    "                        \n",
    "                        # Wrap content\n",
    "                        words = content.split()\n",
    "                        content_lines = []\n",
    "                        current_line = []\n",
    "                        \n",
    "                        for word in words:\n",
    "                            test_line = ' '.join(current_line + [word])\n",
    "                            line_width, _ = get_text_dimensions(test_line, font)\n",
    "                            \n",
    "                            if line_width <= content_max_width:\n",
    "                                current_line.append(word)\n",
    "                            else:\n",
    "                                if current_line:\n",
    "                                    content_lines.append(' '.join(current_line))\n",
    "                                current_line = [word]\n",
    "                        \n",
    "                        if current_line:\n",
    "                            content_lines.append(' '.join(current_line))\n",
    "                        \n",
    "                        # Check if any content line is too wide\n",
    "                        for content_line in content_lines:\n",
    "                            content_width, _ = get_text_dimensions(content_line, font)\n",
    "                            if content_width > content_max_width:\n",
    "                                all_lines_fit = False\n",
    "                                break\n",
    "                        \n",
    "                        if not all_lines_fit:\n",
    "                            break\n",
    "                        \n",
    "                        # Store result for this line\n",
    "                        wrapped_result.append({\n",
    "                            'label': label,\n",
    "                            'content_lines': content_lines,\n",
    "                            'label_width': label_width\n",
    "                        })\n",
    "                    \n",
    "                    if all_lines_fit:\n",
    "                        return wrapped_result, font_size, label_font, font\n",
    "                \n",
    "                # If we get here, use smallest font\n",
    "                font_size = min(fonts.keys())\n",
    "                font = fonts[font_size]\n",
    "                label_font = label_fonts.get(font_size, font)\n",
    "                \n",
    "                # Simple wrapping with smallest font\n",
    "                wrapped_result = []\n",
    "                for line in text_lines:\n",
    "                    if ': ' in line:\n",
    "                        label, content = line.split(': ', 1)\n",
    "                        label = label + ': '\n",
    "                    else:\n",
    "                        label = ''\n",
    "                        content = line\n",
    "                    \n",
    "                    label_width, _ = get_text_dimensions(label, label_font)\n",
    "                    content_max_width = max_width - label_width\n",
    "                    \n",
    "                    # Simple character-based wrapping as fallback\n",
    "                    chars_per_line = max(1, content_max_width // 8)\n",
    "                    wrapped_content = textwrap.wrap(content, width=chars_per_line)\n",
    "                    \n",
    "                    wrapped_result.append({\n",
    "                        'label': label,\n",
    "                        'content_lines': wrapped_content,\n",
    "                        'label_width': label_width\n",
    "                    })\n",
    "                \n",
    "                return wrapped_result, font_size, label_font, font\n",
    "            \n",
    "            # Available width for text\n",
    "            text_area_width = polaroid_width - (2 * border_sides) - 20\n",
    "            max_text_width = text_area_width - 20\n",
    "            \n",
    "            # Wrap the text and find appropriate font size\n",
    "            wrapped_lines, font_size, label_font, content_font = wrap_text_with_dynamic_size(\n",
    "                text_lines, max_text_width\n",
    "            )\n",
    "            \n",
    "            # Calculate line heights\n",
    "            _, label_height = get_text_dimensions(\"T5:\", label_font)\n",
    "            _, content_height = get_text_dimensions(\"Mg\", content_font)\n",
    "            line_height = max(label_height, content_height)\n",
    "            line_spacing = int(line_height * 0.4)\n",
    "            total_line_height = line_height + line_spacing\n",
    "            \n",
    "            # Calculate total text height (accounting for multi-line content)\n",
    "            total_text_height = 0\n",
    "            for wrapped in wrapped_lines:\n",
    "                total_text_height += len(wrapped['content_lines']) * total_line_height\n",
    "            \n",
    "            # Calculate starting Y position (center vertically in filename area)\n",
    "            text_area_height = filename_rect_bottom - filename_rect_top\n",
    "            text_start_y = filename_rect_top + (text_area_height - total_text_height) // 2 + 5\n",
    "            \n",
    "            # Draw each line\n",
    "            current_y = text_start_y\n",
    "            for line_idx, wrapped in enumerate(wrapped_lines):\n",
    "                label = wrapped['label']\n",
    "                content_lines = wrapped['content_lines']\n",
    "                label_width = wrapped['label_width']\n",
    "                \n",
    "                # Draw each content line for this label\n",
    "                for content_line_idx, content_line in enumerate(content_lines):\n",
    "                    # Calculate position for this line\n",
    "                    content_width, _ = get_text_dimensions(content_line, content_font)\n",
    "                    \n",
    "                    # Total width of this line (label + content)\n",
    "                    total_width = label_width + content_width\n",
    "                    text_x = border_sides + (text_area_width - total_width) // 2\n",
    "                    \n",
    "                    # Draw label (semi-bold, dark color)\n",
    "                    if label:\n",
    "                        # Draw label without shadow\n",
    "                        draw.text(\n",
    "                            (text_x, current_y),\n",
    "                            label,\n",
    "                            fill='#333333',  # Dark color for labels\n",
    "                            font=label_font\n",
    "                        )\n",
    "                    \n",
    "                    # Draw content\n",
    "                    content_x = text_x + label_width\n",
    "                    \n",
    "                    # Check if this line has modification for styling\n",
    "                    line_has_modification = False\n",
    "                    if line_idx in modifications and modifications[line_idx]:\n",
    "                        mod_words = modifications[line_idx].lower().split()\n",
    "                        content_words = content_line.lower().split()\n",
    "                        for mod_word in mod_words:\n",
    "                            if any(mod_word in content_word or content_word in mod_word \n",
    "                                  for content_word in content_words):\n",
    "                                line_has_modification = True\n",
    "                                break\n",
    "                    \n",
    "                    # Draw content with potential bold effect for modifications\n",
    "                    if line_has_modification and font_size >= 12:\n",
    "                        # Create bold effect for modifications\n",
    "                        offset = 1\n",
    "                        for dx, dy in [(offset, 0), (-offset, 0), (0, offset), (0, -offset)]:\n",
    "                            draw.text(\n",
    "                                (content_x + dx, current_y + dy),\n",
    "                                content_line,\n",
    "                                fill='#444444',\n",
    "                                font=content_font\n",
    "                            )\n",
    "                        # Draw main text\n",
    "                        draw.text(\n",
    "                            (content_x, current_y),\n",
    "                            content_line,\n",
    "                            fill='#222222',\n",
    "                            font=content_font\n",
    "                        )\n",
    "                    else:\n",
    "                        # Draw regular content without shadow\n",
    "                        draw.text(\n",
    "                            (content_x, current_y),\n",
    "                            content_line,\n",
    "                            fill='#000000',\n",
    "                            font=content_font\n",
    "                        )\n",
    "                    \n",
    "                    current_y += total_line_height\n",
    "            \n",
    "            # No shadow effects - just keep the image as is\n",
    "            polaroid = polaroid.convert('RGB')\n",
    "            \n",
    "            # No outer border - just keep the white background\n",
    "            \n",
    "            # Save the Polaroid-style image\n",
    "            polaroid.save(output_filepath)\n",
    "            print(f\"✓ Image generated and saved with Polaroid-style filename!\")\n",
    "            print(f\"  Path: {output_filepath}\")\n",
    "            print(f\"  Filename: {filename}\")\n",
    "            print(f\"  Font size used: {font_size}pt\")\n",
    "            print(f\"  Polaroid dimensions: {polaroid_width}x{polaroid_height}\\n\")\n",
    "            \n",
    "            # Display the Polaroid image\n",
    "            display(polaroid)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error generating image: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "generate_button.on_click(generate_from_loaded_embeddings)\n",
    "\n",
    "# ==================== DISPLAY UNIFIED INTERFACE ====================\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h2>Embedding Selection & Image Generation</h2>\"),\n",
    "    \n",
    "    # T5 Embedding Section\n",
    "    widgets.HTML(\"<h3>1. T5 Embedding (Required)</h3>\"),\n",
    "    t5_file_dropdown,\n",
    "    load_t5_button,\n",
    "    t5_load_output,\n",
    "    \n",
    "    # CLIP Embedding Section\n",
    "    widgets.HTML(\"<br><h3>2. CLIP Embedding (Optional - auto-generated if not loaded)</h3>\"),\n",
    "    clip_file_dropdown,\n",
    "    load_clip_button,\n",
    "    clip_load_output,\n",
    "    \n",
    "    # Generation Controls Section\n",
    "    widgets.HTML(\"<br><h3>3. Generation Controls</h3>\"),\n",
    "    seed_input,\n",
    "    steps_input,\n",
    "    widgets.HTML(\"<br><b>Image Dimensions:</b>\"),\n",
    "    widgets.HBox([width_input, height_input]),\n",
    "    widgets.HTML(\"<br>\"),\n",
    "    generate_button,\n",
    "    \n",
    "    # Output Section\n",
    "    widgets.HTML(\"<br><h3>Generated Image</h3>\"),\n",
    "    generation_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generation\n",
    "\n",
    "Generate multiple images with different embedding combinations and organize them into subfolders based on token length:\n",
    "- `short/` - Generated from short text prompts (encoded at runtime)\n",
    "- `512_tokens/` - Generated from pre-saved T5 embeddings (512 token sequences)\n",
    "- `77_tokens/` - Generated from pre-saved CLIP embeddings (77 token sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==================== BATCH GENERATION WITH SUBFOLDER ORGANIZATION ====================\n",
    "\n",
    "# # Create output subdirectories\n",
    "# OUTPUT_SHORT_DIR = OUTPUT_IMAGES_DIR / \"short\"\n",
    "# OUTPUT_512_DIR = OUTPUT_IMAGES_DIR / \"512_tokens\"\n",
    "# OUTPUT_77_DIR = OUTPUT_IMAGES_DIR / \"77_tokens\"\n",
    "\n",
    "# for dir_path in [OUTPUT_SHORT_DIR, OUTPUT_512_DIR, OUTPUT_77_DIR]:\n",
    "#     os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# # ==================== T5 EMBEDDING SOURCE SELECTION ====================\n",
    "\n",
    "# t5_source_radio = widgets.RadioButtons(\n",
    "#     options=[\n",
    "#         ('Use T5 embedding file (512 tokens)', '512_tokens'),\n",
    "#         ('Generate from short prompt', 'short')\n",
    "#     ],\n",
    "#     value='512_tokens',\n",
    "#     description='T5 Source:',\n",
    "#     style={'description_width': 'initial'},\n",
    "#     layout=widgets.Layout(width='400px')\n",
    "# )\n",
    "\n",
    "# # Short prompt input (for generating T5 at runtime)\n",
    "# batch_short_prompt = widgets.Text(\n",
    "#     value='',\n",
    "#     placeholder='Enter short prompt (e.g., \"a red cat\")',\n",
    "#     description='Short Prompt:',\n",
    "#     style={'description_width': 'initial'},\n",
    "#     layout=widgets.Layout(width='600px')\n",
    "# )\n",
    "\n",
    "# # T5 file selection (for pre-saved embeddings)\n",
    "# batch_t5_dropdown = widgets.Dropdown(\n",
    "#     options=[],\n",
    "#     description='T5 File:',\n",
    "#     style={'description_width': 'initial'},\n",
    "#     layout=widgets.Layout(width='600px')\n",
    "# )\n",
    "\n",
    "# # Populate T5 dropdown\n",
    "# if T5_EMBEDDINGS_DIR.exists():\n",
    "#     t5_files = sorted([str(f.relative_to(T5_EMBEDDINGS_DIR)) for f in T5_EMBEDDINGS_DIR.glob('**/*.json')])\n",
    "#     batch_t5_dropdown.options = t5_files\n",
    "\n",
    "# # ==================== CLIP EMBEDDING SOURCE SELECTION ====================\n",
    "\n",
    "# clip_source_radio = widgets.RadioButtons(\n",
    "#     options=[\n",
    "#         ('Use CLIP embedding file (77 tokens)', '77_tokens'),\n",
    "#         ('Generate from short prompt', 'short'),\n",
    "#         ('Auto-generate from T5 prompt', 'auto')\n",
    "#     ],\n",
    "#     value='77_tokens',\n",
    "#     description='CLIP Source:',\n",
    "#     style={'description_width': 'initial'},\n",
    "#     layout=widgets.Layout(width='400px')\n",
    "# )\n",
    "\n",
    "# # Short prompt input for CLIP\n",
    "# batch_clip_prompt = widgets.Text(\n",
    "#     value='',\n",
    "#     placeholder='Enter short prompt for CLIP (e.g., \"a red cat\")',\n",
    "#     description='CLIP Prompt:',\n",
    "#     style={'description_width': 'initial'},\n",
    "#     layout=widgets.Layout(width='600px')\n",
    "# )\n",
    "\n",
    "# # CLIP file selection\n",
    "# batch_clip_dropdown = widgets.Dropdown(\n",
    "#     options=[],\n",
    "#     description='CLIP File:',\n",
    "#     style={'description_width': 'initial'},\n",
    "#     layout=widgets.Layout(width='600px')\n",
    "# )\n",
    "\n",
    "# # Populate CLIP dropdown\n",
    "# if CLIP_EMBEDDINGS_DIR.exists():\n",
    "#     clip_files = sorted([str(f.relative_to(CLIP_EMBEDDINGS_DIR)) for f in CLIP_EMBEDDINGS_DIR.glob('**/*.json')])\n",
    "#     batch_clip_dropdown.options = clip_files\n",
    "\n",
    "# # ==================== GENERATION CONTROLS ====================\n",
    "\n",
    "# batch_seed = widgets.IntText(value=42, description='Seed:', style={'description_width': 'initial'})\n",
    "# batch_steps = widgets.IntSlider(value=4, min=1, max=50, description='Steps:', style={'description_width': 'initial'}, layout=widgets.Layout(width='400px'))\n",
    "# batch_width = widgets.IntText(value=1024, description='Width:', style={'description_width': 'initial'})\n",
    "# batch_height = widgets.IntText(value=1024, description='Height:', style={'description_width': 'initial'})\n",
    "\n",
    "# batch_generate_button = widgets.Button(\n",
    "#     description='Generate Batch Image',\n",
    "#     button_style='primary',\n",
    "#     layout=widgets.Layout(width='300px', height='50px')\n",
    "# )\n",
    "\n",
    "# batch_output = widgets.Output()\n",
    "\n",
    "# # ==================== VISIBILITY HANDLERS ====================\n",
    "\n",
    "# def update_t5_visibility(change):\n",
    "#     if change['new'] == '512_tokens':\n",
    "#         batch_t5_dropdown.layout.display = 'flex'\n",
    "#         batch_short_prompt.layout.display = 'none'\n",
    "#     else:\n",
    "#         batch_t5_dropdown.layout.display = 'none'\n",
    "#         batch_short_prompt.layout.display = 'flex'\n",
    "\n",
    "# def update_clip_visibility(change):\n",
    "#     if change['new'] == '77_tokens':\n",
    "#         batch_clip_dropdown.layout.display = 'flex'\n",
    "#         batch_clip_prompt.layout.display = 'none'\n",
    "#     elif change['new'] == 'short':\n",
    "#         batch_clip_dropdown.layout.display = 'none'\n",
    "#         batch_clip_prompt.layout.display = 'flex'\n",
    "#     else:  # auto\n",
    "#         batch_clip_dropdown.layout.display = 'none'\n",
    "#         batch_clip_prompt.layout.display = 'none'\n",
    "\n",
    "# t5_source_radio.observe(update_t5_visibility, names='value')\n",
    "# clip_source_radio.observe(update_clip_visibility, names='value')\n",
    "\n",
    "# # Initialize visibility\n",
    "# batch_short_prompt.layout.display = 'none'\n",
    "# batch_clip_prompt.layout.display = 'none'\n",
    "\n",
    "# # ==================== BATCH GENERATION FUNCTION ====================\n",
    "\n",
    "# def determine_output_subfolder(t5_source, clip_source):\n",
    "#     \"\"\"\n",
    "#     Determine output subfolder based on embedding sources.\n",
    "#     Priority: short > 512_tokens > 77_tokens\n",
    "#     \"\"\"\n",
    "#     if t5_source == 'short' or clip_source == 'short':\n",
    "#         return OUTPUT_SHORT_DIR, 'short'\n",
    "#     elif t5_source == '512_tokens':\n",
    "#         return OUTPUT_512_DIR, '512_tokens'\n",
    "#     else:\n",
    "#         return OUTPUT_77_DIR, '77_tokens'\n",
    "\n",
    "# def sanitize_prompt_for_filename(prompt):\n",
    "#     \"\"\"Convert prompt to filename-safe string.\"\"\"\n",
    "#     # Replace spaces with underscores, keep only alphanumeric and underscores\n",
    "#     safe = ''.join(c if c.isalnum() else '_' for c in prompt.lower())\n",
    "#     # Collapse multiple underscores\n",
    "#     while '__' in safe:\n",
    "#         safe = safe.replace('__', '_')\n",
    "#     return safe.strip('_')[:50]  # Limit length\n",
    "\n",
    "# def generate_batch_image(b):\n",
    "#     \"\"\"Generate image with subfolder organization based on embedding sources.\"\"\"\n",
    "#     with batch_output:\n",
    "#         batch_output.clear_output()\n",
    "        \n",
    "#         if 'flux_pipe' not in globals():\n",
    "#             print(\"FLUX not loaded!\")\n",
    "#             return\n",
    "        \n",
    "#         t5_source = t5_source_radio.value\n",
    "#         clip_source = clip_source_radio.value\n",
    "        \n",
    "#         print(f\"T5 source: {t5_source}\")\n",
    "#         print(f\"CLIP source: {clip_source}\")\n",
    "        \n",
    "#         # ==================== PREPARE T5 EMBEDDINGS ====================\n",
    "        \n",
    "#         if t5_source == 'short':\n",
    "#             # Generate T5 embedding from short prompt\n",
    "#             prompt = batch_short_prompt.value.strip()\n",
    "#             if not prompt:\n",
    "#                 print(\"Please enter a short prompt for T5!\")\n",
    "#                 return\n",
    "            \n",
    "#             print(f\"\\nGenerating T5 embedding from short prompt: '{prompt}'\")\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 (\n",
    "#                     t5_tensor,\n",
    "#                     _,\n",
    "#                     _,\n",
    "#                 ) = flux_pipe.encode_prompt(\n",
    "#                     prompt=prompt,\n",
    "#                     prompt_2=None,\n",
    "#                     device=device,\n",
    "#                     num_images_per_prompt=1,\n",
    "#                     prompt_embeds=None,\n",
    "#                     pooled_prompt_embeds=None,\n",
    "#                     max_sequence_length=512,\n",
    "#                 )\n",
    "            \n",
    "#             t5_name = sanitize_prompt_for_filename(prompt)\n",
    "#             t5_prompt_text = prompt\n",
    "#             print(f\"  T5 tensor shape: {t5_tensor.shape}\")\n",
    "            \n",
    "#         else:\n",
    "#             # Load from file\n",
    "#             filename = batch_t5_dropdown.value\n",
    "#             if not filename:\n",
    "#                 print(\"Please select a T5 embedding file!\")\n",
    "#                 return\n",
    "            \n",
    "#             filepath = T5_EMBEDDINGS_DIR / filename\n",
    "#             print(f\"\\nLoading T5 embedding from: {filename}\")\n",
    "            \n",
    "#             with open(filepath, 'r') as f:\n",
    "#                 data = json.load(f)\n",
    "            \n",
    "#             t5_embedding = np.array(data['embedding'])\n",
    "#             t5_prompt_text = data.get('prompt', 'Unknown')\n",
    "            \n",
    "#             t5_tensor = torch.from_numpy(t5_embedding.astype(np.float32)).to(\n",
    "#                 device=device,\n",
    "#                 dtype=torch.bfloat16\n",
    "#             ).unsqueeze(0)\n",
    "            \n",
    "#             t5_tokens, t5_manip = parse_embedding_filename(filename)\n",
    "#             t5_name = t5_tokens + ('_' + t5_manip if t5_manip else '')\n",
    "#             print(f\"  T5 tensor shape: {t5_tensor.shape}\")\n",
    "#             print(f\"  T5 prompt: '{t5_prompt_text}'\")\n",
    "        \n",
    "#         # ==================== PREPARE CLIP EMBEDDINGS ====================\n",
    "        \n",
    "#         if clip_source == '77_tokens':\n",
    "#             # Load from file\n",
    "#             filename = batch_clip_dropdown.value\n",
    "#             if not filename:\n",
    "#                 print(\"Please select a CLIP embedding file!\")\n",
    "#                 return\n",
    "            \n",
    "#             filepath = CLIP_EMBEDDINGS_DIR / filename\n",
    "#             print(f\"\\nLoading CLIP embedding from: {filename}\")\n",
    "            \n",
    "#             with open(filepath, 'r') as f:\n",
    "#                 data = json.load(f)\n",
    "            \n",
    "#             clip_embedding = np.array(data['embedding'])\n",
    "            \n",
    "#             # Use last token (EOS) as pooled embedding\n",
    "#             pooled_embeds = torch.from_numpy(\n",
    "#                 clip_embedding[-1:].astype(np.float32)\n",
    "#             ).to(device=device, dtype=torch.bfloat16)\n",
    "            \n",
    "#             clip_tokens, clip_manip = parse_embedding_filename(filename)\n",
    "#             clip_name = clip_tokens + ('_' + clip_manip if clip_manip else '')\n",
    "#             print(f\"  Pooled embeddings shape: {pooled_embeds.shape}\")\n",
    "            \n",
    "#         elif clip_source == 'short':\n",
    "#             # Generate from short prompt\n",
    "#             prompt = batch_clip_prompt.value.strip()\n",
    "#             if not prompt:\n",
    "#                 print(\"Please enter a short prompt for CLIP!\")\n",
    "#                 return\n",
    "            \n",
    "#             print(f\"\\nGenerating CLIP pooled embedding from short prompt: '{prompt}'\")\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 (\n",
    "#                     _,\n",
    "#                     pooled_embeds,\n",
    "#                     _,\n",
    "#                 ) = flux_pipe.encode_prompt(\n",
    "#                     prompt=prompt,\n",
    "#                     prompt_2=None,\n",
    "#                     device=device,\n",
    "#                     num_images_per_prompt=1,\n",
    "#                     prompt_embeds=None,\n",
    "#                     pooled_prompt_embeds=None,\n",
    "#                     max_sequence_length=512,\n",
    "#                 )\n",
    "            \n",
    "#             clip_name = sanitize_prompt_for_filename(prompt)\n",
    "#             print(f\"  Pooled embeddings shape: {pooled_embeds.shape}\")\n",
    "            \n",
    "#         else:  # auto - generate from T5 prompt\n",
    "#             print(f\"\\nAuto-generating CLIP pooled embedding from T5 prompt: '{t5_prompt_text}'\")\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 (\n",
    "#                     _,\n",
    "#                     pooled_embeds,\n",
    "#                     _,\n",
    "#                 ) = flux_pipe.encode_prompt(\n",
    "#                     prompt=t5_prompt_text,\n",
    "#                     prompt_2=None,\n",
    "#                     device=device,\n",
    "#                     num_images_per_prompt=1,\n",
    "#                     prompt_embeds=None,\n",
    "#                     pooled_prompt_embeds=None,\n",
    "#                     max_sequence_length=512,\n",
    "#                 )\n",
    "            \n",
    "#             clip_name = 'auto'\n",
    "#             print(f\"  Pooled embeddings shape: {pooled_embeds.shape}\")\n",
    "        \n",
    "#         # ==================== DETERMINE OUTPUT PATH ====================\n",
    "        \n",
    "#         output_dir, subfolder_name = determine_output_subfolder(t5_source, clip_source)\n",
    "        \n",
    "#         # Build filename\n",
    "#         filename = f\"{t5_name}_{clip_name}.png\"\n",
    "#         output_path = output_dir / filename\n",
    "        \n",
    "#         print(f\"\\nOutput subfolder: {subfolder_name}/\")\n",
    "#         print(f\"Output filename: {filename}\")\n",
    "        \n",
    "#         # ==================== GENERATE IMAGE ====================\n",
    "        \n",
    "#         try:\n",
    "#             print(f\"\\n{'='*60}\")\n",
    "#             print(\"Running FLUX diffusion...\")\n",
    "#             print(f\"{'='*60}\\n\")\n",
    "            \n",
    "#             image = flux_pipe(\n",
    "#                 prompt_embeds=t5_tensor,\n",
    "#                 pooled_prompt_embeds=pooled_embeds,\n",
    "#                 num_inference_steps=batch_steps.value,\n",
    "#                 guidance_scale=0.0,\n",
    "#                 height=batch_height.value,\n",
    "#                 width=batch_width.value,\n",
    "#                 generator=torch.manual_seed(batch_seed.value)\n",
    "#             ).images[0]\n",
    "            \n",
    "#             image.save(output_path)\n",
    "#             print(f\"Image saved to: {output_path}\")\n",
    "            \n",
    "#             display(image)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error generating image: {e}\")\n",
    "#             import traceback\n",
    "#             traceback.print_exc()\n",
    "\n",
    "# batch_generate_button.on_click(generate_batch_image)\n",
    "\n",
    "# # ==================== DISPLAY BATCH INTERFACE ====================\n",
    "\n",
    "# display(widgets.VBox([\n",
    "#     widgets.HTML(\"<h2>Batch Generation with Subfolder Organization</h2>\"),\n",
    "#     widgets.HTML(\"<p>Images are saved to subfolders based on embedding source:</p>\"),\n",
    "#     widgets.HTML(\"<ul><li><b>short/</b> - From short text prompts</li><li><b>512_tokens/</b> - From T5 embedding files</li><li><b>77_tokens/</b> - From CLIP embedding files</li></ul>\"),\n",
    "    \n",
    "#     widgets.HTML(\"<h3>T5 Embedding Source</h3>\"),\n",
    "#     t5_source_radio,\n",
    "#     batch_t5_dropdown,\n",
    "#     batch_short_prompt,\n",
    "    \n",
    "#     widgets.HTML(\"<br><h3>CLIP Embedding Source</h3>\"),\n",
    "#     clip_source_radio,\n",
    "#     batch_clip_dropdown,\n",
    "#     batch_clip_prompt,\n",
    "    \n",
    "#     widgets.HTML(\"<br><h3>Generation Settings</h3>\"),\n",
    "#     batch_seed,\n",
    "#     batch_steps,\n",
    "#     widgets.HBox([batch_width, batch_height]),\n",
    "#     widgets.HTML(\"<br>\"),\n",
    "#     batch_generate_button,\n",
    "    \n",
    "#     widgets.HTML(\"<br><h3>Output</h3>\"),\n",
    "#     batch_output\n",
    "# ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T14:54:17.592999Z",
     "iopub.status.busy": "2026-01-14T14:54:17.592871Z",
     "iopub.status.idle": "2026-01-14T14:54:21.078285Z",
     "shell.execute_reply": "2026-01-14T14:54:21.077807Z",
     "shell.execute_reply.started": "2026-01-14T14:54:17.592987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Could not delete /home/lauwag/data/.cache: Cannot call rmtree on a symbolic link\n",
      "✓ Deleted /home/lauwag/.cache/huggingface: 51.00 GB\n",
      "✓ Deleted /home/lauwag/.cache/uv: 0.05 GB\n",
      "\n",
      "Total freed: 51.05 GB\n"
     ]
    }
   ],
   "source": [
    "# Clean up cache to free home directory space\n",
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def cleanup_cache():\n",
    "    \"\"\"Delete cache directories to free up home quota\"\"\"\n",
    "    cache_dirs = [\n",
    "        Path.home() / \"data\" / \".cache\",\n",
    "        Path.home() / \".cache\" / \"huggingface\",\n",
    "        Path.home() / \".cache\" / \"torch\",\n",
    "        Path.home() / \".cache\" / \"uv\",\n",
    "    ]\n",
    "    \n",
    "    total_freed = 0\n",
    "    for cache_dir in cache_dirs:\n",
    "        if cache_dir.exists():\n",
    "            try:\n",
    "                # Get size before deletion (approximate)\n",
    "                size = sum(f.stat().st_size for f in cache_dir.rglob('*') if f.is_file())\n",
    "                shutil.rmtree(cache_dir)\n",
    "                total_freed += size\n",
    "                print(f\"\\u2713 Deleted {cache_dir}: {size / 1024**3:.2f} GB\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\u2717 Could not delete {cache_dir}: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal freed: {total_freed / 1024**3:.2f} GB\")\n",
    "\n",
    "# Run cleanup\n",
    "cleanup_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<sub>Latent Vandalism Workshop • Laura Wagner, 2026 • [laurajul.github.io](https://laurajul.github.io/)</sub>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flux uv",
   "language": "python",
   "name": "uv_flux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
