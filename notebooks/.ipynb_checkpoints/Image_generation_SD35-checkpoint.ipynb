{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 & CLIP Embedding Manipulation for Stable Diffusion 3 Medium (TensorRT)\n",
    "\n",
    "This notebook lets you:\n",
    "1. Generate T5 and CLIP embeddings from text prompts\n",
    "2. Save/load embeddings as JSON\n",
    "3. Select positive AND negative embeddings for both T5 and CLIP\n",
    "4. Control guidance scale\n",
    "5. Generate images with SD3 Medium TensorRT using modified embeddings\n",
    "6. Compare with FLUX results\n",
    "\n",
    "**Note:** This uses the TensorRT-optimized SD3 Medium model for faster inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:15:26.861516Z",
     "iopub.status.busy": "2026-01-14T13:15:26.861352Z",
     "iopub.status.idle": "2026-01-14T13:15:26.866263Z",
     "shell.execute_reply": "2026-01-14T13:15:26.865643Z",
     "shell.execute_reply.started": "2026-01-14T13:15:26.861503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Models directory: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/data/models\n",
      "SD3 Medium TensorRT path: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/data/models/stable-diffusion-3-medium-tensorrt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Image as IPImage\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create models directory\n",
    "current_dir = Path.cwd()\n",
    "MODELS_DIR = current_dir.parent / \"data/models\"\n",
    "SD3_MODEL_PATH = os.path.join(MODELS_DIR, \"stable-diffusion-3-medium-tensorrt\")\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "print(f\"Models directory: {os.path.abspath(MODELS_DIR)}\")\n",
    "print(f\"SD3 Medium TensorRT path: {os.path.abspath(SD3_MODEL_PATH)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Stable Diffusion 3 Medium TensorRT from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:15:28.906539Z",
     "iopub.status.busy": "2026-01-14T13:15:28.906403Z",
     "iopub.status.idle": "2026-01-14T13:15:29.040258Z",
     "shell.execute_reply": "2026-01-14T13:15:29.039693Z",
     "shell.execute_reply.started": "2026-01-14T13:15:28.906526Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for HF token at: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/misc/credentials/hf.txt\n",
      "✓ Logged in to Hugging Face\n"
     ]
    }
   ],
   "source": [
    "# Load Hugging Face token from file\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the token file path\n",
    "current_dir = Path.cwd()\n",
    "token_file = current_dir.parent / \"misc/credentials/hf.txt\"\n",
    "\n",
    "print(f\"Looking for HF token at: {token_file}\")\n",
    "\n",
    "if token_file.exists():\n",
    "    with open(token_file, 'r') as f:\n",
    "        hf_token = f.read().strip()\n",
    "    \n",
    "    # Set the token as an environment variable\n",
    "    os.environ['HF_TOKEN'] = hf_token\n",
    "    \n",
    "    # Also login using huggingface_hub\n",
    "    from huggingface_hub import login\n",
    "    login(token=hf_token)\n",
    "    print(\"✓ Logged in to Hugging Face\")\n",
    "else:\n",
    "    print(\"⚠️ No HF token found - you may need to authenticate manually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:28:35.497203Z",
     "iopub.status.busy": "2026-01-14T13:28:35.496990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Stable Diffusion 3 Medium from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b517f2f16e1d45ae8d49a5979d505bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/706 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66de416044554c25975e61828027c210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18faccd1761d4b998e353ee844df4dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/740 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4104bd3a39d749eab2798b7ed36c79ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfee24816e744af08c02fae6ed3b3396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder_3/model-00002-of-00002.safe(…):   0%|          | 0.00/4.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2592cccc9c041cda7e1c61f1d9828c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/model.safetensors:   0%|          | 0.00/247M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99b13b6d1e14d2682498d53136fc538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder_2/model.safetensors:   0%|          | 0.00/1.39G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96dac096ac04b5c98b0b41d2b278228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/574 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78538d1504d6406a967cebd5cac8d231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84535336f79844cdb35f801d42907cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder_3/model-00001-of-00002.safe(…):   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597bf50c725c434b986fdd13f90312cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5e36976b944af4a7cf2ecf19da887d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/19.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdb97cf6de04508a1d2cba3014bad68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/705 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033529be633e4e8e9631845ed016b063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/588 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e9880c6d6441f59a76ad7813443f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6abd5fdcf747bf8c3e2fe5a35d46ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/856 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfd1345f41e483389b5b0defcf6f0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/576 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05bbc83cc34e4f62977183cb663885df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_3/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24e9f67b45f4ff79e33f421d2858ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01732caa13284a7babc3378f930f592c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/20.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e389e30b36343a8893430a925ba1952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b4ba83cb1a46aa8d5989d13db69832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/372 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d93f31312c45b99e88e85ce781c898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transformer/diffusion_pytorch_model.safe(…):   0%|          | 0.00/4.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7d6f60d474444cb8f7cf93be2977f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/739 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e441567ba9444facf31e6344c873dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/diffusion_pytorch_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Stable Diffusion 3 Medium (standard version)\n",
    "try:\n",
    "    MODEL_ID = \"stabilityai/stable-diffusion-3-medium-diffusers\"\n",
    "    \n",
    "    if not os.path.exists(SD3_MODEL_PATH):\n",
    "        print(\"Downloading Stable Diffusion 3 Medium from Hugging Face...\")\n",
    "        sd_pipe = StableDiffusion3Pipeline.from_pretrained(\n",
    "            MODEL_ID,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        sd_pipe.save_pretrained(SD3_MODEL_PATH)\n",
    "        print(f\"✓ Model downloaded and saved to {SD3_MODEL_PATH}\")\n",
    "    else:\n",
    "        print(\"Loading Stable Diffusion 3 Medium from local path...\")\n",
    "        sd_pipe = StableDiffusion3Pipeline.from_pretrained(\n",
    "            SD3_MODEL_PATH,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            local_files_only=True\n",
    "        )\n",
    "    \n",
    "    sd_pipe = sd_pipe.to(device)\n",
    "    print(\"✓ Stable Diffusion 3 Medium loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading SD3 Medium: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings Interface\n",
    "\n",
    "Select **positive** and **negative** embeddings for both T5 and CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directories\n",
    "T5_EMBEDDINGS_DIR = current_dir.parent / \"data/embeddings/T5\"\n",
    "CLIP_EMBEDDINGS_DIR = current_dir.parent / \"data/embeddings/CLIP\"\n",
    "\n",
    "# Global variables for loaded embeddings\n",
    "loaded_t5_pos_embedding = None\n",
    "loaded_t5_neg_embedding = None\n",
    "loaded_clip_pos_embedding = None\n",
    "loaded_clip_neg_embedding = None\n",
    "\n",
    "loaded_t5_pos_prompt = None\n",
    "loaded_t5_neg_prompt = None\n",
    "loaded_clip_pos_prompt = None\n",
    "loaded_clip_neg_prompt = None\n",
    "\n",
    "# Get available embedding files\n",
    "t5_files = []\n",
    "clip_files = []\n",
    "\n",
    "if T5_EMBEDDINGS_DIR.exists():\n",
    "    t5_files = sorted([f.name for f in T5_EMBEDDINGS_DIR.glob('*.json')])\n",
    "\n",
    "if CLIP_EMBEDDINGS_DIR.exists():\n",
    "    clip_files = sorted([f.name for f in CLIP_EMBEDDINGS_DIR.glob('*.json')])\n",
    "\n",
    "# Add 'None' option for negative embeddings\n",
    "t5_files_with_none = ['(None)'] + t5_files\n",
    "clip_files_with_none = ['(None)'] + clip_files\n",
    "\n",
    "print(f\"Found {len(t5_files)} T5 embeddings\")\n",
    "print(f\"Found {len(clip_files)} CLIP embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5 Embeddings Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5 POSITIVE embedding selection\n",
    "t5_pos_dropdown = widgets.Dropdown(\n",
    "    options=t5_files,\n",
    "    description='T5 Positive:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='600px')\n",
    ")\n",
    "\n",
    "load_t5_pos_button = widgets.Button(\n",
    "    description='Load T5 Positive',\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "t5_pos_output = widgets.Output()\n",
    "\n",
    "# T5 NEGATIVE embedding selection\n",
    "t5_neg_dropdown = widgets.Dropdown(\n",
    "    options=t5_files_with_none,\n",
    "    description='T5 Negative:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='600px')\n",
    ")\n",
    "\n",
    "load_t5_neg_button = widgets.Button(\n",
    "    description='Load T5 Negative',\n",
    "    button_style='warning'\n",
    ")\n",
    "\n",
    "t5_neg_output = widgets.Output()\n",
    "\n",
    "def load_t5_pos_embedding(b):\n",
    "    global loaded_t5_pos_embedding, loaded_t5_pos_prompt\n",
    "    \n",
    "    with t5_pos_output:\n",
    "        t5_pos_output.clear_output()\n",
    "        \n",
    "        filename = t5_pos_dropdown.value\n",
    "        if not filename:\n",
    "            print(\"❌ No file selected!\")\n",
    "            return\n",
    "        \n",
    "        filepath = T5_EMBEDDINGS_DIR / filename\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            loaded_t5_pos_embedding = np.array(data['embedding'])\n",
    "            loaded_t5_pos_prompt = data.get('prompt', 'Unknown')\n",
    "            \n",
    "            print(f\"✓ Loaded T5 POSITIVE embedding!\")\n",
    "            print(f\"  File: {filename}\")\n",
    "            print(f\"  Prompt: '{loaded_t5_pos_prompt}'\")\n",
    "            print(f\"  Shape: {loaded_t5_pos_embedding.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading T5 positive embedding: {e}\")\n",
    "\n",
    "def load_t5_neg_embedding(b):\n",
    "    global loaded_t5_neg_embedding, loaded_t5_neg_prompt\n",
    "    \n",
    "    with t5_neg_output:\n",
    "        t5_neg_output.clear_output()\n",
    "        \n",
    "        filename = t5_neg_dropdown.value\n",
    "        if not filename or filename == '(None)':\n",
    "            loaded_t5_neg_embedding = None\n",
    "            loaded_t5_neg_prompt = None\n",
    "            print(\"✓ No negative T5 embedding (will use default)\")\n",
    "            return\n",
    "        \n",
    "        filepath = T5_EMBEDDINGS_DIR / filename\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            loaded_t5_neg_embedding = np.array(data['embedding'])\n",
    "            loaded_t5_neg_prompt = data.get('prompt', 'Unknown')\n",
    "            \n",
    "            print(f\"✓ Loaded T5 NEGATIVE embedding!\")\n",
    "            print(f\"  File: {filename}\")\n",
    "            print(f\"  Prompt: '{loaded_t5_neg_prompt}'\")\n",
    "            print(f\"  Shape: {loaded_t5_neg_embedding.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading T5 negative embedding: {e}\")\n",
    "\n",
    "load_t5_pos_button.on_click(load_t5_pos_embedding)\n",
    "load_t5_neg_button.on_click(load_t5_neg_embedding)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>1. T5 Embeddings</h3>\"),\n",
    "    widgets.HTML(\"<b>Positive Embedding:</b>\"),\n",
    "    t5_pos_dropdown,\n",
    "    load_t5_pos_button,\n",
    "    t5_pos_output,\n",
    "    widgets.HTML(\"<br><b>Negative Embedding (optional):</b>\"),\n",
    "    t5_neg_dropdown,\n",
    "    load_t5_neg_button,\n",
    "    t5_neg_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP Embeddings Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP POSITIVE embedding selection\n",
    "clip_pos_dropdown = widgets.Dropdown(\n",
    "    options=clip_files,\n",
    "    description='CLIP Positive:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='600px')\n",
    ")\n",
    "\n",
    "load_clip_pos_button = widgets.Button(\n",
    "    description='Load CLIP Positive',\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "clip_pos_output = widgets.Output()\n",
    "\n",
    "# CLIP NEGATIVE embedding selection\n",
    "clip_neg_dropdown = widgets.Dropdown(\n",
    "    options=clip_files_with_none,\n",
    "    description='CLIP Negative:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='600px')\n",
    ")\n",
    "\n",
    "load_clip_neg_button = widgets.Button(\n",
    "    description='Load CLIP Negative',\n",
    "    button_style='warning'\n",
    ")\n",
    "\n",
    "clip_neg_output = widgets.Output()\n",
    "\n",
    "def load_clip_pos_embedding(b):\n",
    "    global loaded_clip_pos_embedding, loaded_clip_pos_prompt\n",
    "    \n",
    "    with clip_pos_output:\n",
    "        clip_pos_output.clear_output()\n",
    "        \n",
    "        filename = clip_pos_dropdown.value\n",
    "        if not filename:\n",
    "            print(\"❌ No file selected!\")\n",
    "            return\n",
    "        \n",
    "        filepath = CLIP_EMBEDDINGS_DIR / filename\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            loaded_clip_pos_embedding = np.array(data['embedding'])\n",
    "            loaded_clip_pos_prompt = data.get('prompt', 'Unknown')\n",
    "            \n",
    "            print(f\"✓ Loaded CLIP POSITIVE embedding!\")\n",
    "            print(f\"  File: {filename}\")\n",
    "            print(f\"  Prompt: '{loaded_clip_pos_prompt}'\")\n",
    "            print(f\"  Shape: {loaded_clip_pos_embedding.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading CLIP positive embedding: {e}\")\n",
    "\n",
    "def load_clip_neg_embedding(b):\n",
    "    global loaded_clip_neg_embedding, loaded_clip_neg_prompt\n",
    "    \n",
    "    with clip_neg_output:\n",
    "        clip_neg_output.clear_output()\n",
    "        \n",
    "        filename = clip_neg_dropdown.value\n",
    "        if not filename or filename == '(None)':\n",
    "            loaded_clip_neg_embedding = None\n",
    "            loaded_clip_neg_prompt = None\n",
    "            print(\"✓ No negative CLIP embedding (will use default)\")\n",
    "            return\n",
    "        \n",
    "        filepath = CLIP_EMBEDDINGS_DIR / filename\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            loaded_clip_neg_embedding = np.array(data['embedding'])\n",
    "            loaded_clip_neg_prompt = data.get('prompt', 'Unknown')\n",
    "            \n",
    "            print(f\"✓ Loaded CLIP NEGATIVE embedding!\")\n",
    "            print(f\"  File: {filename}\")\n",
    "            print(f\"  Prompt: '{loaded_clip_neg_prompt}'\")\n",
    "            print(f\"  Shape: {loaded_clip_neg_embedding.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading CLIP negative embedding: {e}\")\n",
    "\n",
    "load_clip_pos_button.on_click(load_clip_pos_embedding)\n",
    "load_clip_neg_button.on_click(load_clip_neg_embedding)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    widgets.HTML(\"<h3>2. CLIP Embeddings</h3>\"),\n",
    "    widgets.HTML(\"<b>Positive Embedding:</b>\"),\n",
    "    clip_pos_dropdown,\n",
    "    load_clip_pos_button,\n",
    "    clip_pos_output,\n",
    "    widgets.HTML(\"<br><b>Negative Embedding (optional):</b>\"),\n",
    "    clip_neg_dropdown,\n",
    "    load_clip_neg_button,\n",
    "    clip_neg_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Image with Positive and Negative Embeddings\n",
    "\n",
    "Control guidance scale to balance between positive and negative embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup output directory\n",
    "OUTPUT_IMAGES_DIR = current_dir.parent / \"output/images/SD3\"\n",
    "os.makedirs(OUTPUT_IMAGES_DIR, exist_ok=True)\n",
    "\n",
    "def parse_embedding_filename(filename):\n",
    "    \"\"\"\n",
    "    Parse embedding filename to extract tokens and manipulation.\n",
    "    Returns (tokens_string, manipulation_string)\n",
    "    \"\"\"\n",
    "    if not filename or filename == '(None)':\n",
    "        return ('none', '')\n",
    "    \n",
    "    # Remove .json extension\n",
    "    base_name = filename.rsplit('.', 1)[0]\n",
    "    \n",
    "    # Split by underscore\n",
    "    parts = base_name.split('_')\n",
    "    \n",
    "    # First 4 parts are the tokens - join without underscores\n",
    "    if len(parts) <= 4:\n",
    "        tokens = ''.join(parts)\n",
    "        return (tokens, '')\n",
    "    \n",
    "    tokens = ''.join(parts[:4])\n",
    "    \n",
    "    # Get manipulation type (simplified)\n",
    "    manipulation_parts = parts[4:]\n",
    "    if manipulation_parts:\n",
    "        manipulation = manipulation_parts[0]\n",
    "    else:\n",
    "        manipulation = ''\n",
    "    \n",
    "    return (tokens, manipulation)\n",
    "\n",
    "def generate_from_loaded_embeddings(seed=42, guidance_scale=7.0):\n",
    "    \"\"\"\n",
    "    Generate image using loaded positive and negative T5/CLIP embeddings.\n",
    "    \"\"\"\n",
    "    if 'sd_pipe' not in globals():\n",
    "        print(\"❌ SD3 pipeline not loaded!\")\n",
    "        return None\n",
    "    \n",
    "    if loaded_t5_pos_embedding is None:\n",
    "        print(\"❌ No positive T5 embedding loaded! Load a T5 positive embedding first.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Generating image from loaded embeddings...\")\n",
    "    print(f\"  Guidance scale: {guidance_scale}\")\n",
    "    print()\n",
    "    \n",
    "    # Process POSITIVE T5 embedding\n",
    "    print(\"POSITIVE T5:\")\n",
    "    print(f\"  Shape: {loaded_t5_pos_embedding.shape}\")\n",
    "    print(f\"  Prompt: '{loaded_t5_pos_prompt}'\")\n",
    "    \n",
    "    t5_pos_tensor = torch.from_numpy(loaded_t5_pos_embedding.astype(np.float32)).to(\n",
    "        device=device,\n",
    "        dtype=torch.bfloat16\n",
    "    ).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    # Process NEGATIVE T5 embedding\n",
    "    if loaded_t5_neg_embedding is not None:\n",
    "        print(\"\\nNEGATIVE T5:\")\n",
    "        print(f\"  Shape: {loaded_t5_neg_embedding.shape}\")\n",
    "        print(f\"  Prompt: '{loaded_t5_neg_prompt}'\")\n",
    "        \n",
    "        t5_neg_tensor = torch.from_numpy(loaded_t5_neg_embedding.astype(np.float32)).to(\n",
    "            device=device,\n",
    "            dtype=torch.bfloat16\n",
    "        ).unsqueeze(0)\n",
    "    else:\n",
    "        print(\"\\nNEGATIVE T5: Using default (empty)\")\n",
    "        t5_neg_tensor = None\n",
    "    \n",
    "    # Process POSITIVE CLIP embedding\n",
    "    print(\"\\nPOSITIVE CLIP:\")\n",
    "    if loaded_clip_pos_embedding is not None:\n",
    "        print(f\"  Shape: {loaded_clip_pos_embedding.shape}\")\n",
    "        print(f\"  Prompt: '{loaded_clip_pos_prompt}'\")\n",
    "        \n",
    "        # Use last token embedding as pooled embedding (EOS token)\n",
    "        clip_pos_pooled = torch.from_numpy(\n",
    "            loaded_clip_pos_embedding[-1:].astype(np.float32)\n",
    "        ).to(device=device, dtype=torch.bfloat16)\n",
    "    else:\n",
    "        print(\"  Generating from T5 prompt using CLIP model...\")\n",
    "        # Generate default CLIP embedding from prompt\n",
    "        with torch.no_grad():\n",
    "            _, clip_pos_pooled, _ = sd_pipe.encode_prompt(\n",
    "                prompt=loaded_t5_pos_prompt,\n",
    "                prompt_2=None,\n",
    "                prompt_3=None,\n",
    "                device=device,\n",
    "                num_images_per_prompt=1,\n",
    "            )\n",
    "    \n",
    "    # Process NEGATIVE CLIP embedding\n",
    "    if loaded_clip_neg_embedding is not None:\n",
    "        print(\"\\nNEGATIVE CLIP:\")\n",
    "        print(f\"  Shape: {loaded_clip_neg_embedding.shape}\")\n",
    "        print(f\"  Prompt: '{loaded_clip_neg_prompt}'\")\n",
    "        \n",
    "        clip_neg_pooled = torch.from_numpy(\n",
    "            loaded_clip_neg_embedding[-1:].astype(np.float32)\n",
    "        ).to(device=device, dtype=torch.bfloat16)\n",
    "    else:\n",
    "        print(\"\\nNEGATIVE CLIP: Using default (empty)\")\n",
    "        clip_neg_pooled = None\n",
    "    \n",
    "    # Construct filename\n",
    "    t5_pos_tokens, t5_pos_manip = parse_embedding_filename(t5_pos_dropdown.value)\n",
    "    t5_neg_tokens, t5_neg_manip = parse_embedding_filename(t5_neg_dropdown.value)\n",
    "    clip_pos_tokens, clip_pos_manip = parse_embedding_filename(clip_pos_dropdown.value)\n",
    "    clip_neg_tokens, clip_neg_manip = parse_embedding_filename(clip_neg_dropdown.value)\n",
    "    \n",
    "    # Build filename: t5pos_t5neg_clippos_clipneg_cfg{guidance}.png\n",
    "    filename_parts = []\n",
    "    filename_parts.append(f\"t5pos_{t5_pos_tokens}{('_'+t5_pos_manip) if t5_pos_manip else ''}\")\n",
    "    if t5_neg_tokens != 'none':\n",
    "        filename_parts.append(f\"t5neg_{t5_neg_tokens}{('_'+t5_neg_manip) if t5_neg_manip else ''}\")\n",
    "    filename_parts.append(f\"clippos_{clip_pos_tokens}{('_'+clip_pos_manip) if clip_pos_manip else ''}\")\n",
    "    if clip_neg_tokens != 'none':\n",
    "        filename_parts.append(f\"clipneg_{clip_neg_tokens}{('_'+clip_neg_manip) if clip_neg_manip else ''}\")\n",
    "    filename_parts.append(f\"cfg{guidance_scale:.1f}\")\n",
    "    \n",
    "    filename = '_'.join(filename_parts) + '.png'\n",
    "    output_filepath = OUTPUT_IMAGES_DIR / filename\n",
    "    \n",
    "    # Generate image\n",
    "    try:\n",
    "        print(f\"\\nRunning SD3 Medium TensorRT diffusion (28 steps)...\")\n",
    "        image = sd_pipe(\n",
    "            prompt_embeds=t5_pos_tensor,\n",
    "            negative_prompt_embeds=t5_neg_tensor,\n",
    "            pooled_prompt_embeds=clip_pos_pooled,\n",
    "            negative_pooled_prompt_embeds=clip_neg_pooled,\n",
    "            num_inference_steps=28,\n",
    "            guidance_scale=guidance_scale,\n",
    "            height=1024,\n",
    "            width=1024,\n",
    "            generator=torch.manual_seed(seed)\n",
    "        ).images[0]\n",
    "        \n",
    "        image.save(output_filepath)\n",
    "        print(f\"\\n✓ Image generated and saved!\")\n",
    "        print(f\"  Path: {output_filepath}\")\n",
    "        print(f\"  Filename: {filename}\")\n",
    "        \n",
    "        return image\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating image: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Generation controls\n",
    "seed_input = widgets.IntText(\n",
    "    value=42,\n",
    "    description='Seed:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "guidance_input = widgets.FloatSlider(\n",
    "    value=7.0,\n",
    "    min=0.0,\n",
    "    max=20.0,\n",
    "    step=0.5,\n",
    "    description='Guidance Scale:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='500px'),\n",
    "    readout_format='.1f'\n",
    ")\n",
    "\n",
    "generate_button = widgets.Button(\n",
    "    description='Generate Image',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='300px', height='50px')\n",
    ")\n",
    "\n",
    "generation_output = widgets.Output()\n",
    "\n",
    "def on_generate_click(b):\n",
    "    with generation_output:\n",
    "        generation_output.clear_output(wait=True)\n",
    "        \n",
    "        image = generate_from_loaded_embeddings(\n",
    "            seed=seed_input.value,\n",
    "            guidance_scale=guidance_input.value\n",
    "        )\n",
    "        \n",
    "        if image:\n",
    "            display(image)\n",
    "\n",
    "generate_button.on_click(on_generate_click)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    widgets.HTML(\"<h3>3. Generate Image</h3>\"),\n",
    "    widgets.HTML(\"<p><b>Guidance Scale:</b> Higher values (7-15) follow the positive prompt more closely and avoid the negative prompt. Lower values (1-5) give more creative freedom.</p>\"),\n",
    "    seed_input,\n",
    "    guidance_input,\n",
    "    generate_button\n",
    "]), generation_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Differences from FLUX:\n",
    "\n",
    "1. **Negative Embeddings**: SD3 supports negative T5 and CLIP embeddings to steer generation away from unwanted concepts\n",
    "2. **Guidance Scale**: Control how strongly the model follows positive vs negative prompts (FLUX.1-schnell doesn't use guidance)\n",
    "3. **More Inference Steps**: SD3 uses 28 steps by default vs FLUX's 4 steps\n",
    "4. **Different Architecture**: SD3 uses MMDiT (Multimodal Diffusion Transformer) with CFG, FLUX uses rectified flow\n",
    "5. **TensorRT Optimization**: This model is optimized with TensorRT for faster inference on NVIDIA GPUs\n",
    "\n",
    "### Workflow:\n",
    "\n",
    "1. Load **positive** T5 embedding (required)\n",
    "2. Load **negative** T5 embedding (optional)\n",
    "3. Load **positive** CLIP embedding (required or auto-generated)\n",
    "4. Load **negative** CLIP embedding (optional)\n",
    "5. Adjust **guidance scale** (7.0 is default, 10-15 for stronger adherence)\n",
    "6. Set **seed** for reproducibility\n",
    "7. Generate and compare results!\n",
    "\n",
    "### Experiment Ideas:\n",
    "\n",
    "- Use manipulated embeddings (zeroed, scaled, inverted) as negative prompts\n",
    "- Compare same embeddings with different guidance scales\n",
    "- Mix and match: positive normal + negative zeroed\n",
    "- Compare SD3 Medium TensorRT vs FLUX results side-by-side\n",
    "- Test inference speed improvements from TensorRT optimization\n",
    "\n",
    "### About SD3 Medium TensorRT:\n",
    "\n",
    "- **Optimized for NVIDIA GPUs**: Uses TensorRT engine for faster inference\n",
    "- **Medium size**: Smaller than SD3 Large but still high quality\n",
    "- **Same embedding structure**: Compatible with your T5/CLIP embedding workflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flux uv",
   "language": "python",
   "name": "uv_flux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
