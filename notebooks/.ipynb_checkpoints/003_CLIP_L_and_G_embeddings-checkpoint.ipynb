{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP-L and CLIP-G Embeddings and Ornithology for SD3 ü¶öü¶úüê¶\n",
    "\n",
    "SD3 uses **two** CLIP models that get concatenated:\n",
    "- **CLIP-L** (OpenAI): 768-dim pooled embedding\n",
    "- **CLIP-G** (OpenCLIP): 1280-dim pooled embedding\n",
    "- **Combined**: 2048-dim pooled embedding for SD3\n",
    "\n",
    "This notebook generates both and saves them together for use with SD3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T15:12:14.278851Z",
     "iopub.status.busy": "2026-01-14T15:12:14.278687Z",
     "iopub.status.idle": "2026-01-14T15:12:14.283163Z",
     "shell.execute_reply": "2026-01-14T15:12:14.282548Z",
     "shell.execute_reply.started": "2026-01-14T15:12:14.278837Z"
    }
   },
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "    T[Text Prompt]\n",
    "\n",
    "    CLIPL[CLIP-L Encoder]\n",
    "    CLIPG[CLIP-G Encoder]\n",
    "\n",
    "    PL[Pooled CLIP-L embedding]\n",
    "    PG[Pooled CLIP-G embedding]\n",
    "\n",
    "    F[Fusion and Projection]\n",
    "\n",
    "    SD35[SD 3.5\\nDiffusion Transformer]\n",
    "\n",
    "    T --> CLIPL --> PL\n",
    "    T --> CLIPG --> PG\n",
    "\n",
    "    PL --> F\n",
    "    PG --> F\n",
    "\n",
    "    F -->|global conditioning| SD35\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T11:16:45.285563Z",
     "iopub.status.busy": "2026-01-15T11:16:45.285320Z",
     "iopub.status.idle": "2026-01-15T11:16:45.294564Z",
     "shell.execute_reply": "2026-01-15T11:16:45.293782Z",
     "shell.execute_reply.started": "2026-01-15T11:16:45.285545Z"
    }
   },
   "source": [
    "# CLIP Pooled Embedding Extraction in SD3.5\n",
    "\n",
    "## Overview\n",
    "\n",
    "SD3.5 uses two CLIP text encoders:\n",
    "- **CLIP-L (OpenCLIP ViT-L)**: Outputs 77√ó768\n",
    "- **CLIP-G (OpenCLIP ViT-G)**: Outputs 77√ó1280\n",
    "\n",
    "The pooled embedding is created by extracting the **EOS token embedding** from each encoder and concatenating them.\n",
    "\n",
    "## The Process\n",
    "\n",
    "### Step 1: Text Encoding\n",
    "Each CLIP encoder processes the tokenized text and outputs a sequence of embeddings:\n",
    "\n",
    "- **CLIP-L**: `[BOS, token1, token2, ..., EOS, PAD, PAD, ...]` ‚Üí **77√ó768**\n",
    "- **CLIP-G**: `[BOS, token1, token2, ..., EOS, PAD, PAD, ...]` ‚Üí **77√ó1280**\n",
    "\n",
    "### Step 2: Pooling (Extract EOS Token)\n",
    "The \"pooled\" representation is simply the embedding at the **EOS (End-of-Sequence) token position**:\n",
    "\n",
    "- **CLIP-L pooled**: Extract position 76 (or last non-padding position) ‚Üí **768-dim vector**\n",
    "- **CLIP-G pooled**: Extract position 76 (or last non-padding position) ‚Üí **1280-dim vector**\n",
    "\n",
    "> **Why the EOS token?** The EOS token acts as a sentence-level summary because it has attended to all previous tokens through the transformer's self-attention mechanism.\n",
    "\n",
    "### Step 3: Concatenation\n",
    "The two pooled vectors are simply concatenated:\n",
    "\n",
    "```\n",
    "pooled_embedding = concat(CLIP-L[EOS], CLIP-G[EOS])\n",
    "                 = concat(768-dim, 1280-dim)\n",
    "                 = 2048-dim vector\n",
    "```\n",
    "\n",
    "This 2048-dimensional vector is what SD3.5 uses as the **pooled text conditioning**.\n",
    "\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Input Text: 'an elephant'] --> B[Tokenization]\n",
    "    B --> C[CLIP-L Encoder]\n",
    "    B --> D[CLIP-G Encoder]\n",
    "    \n",
    "    C --> E[CLIP-L Sequence Output<br/>77 √ó 768]\n",
    "    D --> F[CLIP-G Sequence Output<br/>77 √ó 1280]\n",
    "    \n",
    "    E --> G[Extract EOS Token<br/>Position 76]\n",
    "    F --> H[Extract EOS Token<br/>Position 76]\n",
    "    \n",
    "    G --> I[CLIP-L Pooled<br/>768-dim vector]\n",
    "    H --> J[CLIP-G Pooled<br/>1280-dim vector]\n",
    "    \n",
    "    I --> K[Concatenate]\n",
    "    J --> K\n",
    "    \n",
    "    K --> L[Final Pooled Embedding<br/>2048-dim vector]\n",
    "    \n",
    "    style L stroke:#90EE90\n",
    "    style E stroke:#FFE4B5\n",
    "    style F stroke:#FFE4B5\n",
    "    style I stroke:#87CEEB\n",
    "    style J stroke:#87CEEB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CLIP-L and CLIP-G Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:27:29.480368Z",
     "iopub.status.busy": "2026-01-16T00:27:29.480196Z",
     "iopub.status.idle": "2026-01-16T00:27:29.484413Z",
     "shell.execute_reply": "2026-01-16T00:27:29.483942Z",
     "shell.execute_reply.started": "2026-01-16T00:27:29.480354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "# Load models path from config\n",
    "models_path_file = current_dir.parent / \"misc/paths/models.txt\"\n",
    "with open(models_path_file, 'r') as f:\n",
    "    models_path = f.read().strip()\n",
    "MODELS_DIR = current_dir.parent / models_path\n",
    "\n",
    "CLIP_L_PATH = MODELS_DIR / \"clip-vit-large-patch14\"\n",
    "CLIP_G_PATH = MODELS_DIR / \"clip-vit-large-patch14-336\"  # CLIP-G is the 336px variant\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:27:29.996081Z",
     "iopub.status.busy": "2026-01-16T00:27:29.995877Z",
     "iopub.status.idle": "2026-01-16T00:27:30.195666Z",
     "shell.execute_reply": "2026-01-16T00:27:30.195059Z",
     "shell.execute_reply.started": "2026-01-16T00:27:29.996065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP-L (OpenAI)...\n",
      "  Loading from local...\n",
      "‚úì CLIP-L loaded!\n",
      "  Embedding dimension: 768\n",
      "  Max sequence length: 77\n"
     ]
    }
   ],
   "source": [
    "# Load CLIP-L (OpenAI CLIP - 768 dim)\n",
    "print(\"Loading CLIP-L (OpenAI)...\")\n",
    "\n",
    "if not os.path.exists(CLIP_L_PATH):\n",
    "    print(\"  Downloading CLIP-L from Hugging Face...\")\n",
    "    clip_l_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    clip_l_model = CLIPTextModel.from_pretrained(\n",
    "        \"openai/clip-vit-large-patch14\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    clip_l_tokenizer.save_pretrained(CLIP_L_PATH)\n",
    "    clip_l_model.save_pretrained(CLIP_L_PATH)\n",
    "    print(\"  ‚úì Downloaded and saved\")\n",
    "else:\n",
    "    print(\"  Loading from local...\")\n",
    "\n",
    "clip_l_tokenizer = CLIPTokenizer.from_pretrained(CLIP_L_PATH, local_files_only=True)\n",
    "clip_l_model = CLIPTextModel.from_pretrained(\n",
    "    CLIP_L_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    local_files_only=True\n",
    ").to(device)\n",
    "clip_l_model.eval()\n",
    "\n",
    "print(f\"‚úì CLIP-L loaded!\")\n",
    "print(f\"  Embedding dimension: {clip_l_model.config.hidden_size}\")\n",
    "print(f\"  Max sequence length: {clip_l_tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:27:30.739139Z",
     "iopub.status.busy": "2026-01-16T00:27:30.738976Z",
     "iopub.status.idle": "2026-01-16T00:27:31.444223Z",
     "shell.execute_reply": "2026-01-16T00:27:31.443657Z",
     "shell.execute_reply.started": "2026-01-16T00:27:30.739126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading CLIP-G (OpenCLIP bigG)...\n",
      "  Loading from local...\n",
      "‚úì CLIP-G loaded!\n",
      "  Embedding dimension: 1280\n",
      "  Max sequence length: 77\n"
     ]
    }
   ],
   "source": [
    "# Load CLIP-G (OpenCLIP bigG - 1280 dim)\n",
    "# Note: We'll use laion/CLIP-ViT-bigG-14-laion2B-39B-b160k which is CLIP-G\n",
    "print(\"\\nLoading CLIP-G (OpenCLIP bigG)...\")\n",
    "\n",
    "CLIP_G_PATH = MODELS_DIR / \"CLIP-ViT-bigG-14-laion2B\"\n",
    "\n",
    "if not os.path.exists(CLIP_G_PATH):\n",
    "    print(\"  Downloading CLIP-G from Hugging Face...\")\n",
    "    clip_g_tokenizer = CLIPTokenizer.from_pretrained(\"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\")\n",
    "    clip_g_model = CLIPTextModel.from_pretrained(\n",
    "        \"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    clip_g_tokenizer.save_pretrained(CLIP_G_PATH)\n",
    "    clip_g_model.save_pretrained(CLIP_G_PATH)\n",
    "    print(\"  ‚úì Downloaded and saved\")\n",
    "else:\n",
    "    print(\"  Loading from local...\")\n",
    "\n",
    "clip_g_tokenizer = CLIPTokenizer.from_pretrained(CLIP_G_PATH, local_files_only=True)\n",
    "clip_g_model = CLIPTextModel.from_pretrained(\n",
    "    CLIP_G_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    local_files_only=True\n",
    ").to(device)\n",
    "clip_g_model.eval()\n",
    "\n",
    "print(f\"‚úì CLIP-G loaded!\")\n",
    "print(f\"  Embedding dimension: {clip_g_model.config.hidden_size}\")\n",
    "print(f\"  Max sequence length: {clip_g_tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Combined CLIP Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T23:56:22.692320Z",
     "iopub.status.busy": "2026-01-15T23:56:22.692125Z",
     "iopub.status.idle": "2026-01-15T23:56:22.704058Z",
     "shell.execute_reply": "2026-01-15T23:56:22.703585Z",
     "shell.execute_reply.started": "2026-01-15T23:56:22.692304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2a69137d5a4ca587a7bd0a7b6ad868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='a puffy european robin sitting on a tree branch', description='Prompt:', layout=Layout(height=‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a0b37f7b204ea0840e8fe59f32c848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Generate CLIP-L + CLIP-G Embeddings', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d486f804d0cc4d2eb09cd21566aaf2a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "\n",
    "# Create input widget\n",
    "prompt_input = widgets.Textarea(\n",
    "    value='a puffy european robin sitting on a tree branch',\n",
    "    placeholder='Enter your prompt here',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='80%', height='80px')\n",
    ")\n",
    "\n",
    "generate_button = widgets.Button(\n",
    "    description='Generate CLIP-L + CLIP-G Embeddings',\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Global variables\n",
    "current_clip_l_embedding = None\n",
    "current_clip_g_embedding = None\n",
    "current_combined_pooled = None\n",
    "current_prompt = None\n",
    "\n",
    "def generate_clip_embeddings(b):\n",
    "    global current_clip_l_embedding, current_clip_g_embedding\n",
    "    global current_combined_pooled, current_prompt\n",
    "    \n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        \n",
    "        prompt = prompt_input.value\n",
    "        current_prompt = prompt\n",
    "        \n",
    "        print(f\"Generating embeddings for: '{prompt}'\\n\")\n",
    "        \n",
    "        # Generate CLIP-L embeddings\n",
    "        print(\"=== CLIP-L (OpenAI) ===\")\n",
    "        tokens_l = clip_l_tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tokens_l = {k: v.to(device) for k, v in tokens_l.items()}\n",
    "            outputs_l = clip_l_model(**tokens_l)\n",
    "            \n",
    "            # Get sequence embeddings and pooled embedding\n",
    "            clip_l_hidden = outputs_l.last_hidden_state  # [1, 77, 768]\n",
    "            clip_l_pooled = outputs_l.pooler_output      # [1, 768]\n",
    "        \n",
    "        current_clip_l_embedding = clip_l_hidden.float().cpu().numpy()[0]  # [77, 768]\n",
    "        clip_l_pooled_np = clip_l_pooled.float().cpu().numpy()[0]          # [768]\n",
    "        \n",
    "        print(f\"  Sequence shape: {current_clip_l_embedding.shape} (77 tokens √ó 768 dims)\")\n",
    "        print(f\"  Pooled shape: {clip_l_pooled_np.shape} (768 dims)\")\n",
    "        \n",
    "        # Generate CLIP-G embeddings\n",
    "        print(\"\\n=== CLIP-G (OpenCLIP bigG) ===\")\n",
    "        tokens_g = clip_g_tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tokens_g = {k: v.to(device) for k, v in tokens_g.items()}\n",
    "            outputs_g = clip_g_model(**tokens_g)\n",
    "            \n",
    "            # Get sequence embeddings and pooled embedding\n",
    "            clip_g_hidden = outputs_g.last_hidden_state  # [1, 77, 1280]\n",
    "            clip_g_pooled = outputs_g.pooler_output      # [1, 1280]\n",
    "        \n",
    "        current_clip_g_embedding = clip_g_hidden.float().cpu().numpy()[0]  # [77, 1280]\n",
    "        clip_g_pooled_np = clip_g_pooled.float().cpu().numpy()[0]          # [1280]\n",
    "        \n",
    "        print(f\"  Sequence shape: {current_clip_g_embedding.shape} (77 tokens √ó 1280 dims)\")\n",
    "        print(f\"  Pooled shape: {clip_g_pooled_np.shape} (1280 dims)\")\n",
    "        \n",
    "        # Concatenate pooled embeddings for SD3\n",
    "        print(\"\\n=== Combined for SD3 ===\")\n",
    "        current_combined_pooled = np.concatenate([clip_l_pooled_np, clip_g_pooled_np])\n",
    "        print(f\"  Combined pooled shape: {current_combined_pooled.shape} (768 + 1280 = 2048 dims)\")\n",
    "        print(f\"  ‚úì Ready for SD3!\")\n",
    "        \n",
    "        print(f\"\\nFirst 10 values of combined pooled embedding:\")\n",
    "        print(current_combined_pooled[:10])\n",
    "\n",
    "generate_button.on_click(generate_clip_embeddings)\n",
    "display(prompt_input, generate_button, output_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Combined CLIP Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T23:56:25.911851Z",
     "iopub.status.busy": "2026-01-15T23:56:25.911658Z",
     "iopub.status.idle": "2026-01-15T23:56:25.921125Z",
     "shell.execute_reply": "2026-01-15T23:56:25.920599Z",
     "shell.execute_reply.started": "2026-01-15T23:56:25.911836Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda0f6008efc4d6d9a10fd4017bc564c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Save CLIP Embeddings', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b5b40cf4c94d79b5f3ab2e210db31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define embeddings directory\n",
    "CLIP_COMBINED_DIR = current_dir.parent / \"data/embeddings/CLIP_SD3\"\n",
    "os.makedirs(CLIP_COMBINED_DIR, exist_ok=True)\n",
    "\n",
    "save_button = widgets.Button(\n",
    "    description='Save CLIP Embeddings',\n",
    "    button_style='primary'\n",
    ")\n",
    "\n",
    "save_output = widgets.Output()\n",
    "\n",
    "def save_clip_embeddings(b):\n",
    "    with save_output:\n",
    "        save_output.clear_output()\n",
    "        \n",
    "        if current_combined_pooled is None:\n",
    "            print(\"‚ùå No embeddings to save! Generate embeddings first.\")\n",
    "            return\n",
    "        \n",
    "        # Get first 4 tokens from CLIP-L for filename\n",
    "        tokens_l = clip_l_tokenizer(\n",
    "            current_prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        token_ids = tokens_l['input_ids'][0].tolist()\n",
    "        token_strings = [clip_l_tokenizer.decode([tid]) for tid in token_ids]\n",
    "        \n",
    "        # Get first 4 real tokens\n",
    "        filename_tokens = []\n",
    "        for token in token_strings:\n",
    "            cleaned = token.strip().replace('</w>', '').replace('<|startoftext|>', '').replace('<|endoftext|>', '')\n",
    "            if cleaned and cleaned not in ['<|startoftext|>', '<|endoftext|>', '']:\n",
    "                filename_tokens.append(cleaned)\n",
    "            if len(filename_tokens) >= 4:\n",
    "                break\n",
    "        \n",
    "        filename = \"_\".join(filename_tokens) + \".json\"\n",
    "        filepath = CLIP_COMBINED_DIR / filename\n",
    "        \n",
    "        # Save all embeddings\n",
    "        embedding_data = {\n",
    "            \"prompt\": current_prompt,\n",
    "            \"clip_l_sequence\": current_clip_l_embedding.tolist(),\n",
    "            \"clip_g_sequence\": current_clip_g_embedding.tolist(),\n",
    "            \"combined_pooled\": current_combined_pooled.tolist(),\n",
    "            \"shapes\": {\n",
    "                \"clip_l_sequence\": list(current_clip_l_embedding.shape),\n",
    "                \"clip_g_sequence\": list(current_clip_g_embedding.shape),\n",
    "                \"combined_pooled\": list(current_combined_pooled.shape)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(embedding_data, f)\n",
    "        \n",
    "        print(f\"‚úì CLIP embeddings saved!\")\n",
    "        print(f\"  File: {filepath}\")\n",
    "        print(f\"  Size: {os.path.getsize(filepath) / 1024:.2f} KB\")\n",
    "        print(f\"\\nContains:\")\n",
    "        print(f\"  - CLIP-L sequence: [77, 768]\")\n",
    "        print(f\"  - CLIP-G sequence: [77, 1280]\")\n",
    "        print(f\"  - Combined pooled: [2048] (for SD3)\")\n",
    "\n",
    "save_button.on_click(save_clip_embeddings)\n",
    "display(save_button, save_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generation from Text Input\n",
    "\n",
    "Enter multiple prompts (one per line) to generate and save embeddings for all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:20:05.724109Z",
     "iopub.status.busy": "2026-01-16T00:20:05.723908Z",
     "iopub.status.idle": "2026-01-16T00:20:05.738987Z",
     "shell.execute_reply": "2026-01-16T00:20:05.738537Z",
     "shell.execute_reply.started": "2026-01-16T00:20:05.724095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch CLIP-L + CLIP-G Embedding Generator\n",
      "Output directory: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/data/embeddings/CLIP_SD3\n",
      "Enter prompts (one per line):\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe38fe2d3bd44e29374cb6c863dc871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='A curious Raggiana bird-of-paradise peeking through dense green leaves.\\nA magnificent riflebi‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c75bd66b234f64ab020db16fa2a4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='warning', description='Batch Generate & Save', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bb2224f3ac4614a3c460a4cfbf16f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Batch generate CLIP-L + CLIP-G embeddings from text input\n",
    "batch_prompt_input = widgets.Textarea(\n",
    "    value='A curious Raggiana bird-of-paradise peeking through dense green leaves.\\nA magnificent riflebird with iridescent feathers perched on a mossy log.\\nA vibrant King of Saxony bird-of-paradise showing off its long head plumes.\\nA stunning Superb bird-of-paradise doing a courtship dance on the forest floor.',\n",
    "    placeholder='Enter prompts, one per line',\n",
    "    description='Prompts:',\n",
    "    layout=widgets.Layout(width='80%', height='150px')\n",
    ")\n",
    "\n",
    "batch_generate_button = widgets.Button(\n",
    "    description='Batch Generate & Save',\n",
    "    button_style='warning'\n",
    ")\n",
    "\n",
    "batch_output_area = widgets.Output()\n",
    "\n",
    "def generate_combined_embedding(prompt):\n",
    "    \"\"\"Generate CLIP-L + CLIP-G embedding for a single prompt.\"\"\"\n",
    "    # CLIP-L\n",
    "    tokens_l = clip_l_tokenizer(prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        tokens_l = {k: v.to(device) for k, v in tokens_l.items()}\n",
    "        outputs_l = clip_l_model(**tokens_l)\n",
    "        clip_l_hidden = outputs_l.last_hidden_state.float().cpu().numpy()[0]\n",
    "        clip_l_pooled = outputs_l.pooler_output.float().cpu().numpy()[0]\n",
    "    \n",
    "    # CLIP-G\n",
    "    tokens_g = clip_g_tokenizer(prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        tokens_g = {k: v.to(device) for k, v in tokens_g.items()}\n",
    "        outputs_g = clip_g_model(**tokens_g)\n",
    "        clip_g_hidden = outputs_g.last_hidden_state.float().cpu().numpy()[0]\n",
    "        clip_g_pooled = outputs_g.pooler_output.float().cpu().numpy()[0]\n",
    "    \n",
    "    combined_pooled = np.concatenate([clip_l_pooled, clip_g_pooled])\n",
    "    \n",
    "    return clip_l_hidden, clip_g_hidden, combined_pooled\n",
    "\n",
    "def get_filename_tokens(prompt):\n",
    "    \"\"\"Get first 4 tokens for filename.\"\"\"\n",
    "    tokens_l = clip_l_tokenizer(prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\")\n",
    "    token_ids = tokens_l['input_ids'][0].tolist()\n",
    "    token_strings = [clip_l_tokenizer.decode([tid]) for tid in token_ids]\n",
    "    \n",
    "    filename_tokens = []\n",
    "    for token in token_strings:\n",
    "        cleaned = token.strip().replace('</w>', '').replace('<|startoftext|>', '').replace('<|endoftext|>', '')\n",
    "        if cleaned and cleaned not in ['<|startoftext|>', '<|endoftext|>', '']:\n",
    "            filename_tokens.append(cleaned)\n",
    "        if len(filename_tokens) >= 4:\n",
    "            break\n",
    "    return filename_tokens\n",
    "\n",
    "def batch_generate_from_text(b):\n",
    "    with batch_output_area:\n",
    "        batch_output_area.clear_output()\n",
    "        \n",
    "        prompts = [p.strip() for p in batch_prompt_input.value.strip().split('\\n') if p.strip()]\n",
    "        \n",
    "        if not prompts:\n",
    "            print(\"No prompts provided!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Generating {len(prompts)} CLIP-L + CLIP-G embeddings...\\n\")\n",
    "        \n",
    "        for i, prompt in enumerate(prompts, 1):\n",
    "            print(f\"[{i}/{len(prompts)}] '{prompt[:50]}{'...' if len(prompt) > 50 else ''}'\")\n",
    "            \n",
    "            clip_l_hidden, clip_g_hidden, combined_pooled = generate_combined_embedding(prompt)\n",
    "            filename_tokens = get_filename_tokens(prompt)\n",
    "            \n",
    "            filename = \"_\".join(filename_tokens) + \".json\"\n",
    "            filepath = CLIP_COMBINED_DIR / filename\n",
    "            \n",
    "            embedding_data = {\n",
    "                \"prompt\": prompt,\n",
    "                \"clip_l_sequence\": clip_l_hidden.tolist(),\n",
    "                \"clip_g_sequence\": clip_g_hidden.tolist(),\n",
    "                \"combined_pooled\": combined_pooled.tolist(),\n",
    "                \"shapes\": {\n",
    "                    \"clip_l_sequence\": list(clip_l_hidden.shape),\n",
    "                    \"clip_g_sequence\": list(clip_g_hidden.shape),\n",
    "                    \"combined_pooled\": list(combined_pooled.shape)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(embedding_data, f)\n",
    "            \n",
    "            print(f\"   ‚úì Saved: {filename}\")\n",
    "        \n",
    "        print(f\"\\n‚úì All {len(prompts)} embeddings saved to:\")\n",
    "        print(f\"  {CLIP_COMBINED_DIR}\")\n",
    "\n",
    "batch_generate_button.on_click(batch_generate_from_text)\n",
    "\n",
    "print(\"Batch CLIP-L + CLIP-G Embedding Generator\")\n",
    "print(f\"Output directory: {CLIP_COMBINED_DIR}\")\n",
    "print(\"Enter prompts (one per line):\\n\")\n",
    "display(batch_prompt_input, batch_generate_button, batch_output_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generation from Example Prompts File\n",
    "\n",
    "Load CLIP prompts from `misc/example_prompts.txt` and generate embeddings. Files are saved to `examples/` subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:27:38.108397Z",
     "iopub.status.busy": "2026-01-16T00:27:38.108191Z",
     "iopub.status.idle": "2026-01-16T00:27:38.122961Z",
     "shell.execute_reply": "2026-01-16T00:27:38.122470Z",
     "shell.execute_reply.started": "2026-01-16T00:27:38.108380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate from Example Prompts File\n",
      "Source: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/misc/example_prompts.txt\n",
      "Output: /shares/weddigen.ki.uzh/laura_wagner/latent_vandalism_workshop/data/embeddings/CLIP_SD3/examples\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4bc7e19eb149f880e2dfeb8fe65eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='info', description='Generate from File', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14f64e6a1d74ff3b2f4026cd271b507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Batch generate CLIP-L + CLIP-G embeddings from example_prompts.txt\n",
    "# EXAMPLES_DIR = CLIP_COMBINED_DIR / \"examples\"\n",
    "# os.makedirs(EXAMPLES_DIR, exist_ok=True)\n",
    "\n",
    "# # Path to prompts file\n",
    "# prompts_file = current_dir.parent / \"misc/example_prompts.txt\"\n",
    "\n",
    "# def load_clip_prompts_from_file(filepath):\n",
    "#     \"\"\"Load CLIP prompts section from example_prompts.txt\"\"\"\n",
    "#     if not filepath.exists():\n",
    "#         return []\n",
    "    \n",
    "#     with open(filepath, 'r', encoding='utf-8') as f:\n",
    "#         content = f.read()\n",
    "    \n",
    "#     sections = content.split('#')\n",
    "#     prompts = []\n",
    "    \n",
    "#     for section in sections:\n",
    "#         if 'CLIP prompts' in section:\n",
    "#             lines = section.split('\\n')\n",
    "#             for line in lines:\n",
    "#                 line = line.strip()\n",
    "#                 if line and not line.startswith('#') and 'prompts' not in line.lower():\n",
    "#                     prompts.append(line)\n",
    "#             break\n",
    "    \n",
    "#     return prompts\n",
    "\n",
    "# file_batch_button = widgets.Button(\n",
    "#     description='Generate from File',\n",
    "#     button_style='info'\n",
    "# )\n",
    "\n",
    "# file_batch_output = widgets.Output()\n",
    "\n",
    "# def batch_generate_from_file(b):\n",
    "#     with file_batch_output:\n",
    "#         file_batch_output.clear_output()\n",
    "        \n",
    "#         prompts = load_clip_prompts_from_file(prompts_file)\n",
    "        \n",
    "#         if not prompts:\n",
    "#             print(f\"No CLIP prompts found in {prompts_file}!\")\n",
    "#             return\n",
    "        \n",
    "#         print(f\"Loaded {len(prompts)} CLIP prompts\")\n",
    "#         print(f\"Output: {EXAMPLES_DIR}\\n\")\n",
    "        \n",
    "#         for i, prompt in enumerate(prompts, 1):\n",
    "#             print(f\"[{i}/{len(prompts)}] '{prompt[:60]}{'...' if len(prompt) > 60 else ''}'\")\n",
    "            \n",
    "#             clip_l_hidden, clip_g_hidden, combined_pooled = generate_combined_embedding(prompt)\n",
    "            \n",
    "#             # Extract first 3 words from the prompt for filename\n",
    "#             words = prompt.split()\n",
    "#             # Take first 3 words or fewer if prompt is shorter\n",
    "#             filename_words = words[:3]\n",
    "#             # Clean and format words for filename\n",
    "#             cleaned_words = []\n",
    "#             for word in filename_words:\n",
    "#                 # Remove special characters, keep letters, numbers, and underscores\n",
    "#                 cleaned = ''.join(c for c in word if c.isalnum() or c in ['_', '-'])\n",
    "#                 # Remove any remaining problematic characters\n",
    "#                 cleaned = cleaned.replace('/', '_').replace('\\\\', '_').replace(':', '_')\n",
    "#                 cleaned = cleaned.replace('?', '').replace('!', '').replace('.', '')\n",
    "#                 cleaned = cleaned.replace('\"', '').replace(\"'\", \"\").replace(',', '')\n",
    "#                 cleaned = cleaned.strip()\n",
    "#                 if cleaned:  # Only add non-empty cleaned words\n",
    "#                     cleaned_words.append(cleaned.lower())\n",
    "            \n",
    "#             # Join words with underscores, truncate if too long\n",
    "#             filename_base = \"_\".join(cleaned_words)\n",
    "#             # Limit total filename length to avoid filesystem issues\n",
    "#             if len(filename_base) > 100:\n",
    "#                 filename_base = filename_base[:100]\n",
    "            \n",
    "#             # Handle empty filename case\n",
    "#             if not filename_base:\n",
    "#                 filename_base = f\"prompt_{i}\"\n",
    "            \n",
    "#             filename = filename_base + \".json\"\n",
    "#             filepath = EXAMPLES_DIR / filename\n",
    "            \n",
    "#             embedding_data = {\n",
    "#                 \"prompt\": prompt,\n",
    "#                 \"clip_l_sequence\": clip_l_hidden.tolist(),\n",
    "#                 \"clip_g_sequence\": clip_g_hidden.tolist(),\n",
    "#                 \"combined_pooled\": combined_pooled.tolist(),\n",
    "#                 \"shapes\": {\n",
    "#                     \"clip_l_sequence\": list(clip_l_hidden.shape),\n",
    "#                     \"clip_g_sequence\": list(clip_g_hidden.shape),\n",
    "#                     \"combined_pooled\": list(combined_pooled.shape)\n",
    "#                 }\n",
    "#             }\n",
    "            \n",
    "#             with open(filepath, 'w') as f:\n",
    "#                 json.dump(embedding_data, f)\n",
    "            \n",
    "#             print(f\"   ‚úì Saved: {filename}\")\n",
    "        \n",
    "#         print(f\"\\n‚úì All {len(prompts)} embeddings saved to examples/\")\n",
    "\n",
    "# file_batch_button.on_click(batch_generate_from_file)\n",
    "\n",
    "# print(\"Generate from Example Prompts File\")\n",
    "# print(f\"Source: {prompts_file}\")\n",
    "# print(f\"Output: {EXAMPLES_DIR}\\n\")\n",
    "# display(file_batch_button, file_batch_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<sub>Latent Vandalism Workshop ‚Ä¢ Laura Wagner, 2026 ‚Ä¢ [laurajul.github.io](https://laurajul.github.io/)</sub>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flux uv",
   "language": "python",
   "name": "uv_flux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
