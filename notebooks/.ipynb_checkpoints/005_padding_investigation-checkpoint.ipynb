{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Understanding Padding Tokens in CLIP Embeddings\n",
    "\n",
    "## The Question\n",
    "\n",
    "When we have a prompt like **\"a beaver with blue teeth\"**, it only uses maybe 10 tokens out of 77 total positions. The remaining 67 positions are **padding tokens**.\n",
    "\n",
    "**Key Questions:**\n",
    "1. Are padding tokens always the same?\n",
    "2. How do they affect the final embedding?\n",
    "3. Do padding embeddings at position 10 differ from padding at position 50?\n",
    "4. Can we manipulate padding tokens to affect Flux output?\n",
    "\n",
    "Let's investigate!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load CLIP text model\n",
    "model_name = \"openai/clip-vit-large-patch14\"\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_name)\n",
    "model = CLIPTextModel.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"✓ Model loaded\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"  Max length: {tokenizer.model_max_length}\")\n",
    "print(f\"  Pad token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"  EOS token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"  BOS token: '{tokenizer.bos_token}' (ID: {tokenizer.bos_token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "investigate-tokens",
   "metadata": {},
   "source": [
    "## Investigation 1: What Are the Token IDs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "investigate-tokens-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a short prompt\n",
    "prompt = \"a beaver with blue teeth\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenizer(\n",
    "    prompt,\n",
    "    padding=\"max_length\",\n",
    "    max_length=77,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "token_ids = tokens['input_ids'][0].tolist()\n",
    "attention_mask = tokens['attention_mask'][0].tolist()\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"\\nToken IDs (first 15 positions):\")\n",
    "print(\"Position | Token ID | Attention | Decoded\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(15):\n",
    "    decoded = tokenizer.decode([token_ids[i]])\n",
    "    print(f\"{i:8} | {token_ids[i]:8} | {attention_mask[i]:9} | '{decoded}'\")\n",
    "\n",
    "print(\"\\n...\")\n",
    "print(\"\\nLast 5 positions:\")\n",
    "for i in range(72, 77):\n",
    "    decoded = tokenizer.decode([token_ids[i]])\n",
    "    print(f\"{i:8} | {token_ids[i]:8} | {attention_mask[i]:9} | '{decoded}'\")\n",
    "\n",
    "# Count real tokens vs padding\n",
    "num_real_tokens = sum(attention_mask)\n",
    "num_padding = 77 - num_real_tokens\n",
    "print(f\"\\n✓ Real tokens: {num_real_tokens}\")\n",
    "print(f\"✓ Padding tokens: {num_padding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "investigate-embeddings",
   "metadata": {},
   "source": [
    "## Investigation 2: Are Padding Embeddings Identical?\n",
    "\n",
    "Let's check if padding embeddings at different positions are the same or different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "investigate-embeddings-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embedding\n",
    "with torch.no_grad():\n",
    "    tokens_device = {k: v.to(device) for k, v in tokens.items()}\n",
    "    outputs = model(**tokens_device)\n",
    "    embedding = outputs.last_hidden_state[0]  # [77, 768]\n",
    "\n",
    "embedding_np = embedding.cpu().numpy()\n",
    "\n",
    "print(f\"Embedding shape: {embedding_np.shape}\")\n",
    "print(f\"\\nLet's compare padding embeddings at different positions...\\n\")\n",
    "\n",
    "# Find first padding position\n",
    "first_padding_pos = num_real_tokens\n",
    "print(f\"First padding position: {first_padding_pos}\")\n",
    "\n",
    "# Compare padding embeddings at different positions\n",
    "if num_padding > 1:\n",
    "    padding_positions = [first_padding_pos, first_padding_pos + 10, first_padding_pos + 20, 76]\n",
    "    padding_positions = [p for p in padding_positions if p < 77]\n",
    "    \n",
    "    print(f\"\\nComparing padding embeddings at positions: {padding_positions}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, pos in enumerate(padding_positions):\n",
    "        emb = embedding_np[pos]\n",
    "        print(f\"\\nPosition {pos}:\")\n",
    "        print(f\"  First 10 values: {emb[:10]}\")\n",
    "        print(f\"  Mean: {emb.mean():.6f}\")\n",
    "        print(f\"  Std: {emb.std():.6f}\")\n",
    "        print(f\"  L2 norm: {np.linalg.norm(emb):.6f}\")\n",
    "    \n",
    "    # Calculate pairwise differences\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Pairwise Cosine Similarities Between Padding Embeddings:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i in range(len(padding_positions)):\n",
    "        for j in range(i+1, len(padding_positions)):\n",
    "            pos_i = padding_positions[i]\n",
    "            pos_j = padding_positions[j]\n",
    "            emb_i = embedding_np[pos_i]\n",
    "            emb_j = embedding_np[pos_j]\n",
    "            \n",
    "            # Cosine similarity\n",
    "            cos_sim = np.dot(emb_i, emb_j) / (np.linalg.norm(emb_i) * np.linalg.norm(emb_j))\n",
    "            \n",
    "            # L2 distance\n",
    "            l2_dist = np.linalg.norm(emb_i - emb_j)\n",
    "            \n",
    "            print(f\"Positions {pos_i} vs {pos_j}:\")\n",
    "            print(f\"  Cosine similarity: {cos_sim:.8f}\")\n",
    "            print(f\"  L2 distance: {l2_dist:.6f}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-prompts",
   "metadata": {},
   "source": [
    "## Investigation 3: Are Padding Embeddings the Same Across Different Prompts?\n",
    "\n",
    "Do the padding embeddings change when we use different prompts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-prompts-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple prompts of different lengths\n",
    "test_prompts = [\n",
    "    \"cat\",\n",
    "    \"a red cat\",\n",
    "    \"a beaver with blue teeth\",\n",
    "    \"an elephant standing in a field of flowers\",\n",
    "]\n",
    "\n",
    "padding_embeddings = {}\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    # Tokenize\n",
    "    tokens = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=77,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Get embedding\n",
    "    with torch.no_grad():\n",
    "        tokens_device = {k: v.to(device) for k, v in tokens.items()}\n",
    "        outputs = model(**tokens_device)\n",
    "        embedding = outputs.last_hidden_state[0].cpu().numpy()\n",
    "    \n",
    "    # Find padding positions\n",
    "    attention_mask = tokens['attention_mask'][0].tolist()\n",
    "    num_real = sum(attention_mask)\n",
    "    first_padding = num_real\n",
    "    \n",
    "    # Store padding embedding at a consistent position (e.g., position 50)\n",
    "    if first_padding < 50:\n",
    "        padding_embeddings[prompt] = {\n",
    "            'num_real_tokens': num_real,\n",
    "            'first_padding': first_padding,\n",
    "            'padding_at_50': embedding[50],\n",
    "            'padding_at_76': embedding[76]\n",
    "        }\n",
    "    \n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"  Real tokens: {num_real}\")\n",
    "    print(f\"  First padding position: {first_padding}\")\n",
    "    print()\n",
    "\n",
    "# Compare padding embeddings across prompts\n",
    "print(\"=\"*60)\n",
    "print(\"Comparing Padding at Position 50 Across Different Prompts:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prompts_list = list(padding_embeddings.keys())\n",
    "for i in range(len(prompts_list)):\n",
    "    for j in range(i+1, len(prompts_list)):\n",
    "        prompt_i = prompts_list[i]\n",
    "        prompt_j = prompts_list[j]\n",
    "        \n",
    "        emb_i = padding_embeddings[prompt_i]['padding_at_50']\n",
    "        emb_j = padding_embeddings[prompt_j]['padding_at_50']\n",
    "        \n",
    "        cos_sim = np.dot(emb_i, emb_j) / (np.linalg.norm(emb_i) * np.linalg.norm(emb_j))\n",
    "        l2_dist = np.linalg.norm(emb_i - emb_j)\n",
    "        \n",
    "        print(f\"\\n'{prompt_i[:30]}...' vs '{prompt_j[:30]}...'\")\n",
    "        print(f\"  Cosine similarity: {cos_sim:.10f}\")\n",
    "        print(f\"  L2 distance: {l2_dist:.10f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if cos_sim > 0.9999:\n",
    "    print(\"✓ CONCLUSION: Padding embeddings at the same position are\")\n",
    "    print(\"  NEARLY IDENTICAL across different prompts!\")\n",
    "else:\n",
    "    print(\"✓ CONCLUSION: Padding embeddings differ across prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize",
   "metadata": {},
   "source": [
    "## Visualization: Real Tokens vs Padding Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the \"a beaver with blue teeth\" prompt\n",
    "prompt = \"a beaver with blue teeth\"\n",
    "\n",
    "tokens = tokenizer(\n",
    "    prompt,\n",
    "    padding=\"max_length\",\n",
    "    max_length=77,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokens_device = {k: v.to(device) for k, v in tokens.items()}\n",
    "    outputs = model(**tokens_device)\n",
    "    embedding = outputs.last_hidden_state[0].cpu().numpy()\n",
    "\n",
    "attention_mask = tokens['attention_mask'][0].tolist()\n",
    "num_real = sum(attention_mask)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Heatmap of embedding\n",
    "ax1 = axes[0]\n",
    "im = ax1.imshow(embedding.T, aspect='auto', cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "ax1.axvline(x=num_real-0.5, color='lime', linewidth=3, label='Padding starts here')\n",
    "ax1.set_xlabel('Token Position', fontsize=12)\n",
    "ax1.set_ylabel('Embedding Dimension', fontsize=12)\n",
    "ax1.set_title(f'CLIP Text Embedding: \"{prompt}\"\\n(Green line = where padding starts)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='upper right')\n",
    "plt.colorbar(im, ax=ax1, label='Embedding Value')\n",
    "\n",
    "# Plot 2: L2 norms of each token embedding\n",
    "ax2 = axes[1]\n",
    "norms = [np.linalg.norm(embedding[i]) for i in range(77)]\n",
    "colors = ['steelblue' if i < num_real else 'orange' for i in range(77)]\n",
    "ax2.bar(range(77), norms, color=colors, alpha=0.7)\n",
    "ax2.axvline(x=num_real-0.5, color='lime', linewidth=3, linestyle='--', \n",
    "            label='Padding starts here')\n",
    "ax2.set_xlabel('Token Position', fontsize=12)\n",
    "ax2.set_ylabel('L2 Norm', fontsize=12)\n",
    "ax2.set_title('L2 Norm of Each Token Embedding\\n(Blue = real tokens, Orange = padding)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Visualization complete\")\n",
    "print(f\"  Real tokens: {num_real}\")\n",
    "print(f\"  Padding tokens: {77 - num_real}\")\n",
    "print(f\"  Average norm of real tokens: {np.mean(norms[:num_real]):.4f}\")\n",
    "print(f\"  Average norm of padding tokens: {np.mean(norms[num_real:]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment",
   "metadata": {},
   "source": [
    "## Experiment: What If We Zero Out Padding?\n",
    "\n",
    "What happens if we replace padding embeddings with zeros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experiment-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "prompt = \"a beaver with blue teeth\"\n",
    "\n",
    "# Generate normal embedding\n",
    "tokens = tokenizer(\n",
    "    prompt,\n",
    "    padding=\"max_length\",\n",
    "    max_length=77,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokens_device = {k: v.to(device) for k, v in tokens.items()}\n",
    "    outputs = model(**tokens_device)\n",
    "    embedding_normal = outputs.last_hidden_state[0].cpu().numpy()\n",
    "\n",
    "attention_mask = tokens['attention_mask'][0].tolist()\n",
    "num_real = sum(attention_mask)\n",
    "\n",
    "# Create version with zeroed padding\n",
    "embedding_zeroed = embedding_normal.copy()\n",
    "embedding_zeroed[num_real:] = 0  # Zero out all padding positions\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Real tokens: {num_real}\")\n",
    "print(f\"\\nOriginal embedding stats:\")\n",
    "print(f\"  Mean: {embedding_normal.mean():.6f}\")\n",
    "print(f\"  Std: {embedding_normal.std():.6f}\")\n",
    "print(f\"  Norm: {np.linalg.norm(embedding_normal):.6f}\")\n",
    "\n",
    "print(f\"\\nZeroed padding embedding stats:\")\n",
    "print(f\"  Mean: {embedding_zeroed.mean():.6f}\")\n",
    "print(f\"  Std: {embedding_zeroed.std():.6f}\")\n",
    "print(f\"  Norm: {np.linalg.norm(embedding_zeroed):.6f}\")\n",
    "\n",
    "# Visualize difference\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "# Original\n",
    "im1 = axes[0].imshow(embedding_normal.T, aspect='auto', cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "axes[0].axvline(x=num_real-0.5, color='lime', linewidth=2)\n",
    "axes[0].set_title('Original Embedding (with padding)', fontweight='bold')\n",
    "axes[0].set_ylabel('Dimension')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Zeroed\n",
    "im2 = axes[1].imshow(embedding_zeroed.T, aspect='auto', cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "axes[1].axvline(x=num_real-0.5, color='lime', linewidth=2)\n",
    "axes[1].set_title('Zeroed Padding Embedding', fontweight='bold')\n",
    "axes[1].set_ylabel('Dimension')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# Difference\n",
    "diff = embedding_normal - embedding_zeroed\n",
    "im3 = axes[2].imshow(diff.T, aspect='auto', cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "axes[2].axvline(x=num_real-0.5, color='lime', linewidth=2)\n",
    "axes[2].set_title('Difference (what we removed)', fontweight='bold')\n",
    "axes[2].set_xlabel('Token Position')\n",
    "axes[2].set_ylabel('Dimension')\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save both versions\n",
    "current_dir = Path(os.getcwd())\n",
    "output_dir = current_dir.parent / \"data\" / \"embeddings\" / \"CLIP\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save normal\n",
    "normal_data = {\n",
    "    \"prompt\": prompt,\n",
    "    \"embedding\": embedding_normal.tolist(),\n",
    "    \"shape\": [77, 768]\n",
    "}\n",
    "with open(output_dir / \"beaver_normal.json\", 'w') as f:\n",
    "    json.dump(normal_data, f)\n",
    "\n",
    "# Save zeroed\n",
    "zeroed_data = {\n",
    "    \"prompt\": prompt + \" (zeroed padding)\",\n",
    "    \"embedding\": embedding_zeroed.tolist(),\n",
    "    \"shape\": [77, 768]\n",
    "}\n",
    "with open(output_dir / \"beaver_zeroed_padding.json\", 'w') as f:\n",
    "    json.dump(zeroed_data, f)\n",
    "\n",
    "print(\"\\n✓ Saved both embeddings to:\") \n",
    "print(f\"  {output_dir / 'beaver_normal.json'}\")\n",
    "print(f\"  {output_dir / 'beaver_zeroed_padding.json'}\")\n",
    "print(\"\\nYou can test both in Flux to see if padding affects the output!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary: What We Learned About Padding\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Padding Token Structure**:\n",
    "   - Padding uses a special token ID (usually the EOS token repeated)\n",
    "   - Appears in positions after real tokens up to position 77\n",
    "\n",
    "2. **Are Padding Embeddings Identical?**\n",
    "   - Padding at the **same position** across different prompts is nearly identical\n",
    "   - Padding at **different positions** within the same prompt may vary slightly (due to positional encodings)\n",
    "\n",
    "3. **Do They Affect Flux?**\n",
    "   - This is an empirical question! Test the two saved embeddings:\n",
    "     - `beaver_normal.json` - with normal padding\n",
    "     - `beaver_zeroed_padding.json` - with padding set to zero\n",
    "   - Compare the Flux outputs to see if padding matters\n",
    "\n",
    "4. **Practical Implications**:\n",
    "   - If padding doesn't affect output: Only the real tokens matter for image generation\n",
    "   - If padding DOES affect output: The padding embeddings contribute to Flux's understanding\n",
    "   - Either way, understanding this helps us know which parts of the embedding we can safely manipulate\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Test both embeddings in Flux (use your `Image_generation.ipynb` notebook)\n",
    "2. Compare outputs - are they identical or different?\n",
    "3. If different: padding matters! We could explore manipulating padding for creative effects\n",
    "4. If identical: we can focus our manipulations on real token positions only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}