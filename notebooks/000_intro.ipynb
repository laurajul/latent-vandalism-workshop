{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Vandalism: The Joy of Productive Damage to Text-to-Image Synthesis Pipelines\n",
    "\n",
    "**Workshop by Laura Wagner**\n",
    "\n",
    "ðŸ”— [laurajul.github.io](https://laurajul.github.io/)  \n",
    "ðŸ“¦ [Workshop Repository](https://github.com/laurajul/latent-vandalism-workshop)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Text-to-image models have evolved into sophisticated engines of **template culture** (Grund and Scherffig), systems trained to reproduce standardized aesthetics. Fatigued by the constant flood of polished results and the arms race for images benchmarked on visual coherence, commercial value and consumer-friendliness, this workshop explores once again the charm of **AI weirdness** (Shane) - the failure in generative AI and the epistemic value of **productive damage**.\n",
    "\n",
    "Drawing inspiration from **glitch studies** (Menkman), we embrace glitches and artifacts as revelatory moments. Through gently violating the consumer-friendly, polished norms meant to please, we surface the model's implicit assumptions about how things are supposed to look. Participants will work directly with Diffusion Transformers (DiT), focusing on the role of **embeddings** in image-text correlation from embedding space to latent space back into pixel space. Through hands-on meddling with the pipeline, we'll systematically **damage** and **reconfigure** the **semantic substrate** that guides image generation, deliberately perturbing inputs to understand this system's sensitivity and dynamics.\n",
    "\n",
    "This counterfactual, gently adversarial approach, positions productive damage as a research method. Through **iatrogenic techniques** performed on text-to-image models, we probe the layers of **technological inscription** (Latour) embedded in these systems. Values, design choices, and visual norms inscribed become legible where the system breaks down. By deliberately coaxing the model into failure, we trace the contours of what has been encoded into them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very high level Diagram of the text-to-image pipeline\n",
    "\n",
    "\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Text Prompt] --> B[Embedding Space]\n",
    "    B --> C[Latent Space]\n",
    "    C --> D[Pixel Space]\n",
    "    \n",
    "    style A stroke:#e1f5ff\n",
    "    style B stroke:#fff4e1\n",
    "    style C stroke:#ffe1f5\n",
    "    style D stroke:#d4edda\n",
    "```\n",
    "---\n",
    "\n",
    "1. **Embedding Space** (High-dimensional semantic vectors)\n",
    "   - Where text meaning is encoded numerically from tokens\n",
    "\n",
    "2. **Latent Space** (Compressed image representation)\n",
    "   - Where diffusion actually happens\n",
    "   - Much smaller than pixel space (e.g., 64Ã—64Ã—16 instead of 1024Ã—1024Ã—3)\n",
    "   - Embeddings guide the denoising process here\n",
    "\n",
    "3. **Pixel Space** (Final RGB image)\n",
    "   - The inference result\n",
    "   - Decoded from latent space by VAE\n",
    "---\n",
    "\n",
    "### Workshop Focus:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Text Prompt] --> B[Text Encoders]\n",
    "    B --> C[Embedding Space]\n",
    "    \n",
    "    C -.->|WE INTERVENE HERE| D[Modified Embeddings]\n",
    "    \n",
    "    D --> E[Diffusion in Latent Space]\n",
    "    E --> F[VAE Decoder]\n",
    "    F --> G[Pixel Space / Image]\n",
    "    \n",
    "    H[Random Noise] --> E\n",
    "    \n",
    "    style C stroke:#fff4e1\n",
    "    style D stroke:#ffcccc\n",
    "    style E stroke:#ffe1f5\n",
    "    style G stroke:#d4edda\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Differences: SD 3.5 vs FLUX-Schnell\n",
    "\n",
    "| Aspect | SD 3.5 | FLUX-Schnell |\n",
    "|--------|--------|-------------|\n",
    "| **Text Encoders** | T5-XXL, CLIP-L, CLIP-G | T5-XXL, CLIP-L |\n",
    "| **T5 Sequence Length** | 77 tokens | 512 tokens (longer context!) |\n",
    "| **Pooled Embeddings** | CLIP-L + CLIP-G (2048 dims) | CLIP-L only (768 dims) |\n",
    "| **Architecture** | MMDiT (Multimodal Diffusion Transformer) | FLUX Transformer |\n",
    "| **Denoising Process** | Standard diffusion (20-50 steps) | Flow matching (4 steps) |\n",
    "| **Embedding Usage** | Cross-attention + AdaLN modulation | Attention + guidance embedding |\n",
    "|**Speed** | Slower (more steps) | Faster (fewer steps) |\n",
    "\n",
    "\n",
    "\n",
    "Also: No negative embeddings in FLUX!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "#\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph \"Embedding Vandalism\"\n",
    "        A[Original Prompt] --> B[Generate Embeddings]\n",
    "        B --> C[Save to JSON]\n",
    "        C --> D{Vandalism Techniques}\n",
    "        \n",
    "        D --> E[Scaling]\n",
    "        D --> F[Inversion]\n",
    "        D --> G[Contradictory T5 and CLIP]\n",
    "        D --> H[Replace weights with zeroes]\n",
    "        \n",
    "        E --> J[Damaged Embeddings]\n",
    "        F --> J\n",
    "        G --> J\n",
    "        H --> J\n",
    "  \n",
    "    end\n",
    "    \n",
    "    subgraph \"Effect on Latent Diffusion: Productive Failures\"\n",
    "        J --> K[Shortcut Pipeline]\n",
    "        \n",
    "        K --> L[Transformer Attention<br/>with corrupted guidance]\n",
    "    end\n",
    "    \n",
    "    \n",
    "    style J stroke:#ffcccc\n",
    "    style L stroke:#ffe1f5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop Structure\n",
    "\n",
    "### ðŸ““ Introduction & Theory\n",
    "- **`000_intro.ipynb`** - Workshop introduction, theoretical framework, and conceptual overview\n",
    "\n",
    "### ðŸ”§ Embedding generation\n",
    "- **`001_t5_embeddings.ipynb`** - Understanding and extracting T5-XXL embeddings\n",
    "- **`002_CLIP_embeddings.ipynb`** - Working with CLIP text embeddings\n",
    "- **`003_CLIP-L_and_G_embeddings.ipynb`** - Comparing CLIP-L and CLIP-G encoders\n",
    "\n",
    "### ðŸ”¨ Vandalism Techniques\n",
    "- **`004_embedding_manipulation.ipynb`** - Systematic techniques for damaging embeddings (scaling, inversion, mixing, ablation)\n",
    "\n",
    "### ðŸš€ Inference & Generation\n",
    "- **`FLUX_schnell_inference.ipynb`** - Generating images with FLUX-Schnell using damaged embeddings\n",
    "- **`SD_3.5_inference.ipynb`** - Generating images with Stable Diffusion 3.5 using damaged embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### About This Workshop\n",
    "\n",
    "**Workshop by Laura Wagner**\n",
    "\n",
    "ðŸ”— Website: [laurajul.github.io](https://laurajul.github.io/)  \n",
    "ðŸ“¦ Repository: [github.com/laurajul/latent-vandalism-workshop](https://github.com/laurajul/latent-vandalism-workshop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
