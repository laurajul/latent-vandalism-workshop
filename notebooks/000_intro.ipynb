{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Vandalism: The Joy of Productive Damage to Text-to-Image Synthesis Pipelines\n",
    "\n",
    "**Workshop by Laura Wagner**\n",
    "\n",
    "ðŸ”— [laurajul.github.io](https://laurajul.github.io/)  \n",
    "ðŸ“¦ [Workshop Repository](https://github.com/laurajul/latent-vandalism-workshop)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Text-to-image models have evolved into sophisticated engines of template culture (Grund and Scherffig), systems trained to reproduce standardized aesthetics. Fatigued by the constant flood of polished results and the arms race for images benchmarked on visual coherence, commercial value and consumer-friendliness, this workshop explores once again the charm of AI weirdness (Shane) - the failure in generative AI and the epistemic value of productive damage.\n",
    "\n",
    "Drawing inspiration from glitch studies (Menkman), we embrace glitches and artifacts as revelatory moments. Through gently violating the consumer-friendly, polished norms meant to please, we surface the model's implicit assumptions about how things are supposed to look. Participants will work directly with Diffusion Transformers (DiT), focusing on the role of **embeddings** in image-text correlation from embedding space to latent space back into pixel space. Through hands-on meddling with the pipeline, we'll systematically **damage** and **reconfigure** the **semantic substrate** that guides image generation, deliberately perturbing inputs to understand this system's sensitivity and dynamics.\n",
    "\n",
    "This counterfactual, gently adversarial approach, positions productive damage as a research method. Through **iatrogenic techniques** performed on text-to-image models, we probe the layers of technological inscription (Latour) embedded in these systems. Values, design choices, and visual norms inscribed become legible where the system breaks down. By deliberately coaxing the model into failure, we trace the contours of what has been encoded into them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very high level Diagram of the text-to-image pipeline\n",
    "\n",
    "\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Text Prompt] --> B[Embedding Space]\n",
    "    B --> C[Latent Space]\n",
    "    C --> D[Pixel Space]\n",
    "    \n",
    "    style A stroke:#e1f5ff\n",
    "    style B stroke:#fff4e1\n",
    "    style C stroke:#ffe1f5\n",
    "    style D stroke:#d4edda\n",
    "```\n",
    "---\n",
    "\n",
    "1. **Embedding Space** (High-dimensional semantic vectors)\n",
    "   - Where text meaning is encoded numerically from tokens\n",
    "\n",
    "2. **Latent Space** (Compressed image representation)\n",
    "   - Where diffusion actually happens\n",
    "   - Much smaller than pixel space (e.g., 64Ã—64Ã—16 instead of 1024Ã—1024Ã—3)\n",
    "   - Embeddings guide the denoising process here\n",
    "\n",
    "3. **Pixel Space** (Final RGB image)\n",
    "   - The inference result\n",
    "   - Decoded from latent space by VAE\n",
    "---\n",
    "\n",
    "### Workshop Focus:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Text Prompt] --> B[Text Encoders]\n",
    "    B --> C[Embedding Space]\n",
    "    \n",
    "    C -.->|WE INTERVENE HERE| D[Modified Embeddings]\n",
    "    \n",
    "    D --> E[Diffusion in Latent Space]\n",
    "    E --> F[VAE Decoder]\n",
    "    F --> G[Pixel Space / Image]\n",
    "    \n",
    "    H[Random Noise] --> E\n",
    "    \n",
    "    style C stroke:#fff4e1\n",
    "    style D stroke:#ffcccc\n",
    "    style E stroke:#ffe1f5\n",
    "    style G stroke:#d4edda\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Differences: SD 3.5 vs FLUX-Schnell\n",
    "\n",
    "| Aspect | SD 3.5 | FLUX-Schnell |\n",
    "|--------|--------|-------------|\n",
    "| **Text Encoders** | T5-XXL, CLIP-L, CLIP-G | T5-XXL, CLIP-L |\n",
    "| **T5 Sequence Length** | 77 tokens | 512 tokens (longer context!) |\n",
    "| **Pooled Embeddings** | CLIP-L + CLIP-G (2048 dims) | CLIP-L only (768 dims) |\n",
    "| **Architecture** | MMDiT (Multimodal Diffusion Transformer) | FLUX Transformer |\n",
    "| **Denoising Process** | Standard diffusion (20-50 steps) | Flow matching (4 steps) |\n",
    "| **Embedding Usage** | Cross-attention + AdaLN modulation | Attention + guidance embedding |\n",
    "|**Speed** | Slower (more steps) | Faster (fewer steps) |\n",
    "\n",
    "\n",
    "\n",
    "Also: No negative embeddings in FLUX!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "#\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph \"Embedding Vandalism\"\n",
    "    \n",
    "        A[Original Prompt] --> B[Generate Embeddings]\n",
    "        B --> C[Save to JSON]\n",
    "        C --> D{Vandalism Techniques}\n",
    "        \n",
    "        D --> E[Scaling]\n",
    "        D --> F[Inversion]\n",
    "        D --> G[Contradictory T5 and CLIP]\n",
    "        D --> H[Replace weights with zeroes]\n",
    "        \n",
    "        E --> J[Damaged Embeddings]\n",
    "        F --> J\n",
    "        G --> J\n",
    "        H --> J\n",
    "  \n",
    "    end\n",
    "    \n",
    "    subgraph \"Effect on Latent Diffusion: Productive Failures\"\n",
    "        J --> K[Shortcut Pipeline]\n",
    "        \n",
    "        K --> L[Transformer Attention<br/>with corrupted guidance]\n",
    "    end\n",
    "    \n",
    "    \n",
    "    style J stroke:#ffcccc\n",
    "    style L stroke:#ffe1f5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Embedding Dimensions:\n",
    "- **T5-XXL**: 77 tokens Ã— 4096 dimensions (SD 3.5) / 512 tokens Ã— 4096 (FLUX)\n",
    "- **CLIP-L**: 77 tokens Ã— 768 dimensions + 768 pooled\n",
    "- **CLIP-G**: 77 tokens Ã— 1280 dimensions + 1280 pooled (SD 3.5 only)\n",
    "\n",
    "### How They're Used:\n",
    "1. **Text embeddings** (concatenated) â†’ Cross-attention in transformer\n",
    "2. **Pooled embeddings** (concatenated) â†’ Global conditioning (AdaLN/guidance)\n",
    "\n",
    "### Workshop Method:\n",
    "- **Productive damage** as epistemological tool\n",
    "- **Iatrogenic techniques** to probe system boundaries\n",
    "- **Glitch aesthetics** as revelatory moments\n",
    "- **Counterfactual experiments** to understand inscription\n",
    "\n",
    "### What Vandalism Reveals:\n",
    "- Direct manipulation bypasses text encoding limitations\n",
    "- Systematic damage exposes training data biases\n",
    "- Failures make visible the inscribed norms and assumptions\n",
    "- AI weirdness provides epistemic value beyond polish\n",
    "- Each encoder contributes distinct, separable semantic information\n",
    "- Template culture's boundaries become legible where it breaks\n",
    "\n",
    "---\n",
    "\n",
    "**Remember:** The goal isn't to make \"better\" imagesâ€”it's to understand what \"better\" means to these systems, and to find creative freedom in the spaces where that definition breaks down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Framework: References\n",
    "\n",
    "This workshop draws on several theoretical traditions:\n",
    "\n",
    "### Glitch Studies\n",
    "- **Menkman, Rosa.** *The Glitch Moment(um)*. Network Notebooks, 2011.\n",
    "  - Glitches as revelatory moments that expose normally invisible structures\n",
    "  - Productive failures as aesthetic and epistemic resources\n",
    "\n",
    "### Template Culture\n",
    "- **Grund, Katja and Scherffig, Lasse.** Work on template culture and standardized aesthetics in generative AI\n",
    "  - How models reproduce homogeneous visual languages\n",
    "  - The political economy of aesthetic standardization\n",
    "\n",
    "### AI Weirdness\n",
    "- **Shane, Janelle.** Research on AI failures and unexpected behaviors\n",
    "  - The epistemic value of AI mistakes\n",
    "  - How failures reveal system structure\n",
    "\n",
    "### Science and Technology Studies\n",
    "- **Latour, Bruno.** \"Technology is society made durable.\" *Sociological Review*, 1990.\n",
    "  - Technological inscription: How values and choices become embedded in systems\n",
    "  - Making visible the social and political dimensions of technical artifacts\n",
    "\n",
    "### Iatrogenic Methods\n",
    "- Medical concept of harm caused by treatment itself, repurposed as deliberate intervention\n",
    "  - Systematic damage as research methodology\n",
    "  - Counterfactual reasoning through controlled failures\n",
    "\n",
    "---\n",
    "\n",
    "### About This Workshop\n",
    "\n",
    "**Workshop by Laura Wagner**\n",
    "\n",
    "ðŸ”— Website: [laurajul.github.io](https://laurajul.github.io/)  \n",
    "ðŸ“¦ Repository: [github.com/laurajul/latent-vandalism-workshop](https://github.com/laurajul/latent-vandalism-workshop)\n",
    "\n",
    "For questions, feedback, or collaborations on productive damage to generative AI systems, please reach out via the website or repository.\n",
    "\n",
    "---\n",
    "\n",
    "*\"The charm of AI weirdness is not just in the strange outputs, but in what those outputs reveal about the system that produced them.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
