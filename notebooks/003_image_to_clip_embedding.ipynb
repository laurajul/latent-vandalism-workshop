{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photo to CLIP Embedding Converter\n",
    "\n",
    "This notebook allows you to select images from a folder and convert them to CLIP embeddings.\n",
    "The embeddings are saved as JSON files for use in your Flux pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install torch torchvision transformers pillow ipywidgets --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import json\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "current_dir = Path(os.getcwd())\n",
    "output_dir = current_dir.parent / \"data\" / \"embeddings\" / \"CLIP\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Current directory: {current_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model\n",
    "print(\"Loading CLIP model...\")\n",
    "model_name = \"openai/clip-vit-large-patch14\"\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_image_files(directory):\n    \"\"\"Get all image files from a directory\"\"\"\n    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}\n    directory = Path(directory)\n    \n    if not directory.exists():\n        return []\n    \n    image_files = []\n    for file in directory.iterdir():\n        if file.is_file() and file.suffix.lower() in image_extensions:\n            image_files.append(file)\n    \n    return sorted(image_files)\n\n\ndef extract_clip_image_embedding(image_path):\n    \"\"\"\n    Extract CLIP image embeddings from an image file.\n    Returns embeddings with shape [77, 768] to match text embedding format.\n    \n    The ViT-Large vision encoder produces 1024-dimensional hidden states,\n    which are then projected to 768 dimensions to match the text encoder.\n    \n    Args:\n        image_path: Path to the image file\n    \n    Returns:\n        Dictionary with embedding data\n    \"\"\"\n    # Load image\n    image = Image.open(image_path).convert('RGB')\n    \n    # Process the image\n    inputs = processor(images=image, return_tensors=\"pt\")\n    \n    # Get image embeddings\n    with torch.no_grad():\n        # Get vision encoder hidden states [1, 257, 1024]\n        vision_outputs = model.vision_model(**inputs)\n        image_embeds = vision_outputs.last_hidden_state  # [1, 257, 1024]\n        image_embeds = image_embeds.squeeze(0)  # [257, 1024]\n        \n        # Apply visual projection to get 768 dimensions [257, 1024] -> [257, 768]\n        image_embeds = model.visual_projection(image_embeds)  # [257, 768]\n        \n        # Truncate to 77 tokens to match text embedding format\n        # Take the first 77 tokens (including CLS token and first 76 patch tokens)\n        image_embeds = image_embeds[:77, :]  # [77, 768]\n    \n    # Convert to list for JSON serialization\n    embedding_list = image_embeds.cpu().numpy().tolist()\n    \n    return {\n        \"prompt\": str(image_path.stem),  # Use image filename without extension\n        \"embedding\": embedding_list,\n        \"shape\": [77, 768]\n    }\n\n\ndef save_embedding_json(embedding_data, output_path):\n    \"\"\"Save embedding data to JSON file\"\"\"\n    with open(output_path, 'w') as f:\n        json.dump(embedding_data, f)\n    print(f\"Saved: {output_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Widget for selecting image directory\nimage_dir_input = widgets.Text(\n    value=str(current_dir.parent / \"data\" / \"input_img\"),\n    placeholder='Enter path to image folder',\n    description='Image Folder:',\n    style={'description_width': 'initial'},\n    layout=widgets.Layout(width='600px')\n)\n\nload_button = widgets.Button(\n    description='Load Images',\n    button_style='info',\n    tooltip='Load images from the specified folder'\n)\n\nimage_selector = widgets.Dropdown(\n    options=[],\n    description='Select Image:',\n    style={'description_width': 'initial'},\n    layout=widgets.Layout(width='600px'),\n    disabled=True\n)\n\npreview_image = widgets.Image(\n    format='png',\n    width=400,\n    height=400\n)\n\nconvert_button = widgets.Button(\n    description='Convert to CLIP Embedding',\n    button_style='success',\n    tooltip='Extract CLIP embedding and save as JSON',\n    disabled=True\n)\n\noutput_text = widgets.Output()\n\n# Current state\nstate = {\n    'image_files': [],\n    'current_image': None\n}\n\n\ndef on_load_images(b):\n    \"\"\"Load images from the specified directory\"\"\"\n    with output_text:\n        clear_output()\n        image_dir = Path(image_dir_input.value)\n        \n        if not image_dir.exists():\n            print(f\"‚ùå Directory not found: {image_dir}\")\n            image_selector.options = []\n            image_selector.disabled = True\n            convert_button.disabled = True\n            return\n        \n        image_files = get_image_files(image_dir)\n        \n        if not image_files:\n            print(f\"‚ùå No images found in: {image_dir}\")\n            image_selector.options = []\n            image_selector.disabled = True\n            convert_button.disabled = True\n            return\n        \n        state['image_files'] = image_files\n        image_selector.options = [(f.name, f) for f in image_files]\n        image_selector.disabled = False\n        convert_button.disabled = False\n        \n        print(f\"‚úÖ Loaded {len(image_files)} images from {image_dir}\")\n\n\ndef on_image_selected(change):\n    \"\"\"Update preview when image is selected\"\"\"\n    if change['new'] is None:\n        return\n    \n    image_path = change['new']\n    state['current_image'] = image_path\n    \n    # Load and display preview\n    with open(image_path, 'rb') as f:\n        preview_image.value = f.read()\n    \n    with output_text:\n        print(f\"Selected: {image_path.name}\")\n\n\ndef on_convert(b):\n    \"\"\"Convert selected image to CLIP embedding\"\"\"\n    with output_text:\n        clear_output()\n        \n        if state['current_image'] is None:\n            print(\"‚ùå No image selected\")\n            return\n        \n        image_path = state['current_image']\n        \n        print(f\"üîÑ Processing: {image_path.name}\")\n        print(f\"   Output shape: [77, 768]\")\n        \n        try:\n            # Extract embedding\n            embedding_data = extract_clip_image_embedding(image_path)\n            \n            # Generate output filename\n            input_stem = image_path.stem  # filename without extension\n            output_filename = f\"{input_stem}_from_image.json\"\n            output_path = output_dir / output_filename\n            \n            # Save to JSON\n            save_embedding_json(embedding_data, output_path)\n            \n            print(f\"‚úÖ Success!\")\n            print(f\"   Shape: {embedding_data['shape']}\")\n            print(f\"   Saved to: {output_path}\")\n            \n        except Exception as e:\n            print(f\"‚ùå Error: {e}\")\n\n\n# Connect callbacks\nload_button.on_click(on_load_images)\nimage_selector.observe(on_image_selected, names='value')\nconvert_button.on_click(on_convert)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display the interface\nprint(\"=\" * 70)\nprint(\"Photo to CLIP Embedding Converter\")\nprint(\"=\" * 70)\nprint(f\"Output directory: {output_dir}\")\nprint(f\"Output shape: [77, 768]\")\nprint(\"=\" * 70)\n\ndisplay(\n    widgets.VBox([\n        widgets.HBox([image_dir_input, load_button]),\n        image_selector,\n        preview_image,\n        convert_button,\n        output_text\n    ])\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Usage Instructions\n\n1. **Specify Image Folder**: Enter the path to your folder containing example photos\n2. **Load Images**: Click \"Load Images\" to scan the folder\n3. **Select Image**: Choose an image from the dropdown menu\n4. **Convert**: Click \"Convert to CLIP Embedding\" to process and save\n\nThe embeddings will be saved with shape **[77, 768]** to match the format of text CLIP embeddings.\n\nOutput JSON files will be saved in: `../data/embeddings/CLIP/`\n\nFilename format: `{original_image_name}_from_image.json`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Load and Use the Embedding\n",
    "\n",
    "After converting images, you can load and use the embeddings in your Flux pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example: Load a saved embedding\ndef load_embedding(json_path):\n    \"\"\"Load embedding from JSON file\"\"\"\n    with open(json_path, 'r') as f:\n        data = json.load(f)\n    \n    embedding = torch.tensor(data['embedding'])\n    print(f\"Loaded: {data['prompt']}\")\n    print(f\"Shape: {data['shape']}\")\n    \n    return embedding\n\n# Uncomment to test loading:\n# embedding = load_embedding(output_dir / \"your_image_from_image.json\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Notes\n\n- **Output Format**: All embeddings are saved with shape **[77, 768]** to match text CLIP embedding format\n  \n- **Dimension Projection**: The ViT-Large vision encoder produces 1024-dimensional hidden states, which are projected to 768 dimensions using the visual projection layer to match the text encoder's output dimension.\n\n- **Token Truncation**: The notebook extracts the first 77 tokens from the CLIP vision encoder's projected output (which normally produces 257 tokens). This includes the CLS token and the first 76 patch tokens.\n\n- **Compatibility**: The embeddings use the same CLIP model (`clip-vit-large-patch14`) and projection to the shared 768-dimensional space for consistency with text embeddings\n\n- **File Size**: Each embedding file is approximately 750KB\n\n- **JSON Format**: Output matches the format used by text embeddings:\n  - `prompt`: Image filename (without extension)\n  - `embedding`: List of 77 token vectors, each with 768 dimensions\n  - `shape`: [77, 768]"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}